{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2s\n",
    "\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import datetime\n",
    "import plotly.graph_objects as go\n",
    "from config import config\n",
    "\n",
    "connection = mysql.connector.connect(\n",
    "    host=config.get('HOST'),\n",
    "    user=config.get('USERNAME'),\n",
    "    password=config.get('PASSWORD'),\n",
    "    database=config.get('DATABASE')\n",
    ")\n",
    "\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0s\n",
    "\n",
    "# Function to validate date format\n",
    "def validate_date(date_str):\n",
    "    try:\n",
    "        datetime.datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "while True:\n",
    "    start_date = input(\"Please enter the start date (YYYY-MM-DD HH:MM:SS): \")\n",
    "    if validate_date(start_date):\n",
    "        break\n",
    "    else:\n",
    "        print(\"Invalid format. Please enter the date in the format YYYY-MM-DD HH:MM:SS\")\n",
    "\n",
    "while True:\n",
    "    end_date = input(\"Please enter the end date (YYYY-MM-DD HH:MM:SS): \")\n",
    "    if validate_date(end_date):\n",
    "        break\n",
    "    else:\n",
    "        print(\"Invalid format. Please enter the date in the format YYYY-MM-DD HH:MM:SS\")\n",
    "\n",
    "print(f\"Start date: {start_date}\")\n",
    "print(f\"End date: {end_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of tweets grouped by month- 44s\n",
    "\n",
    "tweet_timestamps_query = f\"\"\"\n",
    "SELECT timestamp_ms \n",
    "FROM tweets \n",
    "WHERE timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "    AND timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000;\n",
    "\"\"\"\n",
    "\n",
    "df_timestamp = pd.read_sql(tweet_timestamps_query, connection)\n",
    "\n",
    "df_timestamp['datetime'] = pd.to_datetime(df_timestamp['timestamp_ms'], unit='ms') # Convert timestamp to a date time format\n",
    "\n",
    "df_timestamp['datetime'] = df_timestamp['datetime'].dt.tz_localize('UTC').dt.tz_convert('Europe/Amsterdam')\n",
    "\n",
    "df_timestamp['month'] = df_timestamp['datetime'].dt.month # Extract the month when the conv starts\n",
    "\n",
    "df_timestamp['month_year'] = df_timestamp['datetime'].dt.strftime('%b-%Y')\n",
    "\n",
    "df_timestamp = df_timestamp.sort_values(by='datetime')\n",
    "\n",
    "# Plot the distribution of tweets grouped by month-year\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=df_timestamp, x='month_year', palette='viridis', order=pd.to_datetime(df_timestamp['month_year'], format='%b-%Y').sort_values().dt.strftime('%b-%Y').unique())\n",
    "plt.title('Tweet frequency over Month-Year')\n",
    "plt.xlabel('Month-Year')\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0s\n",
    "\n",
    "def get_count(query):\n",
    "    cursor = connection.cursor()\n",
    "    try:\n",
    "        for result in cursor.execute(query, multi=True):\n",
    "            if result.with_rows:\n",
    "                first_result = result.fetchone()\n",
    "                cursor.fetchall()  # Ensure all remaining rows are fetched\n",
    "        return first_result[0] if first_result else 0\n",
    "    finally:\n",
    "        cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of times an airline was mentioned- 15s\n",
    "\n",
    "tweet_mentioned_ba = f\"\"\"\n",
    "SELECT COUNT(DISTINCT id)\n",
    "FROM tweets\n",
    "WHERE (tweets.user_mentions LIKE '%18332190%' OR tweets.mentioned_airlines LIKE '%British_Airways%')\n",
    "    AND timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000 \n",
    "    AND timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000; \n",
    "\"\"\"\n",
    "\n",
    "tweet_mentioned_klm = f\"\"\" \n",
    "SELECT COUNT(DISTINCT id)\n",
    "FROM tweets\n",
    "WHERE (tweets.user_mentions LIKE '%56377143%' OR tweets.mentioned_airlines LIKE '%KLM%')\n",
    "    AND timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000 \n",
    "    AND timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000; \n",
    "\"\"\"\n",
    "\n",
    "tweet_mentioned_lufthansa = f\"\"\" \n",
    "SELECT COUNT(DISTINCT id)\n",
    "FROM tweets\n",
    "WHERE (tweets.user_mentions LIKE '%124476322%' OR tweets.mentioned_airlines LIKE '%Lufthansa%')\n",
    "    AND timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000 \n",
    "    AND timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000; \n",
    "\"\"\"\n",
    "\n",
    "tweet_mentioned_af = f\"\"\" \n",
    "SELECT COUNT(DISTINCT id)\n",
    "FROM tweets\n",
    "WHERE (tweets.user_mentions LIKE '%106062176%' OR tweets.mentioned_airlines LIKE '%AirFrance%')\n",
    "    AND timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000 \n",
    "    AND timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "tweet_mentioned_ba_count = get_count(tweet_mentioned_ba)\n",
    "tweet_mentioned_klm_count = get_count(tweet_mentioned_klm)\n",
    "tweet_mentioned_lufthansa_count = get_count(tweet_mentioned_lufthansa)\n",
    "tweet_mentioned_af_count = get_count(tweet_mentioned_af)\n",
    "\n",
    "airlines = ['British Airways', 'KLM', 'Lufthansa', 'Air France']\n",
    "counts = [\n",
    "    tweet_mentioned_ba_count, \n",
    "    tweet_mentioned_klm_count, \n",
    "    tweet_mentioned_lufthansa_count, \n",
    "    tweet_mentioned_af_count\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=airlines, y=counts, palette='viridis')\n",
    "plt.title('Number of Times Each Airline was Mentioned')\n",
    "plt.xlabel('Airlines')\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency of tweets about baggage isssues and flight delays- 3 s\n",
    "\n",
    "baggage_query = f\"\"\"\n",
    "SELECT COUNT(id)\n",
    "FROM tweets\n",
    "WHERE baggage = 1\n",
    "    AND timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000 \n",
    "    AND timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000;\n",
    "\"\"\" \n",
    "\n",
    "flight_delays_query = f\"\"\"\n",
    "SELECT COUNT(id)\n",
    "FROM tweets\n",
    "WHERE delay_and_cancellation = 1\n",
    "    AND timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000 \n",
    "    AND timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000;\n",
    "\"\"\" \n",
    "\n",
    "issue = ['baggage', 'delay']\n",
    "issue_counts = [get_count(baggage_query), get_count(flight_delays_query)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=issue, y=issue_counts, palette='viridis')\n",
    "plt.title('Frequency of Tweets Aboout Baggage Issues and Flight Delays')\n",
    "plt.xlabel('Issue')\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General statistics about conversations- 6 s\n",
    "\n",
    "# Extract conversation starts time units for all connversations\n",
    "conv_start = f\"\"\"\n",
    "SELECT start \n",
    "FROM conversations \n",
    "WHERE start >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "    AND start <= UNIX_TIMESTAMP('{end_date}') * 1000;\n",
    "\"\"\"\n",
    "\n",
    "df_start = pd.read_sql(conv_start, connection)\n",
    "\n",
    "df_start['datetime'] = pd.to_datetime(df_start['start'], unit='ms') # Convert timestamp to a date time format\n",
    "\n",
    "df_start['datetime'] = df_start['datetime'].dt.tz_localize('UTC').dt.tz_convert('Europe/Amsterdam')\n",
    "\n",
    "df_start['month'] = df_start['datetime'].dt.month # Extract the month when the conv starts\n",
    "\n",
    "monthly_distribution = df_start['month'].value_counts().reindex(range(1, 13), fill_value=0).sort_index() # Count the number of conversations starting on each month\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = sns.barplot(x=monthly_distribution.index, y=monthly_distribution.values)\n",
    "plt.xlabel('Month of the year', fontsize=12)\n",
    "plt.ylabel('Number of conversations', fontsize=12)\n",
    "plt.title('Monthly distribution of conversations starts', fontsize=14, weight='bold')\n",
    "plt.xticks(range(12), ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'], fontsize = 9)\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.0f'), \n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha = 'center', va = 'center', \n",
    "                xytext = (0, 9), \n",
    "                textcoords = 'offset points',\n",
    "                fontsize=11, weight='bold', color='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Airline specific conversation statistics- 1m 18s\n",
    "\n",
    "# Extract conversation starts time units for conversations per airline\n",
    "def extract_month(query, connection):\n",
    "\n",
    "    df = pd.read_sql(query, connection)\n",
    "\n",
    "    df['datetime'] = pd.to_datetime(df['start'], unit='ms') # Convert timestamp to a date time format\n",
    "\n",
    "    df['datetime'] = df['datetime'].dt.tz_localize('UTC').dt.tz_convert('Europe/Amsterdam')\n",
    "\n",
    "    df['month'] = df['datetime'].dt.month # Extract the month when the conv starts\n",
    "\n",
    "    return df\n",
    "\n",
    "british_airways_conv_start_query = f\"\"\"\n",
    "SELECT DISTINCT conversations.conversation_id, conversations.start\n",
    "FROM conversations\n",
    "JOIN hasher ON hasher.conversation_id = conversations.conversation_id\n",
    "JOIN tweets ON hasher.id = tweets.id \n",
    "WHERE (tweets.user_mentions LIKE '%18332190%' \n",
    "    OR tweets.mentioned_airlines LIKE '%British_Airways%' \n",
    "    OR conversations.airline LIKE '%British_Airways%')\n",
    "    AND conversations.start >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "    AND conversations.start <= UNIX_TIMESTAMP('{end_date}') * 1000;\n",
    "\"\"\"\n",
    "KLM_conv_start_query = f\"\"\" \n",
    "SELECT DISTINCT conversations.conversation_id, conversations.start\n",
    "FROM conversations\n",
    "JOIN hasher ON hasher.conversation_id = conversations.conversation_id\n",
    "JOIN tweets ON hasher.id = tweets.id \n",
    "WHERE (tweets.user_mentions LIKE '%56377143%' \n",
    "    OR tweets.mentioned_airlines LIKE '%KLM%' \n",
    "    OR conversations.airline LIKE '%KLM%')\n",
    "    AND conversations.start >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "    AND conversations.start <= UNIX_TIMESTAMP('{end_date}') * 1000;\n",
    "\"\"\"\n",
    "lufthansa_conv_start_query = f\"\"\" \n",
    "SELECT DISTINCT conversations.conversation_id, conversations.start\n",
    "FROM conversations\n",
    "JOIN hasher ON hasher.conversation_id = conversations.conversation_id\n",
    "JOIN tweets ON hasher.id = tweets.id \n",
    "WHERE (tweets.user_mentions LIKE '%124476322%' \n",
    "    OR tweets.mentioned_airlines LIKE '%lufthansa%' \n",
    "    OR conversations.airline LIKE '%lufthansa%')\n",
    "    AND conversations.start >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "    AND conversations.start <= UNIX_TIMESTAMP('{end_date}') * 1000;\n",
    "\"\"\"\n",
    "air_france_conv_start_query = f\"\"\" \n",
    "SELECT DISTINCT conversations.conversation_id, conversations.start\n",
    "FROM conversations\n",
    "JOIN hasher ON hasher.conversation_id = conversations.conversation_id\n",
    "JOIN tweets ON hasher.id = tweets.id \n",
    "WHERE (tweets.user_mentions LIKE '%106062176%' \n",
    "    OR tweets.mentioned_airlines LIKE '%AirFrance%' \n",
    "    OR conversations.airline LIKE '%AirFrance%')\n",
    "    AND conversations.start >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "    AND conversations.start <= UNIX_TIMESTAMP('{end_date}') * 1000;\n",
    "\"\"\"\n",
    "\n",
    "british_airways_conv_start = extract_month(british_airways_conv_start_query, connection)\n",
    "KLM_conv_start = extract_month(KLM_conv_start_query, connection)\n",
    "lufthansa_conv_start = extract_month(lufthansa_conv_start_query, connection)\n",
    "air_france_conv_start = extract_month(air_france_conv_start_query, connection) \n",
    "\n",
    "# Count the number of conversations starting each month\n",
    "british_airways_monthly_distribution = british_airways_conv_start['month'].value_counts().reindex(range(1, 13), fill_value=0).sort_index() \n",
    "KLM_monthly_distribution = KLM_conv_start['month'].value_counts().reindex(range(1, 13), fill_value=0).sort_index() \n",
    "lufthansa_monthly_distribution = lufthansa_conv_start['month'].value_counts().reindex(range(1, 13), fill_value=0).sort_index() \n",
    "air_france_monthly_distribution = air_france_conv_start['month'].value_counts().reindex(range(1, 13), fill_value=0).sort_index()\n",
    "\n",
    "# Concatenate data frames into one data frame\n",
    "airlines_monthly_distribution_df = pd.concat([british_airways_monthly_distribution, KLM_monthly_distribution, lufthansa_monthly_distribution, air_france_monthly_distribution], axis = 1)\n",
    "airlines_monthly_distribution_df.columns = ['British Airways', 'KLM', 'Lufthansa', 'Air France']\n",
    "\n",
    "# Plot the monthly distribution data frame\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=airlines_monthly_distribution_df, linewidth=2.5)\n",
    "\n",
    "plt.title('Monthly Distribution of Inclusion in Conversations per Airline', fontsize=14, weight='bold')\n",
    "plt.xlabel('Month of the year', fontsize=12)\n",
    "plt.ylabel('Number of Conversations', fontsize=12)\n",
    "plt.xticks(ticks=range(1, 13), labels=['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'],rotation=45)\n",
    "plt.legend(title='Airlines', loc='upper right', fontsize=12)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queries for number of conversations where an airline was mentioned per airline- 0s\n",
    "conv_mentioned_ba = f\"\"\"\n",
    "SELECT COUNT(DISTINCT conversations.conversation_id)\n",
    "FROM conversations\n",
    "JOIN hasher ON hasher.conversation_id = conversations.conversation_id\n",
    "JOIN tweets ON hasher.id = tweets.id\n",
    "WHERE (tweets.user_mentions LIKE '%18332190%' OR tweets.mentioned_airlines LIKE '%British_Airways%')\n",
    "    AND conversations.start >= UNIX_TIMESTAMP('{start_date}') * 1000 \n",
    "    AND conversations.start <= UNIX_TIMESTAMP('{end_date}') * 1000; \n",
    "\"\"\"\n",
    "\n",
    "conv_mentioned_klm = f\"\"\" \n",
    "SELECT COUNT(DISTINCT conversations.conversation_id)\n",
    "FROM conversations\n",
    "JOIN hasher ON hasher.conversation_id = conversations.conversation_id\n",
    "JOIN tweets ON hasher.id = tweets.id\n",
    "WHERE (tweets.user_mentions LIKE '%56377143%' OR tweets.mentioned_airlines LIKE '%KLM%')\n",
    "    AND conversations.start >= UNIX_TIMESTAMP('{start_date}') * 1000 \n",
    "    AND conversations.start <= UNIX_TIMESTAMP('{end_date}') * 1000; \n",
    "\"\"\"\n",
    "\n",
    "conv_mentioned_lufthansa = f\"\"\" \n",
    "SELECT COUNT(DISTINCT conversations.conversation_id)\n",
    "FROM conversations\n",
    "JOIN hasher ON hasher.conversation_id = conversations.conversation_id\n",
    "JOIN tweets ON hasher.id = tweets.id\n",
    "WHERE (tweets.user_mentions LIKE '%124476322%' OR tweets.mentioned_airlines LIKE '%Lufthansa%')\n",
    "    AND conversations.start >= UNIX_TIMESTAMP('{start_date}') * 1000 \n",
    "    AND conversations.start <= UNIX_TIMESTAMP('{end_date}') * 1000; \n",
    "\"\"\"\n",
    "\n",
    "conv_mentioned_af = f\"\"\" \n",
    "SELECT COUNT(DISTINCT conversations.conversation_id)\n",
    "FROM conversations\n",
    "JOIN hasher ON hasher.conversation_id = conversations.conversation_id\n",
    "JOIN tweets ON hasher.id = tweets.id\n",
    "WHERE (tweets.user_mentions LIKE '%106062176%' OR tweets.mentioned_airlines LIKE '%AirFrance%')\n",
    "    AND conversations.start >= UNIX_TIMESTAMP('{start_date}') * 1000 \n",
    "    AND conversations.start <= UNIX_TIMESTAMP('{end_date}') * 1000;\n",
    "\"\"\"\n",
    "\n",
    "# Queries for number of conversations where an airline was mentioned and the airline replied\n",
    "\n",
    "conv_replies_mentioned_ba = f\"\"\"\n",
    "SELECT COUNT(DISTINCT conversations.conversation_id)\n",
    "FROM conversations\n",
    "JOIN hasher ON hasher.conversation_id = conversations.conversation_id\n",
    "JOIN tweets ON hasher.id = tweets.id\n",
    "WHERE (tweets.user_mentions LIKE '%18332190%' OR tweets.mentioned_airlines LIKE '%British_Airways%')\n",
    "    AND conversations.airline LIKE '%British_Airways%'\n",
    "    AND conversations.start >= UNIX_TIMESTAMP('{start_date}') * 1000 \n",
    "    AND conversations.start <= UNIX_TIMESTAMP('{end_date}') * 1000;\n",
    "\"\"\"\n",
    "\n",
    "conv_replies_mentioned_klm = f\"\"\" \n",
    "SELECT COUNT(DISTINCT conversations.conversation_id)\n",
    "FROM conversations\n",
    "JOIN hasher ON hasher.conversation_id = conversations.conversation_id\n",
    "JOIN tweets ON hasher.id = tweets.id\n",
    "WHERE (tweets.user_mentions LIKE '%56377143%' OR tweets.mentioned_airlines LIKE '%KLM%') \n",
    "    AND conversations.airline LIKE '%KLM%'\n",
    "    AND conversations.start >= UNIX_TIMESTAMP('{start_date}') * 1000 \n",
    "    AND conversations.start <= UNIX_TIMESTAMP('{end_date}') * 1000;\n",
    "\"\"\"\n",
    "\n",
    "conv_replies_mentioned_lufthansa = f\"\"\" \n",
    "SELECT COUNT(DISTINCT conversations.conversation_id)\n",
    "FROM conversations\n",
    "JOIN hasher ON hasher.conversation_id = conversations.conversation_id\n",
    "JOIN tweets ON hasher.id = tweets.id\n",
    "WHERE (tweets.user_mentions LIKE '%124476322%' OR tweets.mentioned_airlines LIKE '%Lufthansa%') \n",
    "    AND conversations.airline LIKE '%Lufthansa%'\n",
    "    AND conversations.start >= UNIX_TIMESTAMP('{start_date}') * 1000 \n",
    "    AND conversations.start <= UNIX_TIMESTAMP('{end_date}') * 1000;\n",
    "\"\"\"\n",
    "\n",
    "conv_replies_mentioned_af = f\"\"\" \n",
    "SELECT COUNT(DISTINCT conversations.conversation_id)\n",
    "FROM conversations\n",
    "JOIN hasher ON hasher.conversation_id = conversations.conversation_id\n",
    "JOIN tweets ON hasher.id = tweets.id\n",
    "WHERE (tweets.user_mentions LIKE '%106062176%' OR tweets.mentioned_airlines LIKE '%AirFrance%')\n",
    "    AND conversations.airline LIKE '%AirFrance%'\n",
    "    AND conversations.start >= UNIX_TIMESTAMP('{start_date}') * 1000 \n",
    "    AND conversations.start <= UNIX_TIMESTAMP('{end_date}') * 1000;\n",
    "\"\"\"\n",
    "\n",
    "# Queries for number of conversations where an airline was mentioned and the airline did not reply\n",
    "\n",
    "conv_non_replies_where_mentioned_ba = f\"\"\"\n",
    "SELECT COUNT(DISTINCT conversations.conversation_id)\n",
    "FROM conversations\n",
    "JOIN hasher ON hasher.conversation_id = conversations.conversation_id\n",
    "JOIN tweets ON hasher.id = tweets.id\n",
    "WHERE (tweets.user_mentions LIKE '%18332190%' OR tweets.mentioned_airlines LIKE '%British_Airways%')\n",
    "  AND conversations.airline NOT LIKE '%British_Airways%'\n",
    "  AND conversations.start >= UNIX_TIMESTAMP('{start_date}') * 1000 \n",
    "  AND conversations.start <= UNIX_TIMESTAMP('{end_date}') * 1000;\n",
    "\"\"\"\n",
    "\n",
    "conv_non_replies_where_mentioned_klm = f\"\"\"\n",
    "SELECT COUNT(DISTINCT conversations.conversation_id)\n",
    "FROM conversations\n",
    "JOIN hasher ON hasher.conversation_id = conversations.conversation_id\n",
    "JOIN tweets ON hasher.id = tweets.id\n",
    "WHERE (tweets.user_mentions LIKE '%56377143%' OR tweets.mentioned_airlines LIKE '%KLM%')\n",
    "  AND conversations.airline NOT LIKE '%KLM%'\n",
    "  AND conversations.start >= UNIX_TIMESTAMP('{start_date}') * 1000 \n",
    "  AND conversations.start <= UNIX_TIMESTAMP('{end_date}') * 1000; \n",
    "\"\"\"\n",
    "\n",
    "conv_non_replies_where_mentioned_lufthansa = f\"\"\"\n",
    "SELECT COUNT(DISTINCT conversations.conversation_id)\n",
    "FROM conversations\n",
    "JOIN hasher ON hasher.conversation_id = conversations.conversation_id\n",
    "JOIN tweets ON hasher.id = tweets.id\n",
    "WHERE (tweets.user_mentions LIKE '%124476322%' OR tweets.mentioned_airlines LIKE '%Lufthansa%')\n",
    "  AND conversations.airline NOT LIKE '%Lufthansa%'\n",
    "  AND conversations.start >= UNIX_TIMESTAMP('{start_date}') * 1000 \n",
    "  AND conversations.start <= UNIX_TIMESTAMP('{end_date}') * 1000; \n",
    "\"\"\"\n",
    "\n",
    "conv_non_replies_where_mentioned_af = f\"\"\"\n",
    "SELECT COUNT(DISTINCT conversations.conversation_id)\n",
    "FROM conversations\n",
    "JOIN hasher ON hasher.conversation_id = conversations.conversation_id\n",
    "JOIN tweets ON hasher.id = tweets.id\n",
    "WHERE (tweets.user_mentions LIKE '%106062176%' OR tweets.mentioned_airlines LIKE '%AirFrance%')\n",
    "  AND conversations.airline NOT LIKE '%AirFrance%'\n",
    "  AND conversations.start >= UNIX_TIMESTAMP('{start_date}') * 1000 \n",
    "  AND conversations.start <= UNIX_TIMESTAMP('{end_date}') * 1000;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2m 30s\n",
    "\n",
    "mentioned_ba_count = get_count(conv_mentioned_ba)\n",
    "replies_mentioned_ba_count = get_count(conv_replies_mentioned_ba)\n",
    "non_replies_where_mentioned_ba_count = get_count(conv_non_replies_where_mentioned_ba)\n",
    "\n",
    "mentioned_klm_count = get_count(conv_mentioned_klm)\n",
    "replies_mentioned_klm_count = get_count(conv_replies_mentioned_klm)\n",
    "non_replies_where_mentioned_klm_count = get_count(conv_non_replies_where_mentioned_klm)\n",
    "\n",
    "mentioned_lufthansa_count = get_count(conv_mentioned_lufthansa)\n",
    "replies_mentioned_lufthansa_count = get_count(conv_replies_mentioned_lufthansa)\n",
    "non_replies_where_mentioned_lufthansa_count = get_count(conv_non_replies_where_mentioned_lufthansa)\n",
    "\n",
    "mentioned_af_count = get_count(conv_mentioned_af)\n",
    "replies_mentioned_af_count = get_count(conv_replies_mentioned_af)\n",
    "non_replies_where_mentioned_af_count = get_count(conv_non_replies_where_mentioned_af)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0s\n",
    "\n",
    "replied_percentage_ba = replies_mentioned_ba_count / mentioned_ba_count * 100\n",
    "non_replied_percentage_ba = non_replies_where_mentioned_ba_count / mentioned_ba_count * 100\n",
    "\n",
    "replied_percentage_klm = replies_mentioned_klm_count / mentioned_klm_count * 100\n",
    "non_replied_percentage_klm = non_replies_where_mentioned_klm_count / mentioned_klm_count * 100\n",
    "\n",
    "replied_percentage_lufthansa = replies_mentioned_lufthansa_count / mentioned_lufthansa_count * 100\n",
    "non_replied_percentage_lufthansa = non_replies_where_mentioned_lufthansa_count / mentioned_lufthansa_count * 100\n",
    "\n",
    "replied_percentage_af = replies_mentioned_af_count / mentioned_af_count * 100\n",
    "non_replied_percentage_af = non_replies_where_mentioned_af_count / mentioned_af_count * 100\n",
    "\n",
    "mentioned_convs = [mentioned_ba_count, mentioned_klm_count, mentioned_lufthansa_count, mentioned_af_count]\n",
    "replied_percentages = [replied_percentage_ba, replied_percentage_klm, replied_percentage_lufthansa, replied_percentage_af]\n",
    "non_replied_percentages = [non_replied_percentage_ba, non_replied_percentage_klm, non_replied_percentage_lufthansa, non_replied_percentage_af]\n",
    "\n",
    "replied_counts = [replies_mentioned_ba_count, replies_mentioned_klm_count, replies_mentioned_lufthansa_count, replies_mentioned_af_count]\n",
    "non_replied_counts = [non_replies_where_mentioned_ba_count, non_replies_where_mentioned_klm_count, non_replies_where_mentioned_lufthansa_count, non_replies_where_mentioned_af_count]\n",
    "\n",
    "bar_width = 0.5\n",
    "ind = range(len(airlines))\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (9, 6))\n",
    "\n",
    "p1 = ax.bar(ind, replied_counts, bar_width, label='Replied')\n",
    "p2 = ax.bar(ind, non_replied_counts, bar_width, bottom=replied_counts, label='Non Replied')\n",
    "\n",
    "ax.set_ylabel('Number of Conversations, where mentioned', fontsize= 11)\n",
    "ax.set_title('Number of Mentioned Conversations and Reply Percentages per Airline', fontsize= 12, weight= 'bold')\n",
    "ax.set_xticks(ind)\n",
    "ax.set_xticklabels(airlines)\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "for i in range(len(airlines)):\n",
    "    ax.text(i, replied_counts[i] / 2, f\"{replied_percentages[i]:.1f}%\", ha='center', va='center', color='white', weight='bold', fontsize=10)\n",
    "    ax.text(i, replied_counts[i] + non_replied_counts[i] / 2, f\"{non_replied_percentages[i]:.1f}%\", ha='center', va='center', color='white', weight='bold', fontsize=11)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14s\n",
    "\n",
    "all_tweets_query = f\"\"\"SELECT text, user_id, mentioned_airlines, user_mentions, label, timestamp_ms, baggage, money, staff, delay_and_cancellation\n",
    "            FROM tweets\n",
    "            WHERE timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000 \n",
    "                AND timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000 \n",
    "                AND ((\n",
    "                tweets.mentioned_airlines LIKE '%AirFrance%'\n",
    "                OR tweets.mentioned_airlines LIKE '%Lufthansa%'\n",
    "                OR tweets.mentioned_airlines LIKE '%KLM%'\n",
    "                OR tweets.mentioned_airlines LIKE '%British Airways%'\n",
    "            )\n",
    "            OR (\n",
    "                tweets.user_mentions LIKE '%106062176%'\n",
    "                OR tweets.user_mentions LIKE '%124476322%'\n",
    "                OR tweets.user_mentions LIKE '%56377143%'\n",
    "                OR tweets.user_mentions LIKE '%18332190%'\n",
    "            ))\"\"\"\n",
    "\n",
    "cursor.execute(all_tweets_query)\n",
    "texts = cursor.fetchall()\n",
    "print(\"Text fetched.\")\n",
    "number_tweets = cursor.rowcount\n",
    "print(\"Total number of rows in table: \", number_tweets)\n",
    "\n",
    "df = pd.DataFrame(texts, columns=['text', 'user_id', 'mentioned_airlines', 'user_mentions', 'label', 'timestamp', 'baggage', 'money', 'staff', 'delay_and_cancellation'])\n",
    "\n",
    "df['datetime'] = pd.to_datetime(df['timestamp'], unit='ms') \n",
    "\n",
    "df['datetime'] = df['datetime'].dt.tz_localize('UTC').dt.tz_convert('Europe/Amsterdam')\n",
    "\n",
    "df['month'] = df['datetime'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setiment distribution among all airlines- 0s\n",
    "\n",
    "sentiment_counts = df['label'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "ax = sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette='Blues')\n",
    "\n",
    "plt.xlabel('Sentiment', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Sentiment Distribution Among all Airlines', fontsize=14, weight='bold')\n",
    "plt.xticks(fontsize=11)\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.0f'), \n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha = 'center', va = 'center', \n",
    "                xytext = (0, 9), \n",
    "                textcoords = 'offset points',\n",
    "                fontsize=11, weight='bold', color='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of Positive, Neutral, and Negative Sentiments per Airline- 2s\n",
    "\n",
    "df['AirFrance'] = df['mentioned_airlines'].apply(lambda x: 'AirFrance' in x) | df['user_mentions'].apply(lambda x: '106062176' in x)\n",
    "df['Lufthansa'] = df['mentioned_airlines'].apply(lambda x: 'Lufthansa' in x) | df['user_mentions'].apply(lambda x: '124476322' in x)\n",
    "df['KLM'] = df['mentioned_airlines'].apply(lambda x: 'KLM' in x) | df['user_mentions'].apply(lambda x: '56377143' in x)\n",
    "df['British_Airways'] = df['mentioned_airlines'].apply(lambda x: 'British_Airways' in x) | df['user_mentions'].apply(lambda x: '18332190' in x)\n",
    "\n",
    "airlines_2 = ['AirFrance', 'Lufthansa', 'KLM', 'British_Airways']\n",
    "sentiments = ['positive', 'neutral', 'negative']\n",
    "\n",
    "data = []\n",
    "for airline in airlines_2:\n",
    "    sentiment_counts = []\n",
    "    for sentiment in sentiments:\n",
    "        count = df[df[airline] & (df['label'] == sentiment)].shape[0]\n",
    "        sentiment_counts.append(count)\n",
    "    total = sum(sentiment_counts)\n",
    "    percentages = [(count / total) * 100 if total > 0 else 0 for count in sentiment_counts]\n",
    "    data.append(percentages)\n",
    "\n",
    "plot_df = pd.DataFrame(data, columns=sentiments, index=airlines_2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Create the stacked bar plot\n",
    "custom_colors = ['#01295C', '#EB2226', '#A7A9AC', '#B9CFED']\n",
    "\n",
    "plot_df.plot(kind='bar', stacked=True, color=custom_colors, ax=ax, edgecolor='white')\n",
    "\n",
    "for i, airline in enumerate(airlines_2):\n",
    "    pos_base = 0\n",
    "    neu_base = plot_df.loc[airline, 'positive']\n",
    "    neg_base = plot_df.loc[airline, 'positive'] + plot_df.loc[airline, 'neutral']\n",
    "    for sentiment in sentiments:\n",
    "        count = plot_df.loc[airline, sentiment]\n",
    "        percentage = f\"{count:.1f}%\"\n",
    "        if sentiment == 'positive':\n",
    "            ax.text(i, pos_base + count / 2, percentage, ha='center', va='center', fontsize=11, weight='bold', color='white')\n",
    "            pos_base += count\n",
    "        elif sentiment == 'neutral':\n",
    "            ax.text(i, neu_base + count / 2, percentage, ha='center', va='center', fontsize=11, weight='bold', color='white')\n",
    "            neu_base += count\n",
    "        else:\n",
    "            ax.text(i, neg_base + count / 2, percentage, ha='center', va='center', fontsize=11, weight='bold', color='white')\n",
    "            neg_base += count\n",
    "\n",
    "            \n",
    "ax.set_ylabel('Percentage', fontsize=14)\n",
    "ax.set_xlabel('Airline', fontsize=14)\n",
    "ax.set_title('Percentage of Positive, Neutral, and Negative Sentiments per Airline', fontsize=16, weight='bold', pad=20)\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax.legend(title='Sentiment', fontsize=12, title_fontsize='13', loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment change through months for KLM- 6s\n",
    "\n",
    "klm_query = f\"\"\"\n",
    "SELECT text, label, timestamp_ms\n",
    "FROM tweets\n",
    "WHERE (mentioned_airlines LIKE '%%KLM%%' OR user_mentions LIKE '%%56377143%%')\n",
    "    AND timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "    AND timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000;\n",
    "\"\"\"\n",
    "\n",
    "# Load the result of the query into a DataFrame\n",
    "klm_df = pd.read_sql(klm_query, connection)\n",
    "\n",
    "# Convert the datetime column to pandas datetime\n",
    "klm_df['datetime'] = pd.to_datetime(klm_df['timestamp_ms'], unit='ms')\n",
    "\n",
    "klm_df['datetime'] = klm_df['datetime'].dt.tz_localize('UTC').dt.tz_convert('Europe/Amsterdam')\n",
    "\n",
    "klm_df['month'] = klm_df['datetime'].dt.month\n",
    "\n",
    "klm_df = klm_df.sort_values(by='datetime')\n",
    "\n",
    "klm_df['month_year'] = klm_df['datetime'].dt.strftime('%b-%Y')\n",
    "\n",
    "# Count occurrences of each label per month\n",
    "label_counts = klm_df.groupby(['month_year', 'label']).size().unstack(fill_value=0)\n",
    "\n",
    "label_counts.index = pd.to_datetime(label_counts.index, format='%b-%Y')\n",
    "\n",
    "# Sort the label_counts DataFrame by the index\n",
    "label_counts = label_counts.sort_index()\n",
    "\n",
    "# Plot the distribution of tweets grouped by month-year\n",
    "plt.figure(figsize=(12, 6))\n",
    "label_counts.plot(kind='line', marker='o', ax=plt.gca(), color=['red', 'orange', 'green'])\n",
    "plt.title('Sentiment Evolution Over Time for KLM')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Sentiment', labels=['Negative', 'Neutral', 'Positive'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration of sentiment evolution- 2m\n",
    "\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "airline_user_id = 18332190  # Replace with actual user ID for the airline\n",
    "\n",
    "\n",
    "# Define the query to get all tweets in the conversations involving British Airways\n",
    "query = f\"\"\"\n",
    "SELECT tweets.text, tweets.label, tweets.user_id, tweets.timestamp_ms, hasher.conversation_id\n",
    "FROM tweets\n",
    "JOIN hasher ON hasher.id = tweets.id\n",
    "JOIN conversations ON hasher.conversation_id = conversations.conversation_id\n",
    "WHERE conversations.airline LIKE '%%British_Airways%%'\n",
    "    AND tweets.timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "    AND tweets.timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000\n",
    "ORDER BY tweets.timestamp_ms;\n",
    "\"\"\"\n",
    "\n",
    "# Load the result of the query into a DataFrame\n",
    "df_tweets_british_airways = pd.read_sql(query, connection)\n",
    "\n",
    "# Convert the datetime column to pandas datetime\n",
    "df_tweets_british_airways['datetime'] = pd.to_datetime(df_tweets_british_airways['timestamp_ms'])\n",
    "\n",
    "df_tweets_british_airways['datetime'] = df_tweets_british_airways['datetime'].dt.tz_localize('UTC').dt.tz_convert('Europe/Amsterdam')\n",
    "\n",
    "# Initialize empty lists to store sentiments\n",
    "sentiments_before_reply = []\n",
    "sentiments_after_reply = []\n",
    "\n",
    "# Group tweets by conversation_id\n",
    "grouped = df_tweets_british_airways.groupby('conversation_id')\n",
    "\n",
    "\n",
    "# Process each conversation\n",
    "for conversation_id, group in grouped:\n",
    "    # Identify the timestamp of the airline's reply\n",
    "    airline_reply = group[group['user_id'] == airline_user_id]\n",
    "    if not airline_reply.empty:\n",
    "        airline_reply_time = airline_reply['datetime'].iloc[0]\n",
    "\n",
    "        # Tweets before the airline reply\n",
    "        tweets_before_reply = group[group['datetime'] < airline_reply_time]\n",
    "        sentiments_before_reply.extend(tweets_before_reply['label'].tolist())\n",
    "        \n",
    "        # Tweets after the airline reply\n",
    "        tweets_after_reply = group[group['datetime'] >= airline_reply_time]\n",
    "        sentiments_after_reply.extend(tweets_after_reply['label'].tolist())\n",
    "\n",
    "# Convert to DataFrame for plotting\n",
    "df_before_reply = pd.DataFrame(sentiments_before_reply, columns=['label'])\n",
    "df_after_reply = pd.DataFrame(sentiments_after_reply, columns=['label'])\n",
    "\n",
    "# Calculate percentages\n",
    "before_counts = df_before_reply['label'].value_counts(normalize=True) * 100\n",
    "after_counts = df_after_reply['label'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Plot sentiment before and after the reply\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Sentiment before reply\n",
    "plt.subplot(1, 2, 1)\n",
    "before_counts.reindex(['neutral', 'positive', 'negative'], fill_value=0).plot(kind='bar', color=['orange', 'green', 'red'])\n",
    "plt.title('Sentiment Before Airline Reply')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Percentage')\n",
    "plt.xticks([0, 1, 2], ['Neutral', 'Positive', 'Negative'])\n",
    "plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda y, _: f'{y:.0f}%'))\n",
    "\n",
    "# Sentiment after reply\n",
    "plt.subplot(1, 2, 2)\n",
    "after_counts.reindex(['neutral', 'positive', 'negative'], fill_value=0).plot(kind='bar', color=['orange', 'green', 'red'])\n",
    "plt.title('Sentiment After Airline Reply')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Percentage')\n",
    "plt.xticks([0, 1, 2], ['Neutral', 'Positive', 'Negative'])\n",
    "plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda y, _: f'{y:.0f}%'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of sentiment percentages before and after an airline reply in a single conversation- 1s\n",
    "\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "query = f\"\"\"\n",
    "SELECT tweets.text,tweets.label, tweets.user_id, tweets.timestamp_ms\n",
    "FROM tweets\n",
    "JOIN hasher ON hasher.id = tweets.id\n",
    "JOIN conversations ON hasher.conversation_id = conversations.conversation_id\n",
    "WHERE conversations.conversation_id = (\n",
    "    SELECT conversation_id\n",
    "    FROM conversations\n",
    "    WHERE airline LIKE '%%British_Airways%%'\n",
    "    ORDER BY length DESC\n",
    "    LIMIT 1\n",
    "    )\n",
    "    AND timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "    AND timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000\n",
    "ORDER BY tweets.timestamp_ms;\n",
    "\"\"\"\n",
    "airline_user_id = 18332190\n",
    "# Load the result of the query into a DataFrame\n",
    "df_tweets_in_max_conversation = pd.read_sql(query, connection)\n",
    "\n",
    "df_tweets_in_max_conversation['datetime'] = pd.to_datetime(df_tweets_in_max_conversation['timestamp_ms'], unit='ms')\n",
    "\n",
    "df_tweets_in_max_conversation['datetime'] = df_tweets_in_max_conversation['datetime'].dt.tz_localize('UTC').dt.tz_convert('Europe/Amsterdam')\n",
    "\n",
    "\n",
    "# Identify the timestamp of the airline's reply\n",
    "airline_reply = df_tweets_in_max_conversation[df_tweets_in_max_conversation['user_id'] == airline_user_id]\n",
    "if not airline_reply.empty:\n",
    "    airline_reply_time = airline_reply['datetime'].iloc[0]\n",
    "else:\n",
    "    airline_reply_time = None\n",
    "\n",
    "if airline_reply_time is not None:\n",
    "    # Tweets before the airline reply\n",
    "    tweets_before_reply = df_tweets_in_max_conversation[df_tweets_in_max_conversation['datetime'] < airline_reply_time]\n",
    "    \n",
    "    # Tweets after the airline reply\n",
    "    tweets_after_reply = df_tweets_in_max_conversation[df_tweets_in_max_conversation['datetime'] >= airline_reply_time]\n",
    "\n",
    "if airline_reply_time is not None:\n",
    "    # Calculate sentiment percentages\n",
    "    before_counts = tweets_before_reply['label'].value_counts(normalize=True) * 100\n",
    "    after_counts = tweets_after_reply['label'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    # Plot sentiment before and after the reply\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    # Sentiment before reply\n",
    "    plt.subplot(1, 2, 1)\n",
    "    before_counts.plot(kind='bar', color=['orange', 'red', 'green'])\n",
    "    plt.title('Sentiment Before Airline Reply')\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda y, _: f'{y:.0f}%'))\n",
    "    \n",
    "    # Sentiment after reply\n",
    "    plt.subplot(1, 2, 2)\n",
    "    after_counts.plot(kind='bar', color=['red', 'orange', 'green'])\n",
    "    plt.title('Sentiment After Airline Reply')\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda y, _: f'{y:.0f}%'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No airline reply found, unable to plot sentiment comparison.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the Dataframe for British Airways- 17 s\n",
    "\n",
    "conv_ba1 = f\"\"\"\n",
    "    SELECT text, id, staff, baggage, delay_and_cancellation, money, timestamp_ms\n",
    "    FROM tweets \n",
    "    WHERE mentioned_airlines LIKE '%British_Airways%'\n",
    "        AND timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "        AND timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000;   \n",
    "\"\"\"\n",
    "conv_ba2 = \"\"\"\n",
    "    SELECT tweets.text, tweets.id, tweets.staff, tweets.baggage, tweets.delay_and_cancellation, tweets.money, tweets.timestamp_ms\n",
    "    FROM tweets \n",
    "    JOIN hasher ON tweets.id = hasher.id\n",
    "    JOIN conversations ON hasher.conversation_id = conversations.conversation_id\n",
    "    WHERE conversations.conversation_id IN (\n",
    "        SELECT conversation_id\n",
    "        FROM conversations\n",
    "        WHERE airline LIKE '%British_Airways%'\n",
    "            AND timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "            AND timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000\n",
    "    ); \n",
    "\"\"\"\n",
    "\n",
    "df_ba1 = pd.read_sql(conv_ba1, connection)\n",
    "df_ba2 = pd.read_sql(conv_ba2, connection)\n",
    "\n",
    "df_ba = pd.concat([df_ba1, df_ba2], ignore_index=True, axis=0)\n",
    "df_ba = df_ba.drop_duplicates()\n",
    "\n",
    "df_ba['datetime'] = pd.to_datetime(df_ba['timestamp_ms'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the dataframe for AirFrance- 6s\n",
    "\n",
    "conv_af1 = f\"\"\"\n",
    "    SELECT text, id, staff, baggage, delay_and_cancellation, money, timestamp_ms\n",
    "    FROM tweets \n",
    "    WHERE mentioned_airlines LIKE '%AirFrance%'\n",
    "        AND timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "        AND timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000;\n",
    "\"\"\"\n",
    "conv_af2 = f\"\"\"\n",
    "    SELECT tweets.text, tweets.id, tweets.staff, tweets.baggage, tweets.delay_and_cancellation, tweets.money, tweets.timestamp_ms\n",
    "    FROM tweets \n",
    "    JOIN hasher ON tweets.id = hasher.id\n",
    "    JOIN conversations ON hasher.conversation_id = conversations.conversation_id\n",
    "    WHERE conversations.conversation_id IN (\n",
    "        SELECT conversation_id\n",
    "        FROM conversations\n",
    "        WHERE airline LIKE '%AirFrance%'\n",
    "            AND timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "            AND timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000   \n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "df_af1 = pd.read_sql(conv_af1, connection)\n",
    "df_af2 = pd.read_sql(conv_af2, connection)\n",
    "\n",
    "df_af = pd.concat([df_af1, df_af2], ignore_index=True, axis=0)\n",
    "df_af = df_af.drop_duplicates()\n",
    "\n",
    "df_af['datetime'] = pd.to_datetime(df_af['timestamp_ms'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the dataframe for KLM- 7s\n",
    "\n",
    "conv_klm1 = f\"\"\"\n",
    "    SELECT text, id, staff, baggage, delay_and_cancellation, money, timestamp_ms\n",
    "    FROM tweets \n",
    "    WHERE mentioned_airlines LIKE '%KLM%'\n",
    "        AND timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "        AND timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000; \n",
    "\"\"\"\n",
    "conv_klm2 = f\"\"\"\n",
    "    SELECT tweets.text, tweets.id, tweets.staff, tweets.baggage, tweets.delay_and_cancellation, tweets.money, tweets.timestamp_ms\n",
    "    FROM tweets \n",
    "    JOIN hasher ON tweets.id = hasher.id\n",
    "    JOIN conversations ON hasher.conversation_id = conversations.conversation_id\n",
    "    WHERE conversations.conversation_id IN (\n",
    "        SELECT conversation_id\n",
    "        FROM conversations\n",
    "        WHERE airline LIKE '%KLM%'\n",
    "            AND timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "            AND timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "df_klm1 = pd.read_sql(conv_klm1, connection)\n",
    "df_klm2 = pd.read_sql(conv_klm2, connection)\n",
    "\n",
    "df_klm = pd.concat([df_klm1, df_klm2], ignore_index=True, axis=0)\n",
    "df_klm = df_klm.drop_duplicates()\n",
    "\n",
    "df_klm['datetime'] = pd.to_datetime(df_klm['timestamp_ms'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the dataframe for Lufthansa- 7s\n",
    "\n",
    "conv_lh1 = f\"\"\"\n",
    "    SELECT text, id, staff, baggage, delay_and_cancellation, money, timestamp_ms\n",
    "    FROM tweets \n",
    "    WHERE mentioned_airlines LIKE '%Lufthansa%'\n",
    "       AND timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "       AND timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000; \n",
    "\"\"\"\n",
    "conv_lh2 = f\"\"\"\n",
    "    SELECT tweets.text, tweets.id, tweets.staff, tweets.baggage, tweets.delay_and_cancellation, tweets.money, tweets.timestamp_ms\n",
    "    FROM tweets \n",
    "    JOIN hasher ON tweets.id = hasher.id\n",
    "    JOIN conversations ON hasher.conversation_id = conversations.conversation_id\n",
    "    WHERE conversations.conversation_id IN (\n",
    "        SELECT conversation_id\n",
    "        FROM conversations\n",
    "        WHERE airline LIKE '%Lufthansa%'\n",
    "            AND timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "            AND timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "df_lh1 = pd.read_sql(conv_lh1, connection)\n",
    "df_lh2 = pd.read_sql(conv_lh2, connection)\n",
    "\n",
    "df_lh = pd.concat([df_lh1, df_lh2], ignore_index=True, axis=0)\n",
    "df_lh = df_lh.drop_duplicates()\n",
    "\n",
    "df_lh['datetime'] = pd.to_datetime(df_lh['timestamp_ms'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0s\n",
    "\n",
    "columns_drop = ['text', 'id', 'timestamp_ms', 'datetime']\n",
    "\n",
    "df_topics_ba = df_ba.drop(columns=columns_drop)\n",
    "df_topics_af = df_af.drop(columns=columns_drop)\n",
    "df_topics_klm = df_klm.drop(columns=columns_drop)\n",
    "df_topics_lh = df_lh.drop(columns=columns_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0s\n",
    "\n",
    "counts_ba = df_topics_ba.apply(lambda col: col.value_counts().get(1, 0))\n",
    "counts_af = df_topics_af.apply(lambda col: col.value_counts().get(1, 0))\n",
    "counts_klm = df_topics_klm.apply(lambda col: col.value_counts().get(1, 0))\n",
    "counts_lh = df_topics_lh.apply(lambda col: col.value_counts().get(1, 0))\n",
    "\n",
    "counts_ba_sum = counts_ba.sum()\n",
    "counts_af_sum = counts_af.sum()\n",
    "counts_klm_sum = counts_klm.sum()\n",
    "counts_lh_sum = counts_lh.sum()\n",
    "\n",
    "percentages_ba = round((counts_ba / counts_ba_sum) * 100, 2)\n",
    "percentages_af = round((counts_af / counts_af_sum) * 100, 2)\n",
    "percentages_klm = round((counts_klm / counts_klm_sum) * 100, 2)\n",
    "percentages_lh = round((counts_lh / counts_lh_sum) * 100, 2)\n",
    "\n",
    "percentages = pd.DataFrame({\n",
    "    'Airline': ['British Airways', 'AirFrance', 'KLM', 'Lufthansa'],\n",
    "    'Staff': [percentages_ba[0], percentages_af[0], percentages_klm[0], percentages_lh[0]],\n",
    "    'Baggage': [percentages_ba[1], percentages_af[1], percentages_klm[1], percentages_lh[1]],\n",
    "    'Delay and Cancellation': [percentages_ba[2], percentages_af[2], percentages_klm[2], percentages_lh[2]],\n",
    "    'Money': [percentages_ba[3], percentages_af[3], percentages_klm[3], percentages_lh[3]]\n",
    "})\n",
    "\n",
    "ax =percentages.set_index('Airline').plot(kind='bar', stacked=True, figsize=(10, 6), color=custom_colors)\n",
    "\n",
    "for p in ax.patches:\n",
    "    width, height = p.get_width(), p.get_height()\n",
    "    x, y = p.get_xy()\n",
    "    if height > 0:\n",
    "        ax.annotate(f'{height:.2f}%', (x + width/2, y + height/2), ha='center', va='center', fontsize=10, color ='white', weight='bold')\n",
    "\n",
    "plt.xlabel('Airline')\n",
    "plt.ylabel('Percentage of Tweets')\n",
    "plt.title('Percentage of Tweets per Topic by Airline', weight='bold')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title='Topics', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of tweets where an airline was included (mnetioned or responded) per topic- 23s\n",
    "\n",
    "conv_topics1 = f\"\"\"\n",
    "    SELECT text, id, staff, baggage, delay_and_cancellation, money, timestamp_ms\n",
    "    FROM tweets \n",
    "    WHERE (mentioned_airlines LIKE '%Lufthansa%' \n",
    "        OR mentioned_airlines LIKE '%British_Airways%' \n",
    "        OR mentioned_airlines LIKE '%KLM%' \n",
    "        OR mentioned_airlines LIKE '%AirFrance%')\n",
    "        AND timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "        AND timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000;\n",
    "\"\"\"\n",
    "conv_topics2 = f\"\"\"\n",
    "    SELECT tweets.text, tweets.id, tweets.staff, tweets.baggage, tweets.delay_and_cancellation, tweets.money, tweets.timestamp_ms\n",
    "    FROM tweets \n",
    "    JOIN hasher ON tweets.id = hasher.id\n",
    "    JOIN conversations ON hasher.conversation_id = conversations.conversation_id\n",
    "    WHERE conversations.conversation_id IN (\n",
    "        SELECT conversation_id\n",
    "        FROM conversations\n",
    "        WHERE (airline LIKE '%Lufthansa%' \n",
    "            OR airline LIKE '%British_Airways%' \n",
    "            OR airline LIKE '%KLM%' \n",
    "            OR airline LIKE '%AirFrance%')\n",
    "            AND start >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "            AND start <= UNIX_TIMESTAMP('{end_date}') * 1000\n",
    "    );\n",
    "\"\"\"\n",
    "df_topics1 = pd.read_sql(conv_topics1, connection)\n",
    "df_topics2 = pd.read_sql(conv_topics2, connection)\n",
    "\n",
    "df_topics = pd.concat([df_topics1, df_topics2], ignore_index=True, axis=0)\n",
    "df_topics = df_topics.drop_duplicates()\n",
    "\n",
    "df_topics['datetime'] = pd.to_datetime(df_topics['timestamp_ms'], unit='ms')\n",
    "\n",
    "df_topics = df_topics.drop(columns=columns_drop)\n",
    "\n",
    "counts_topics = df_topics.apply(lambda col: col.value_counts().get(1, 0))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "counts_topics.plot(kind='bar', color=sns.color_palette('colorblind'))\n",
    "plt.xlabel('topics')\n",
    "plt.ylabel('number of tweets')\n",
    "plt.title('Number of tweets per topic', weight='bold')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ba = df[df['mentioned_airlines'].apply(lambda x: 'British_Airways' in x) | df['user_mentions'].apply(lambda x: '18332190' in x)]\n",
    "\n",
    "categories = ['baggage', 'money', 'staff', 'delay_and_cancellation']\n",
    "sentiments = ['positive', 'neutral', 'negative']\n",
    "sentiment_colors = sns.color_palette(\"muted\", 3)\n",
    "\n",
    "# Calculate sentiment percentages for each category\n",
    "sentiment_percentages = {category: [] for category in categories}\n",
    "total_counts = {category: 0 for category in categories}\n",
    "\n",
    "for category in categories:\n",
    "    category_data = df_ba[df_ba[category] == 1]\n",
    "    total_count = category_data.shape[0]\n",
    "    total_counts[category] = total_count\n",
    "    for sentiment in sentiments:\n",
    "        sentiment_count = category_data[category_data['label'] == sentiment].shape[0]\n",
    "        sentiment_percentage = (sentiment_count / total_count) * 100 if total_count > 0 else 0\n",
    "        sentiment_percentages[category].append(sentiment_percentage)\n",
    "\n",
    "# Custom color palette\n",
    "custom_colors = ['#0D2859', '#D83B33', '#A7A9AC']\n",
    "\n",
    "# Prepare the DataFrame for plotting\n",
    "plot_data = pd.DataFrame(sentiment_percentages, index=sentiments).T\n",
    "\n",
    "# Plot the horizontal stacked bar chart\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottoms = [0] * len(categories)\n",
    "\n",
    "for sentiment, color in zip(sentiments, custom_colors):\n",
    "    ax.barh(categories, plot_data[sentiment], left=bottoms, label=sentiment.capitalize(), color=color)\n",
    "    bottoms = [i + j for i, j in zip(bottoms, plot_data[sentiment])]\n",
    "\n",
    "    # Annotate percentages on the bars\n",
    "    for i, (percent, total) in enumerate(zip(plot_data[sentiment], total_counts.values())):\n",
    "        if total > 0:\n",
    "            ax.text(bottoms[i] - percent / 2, i, f'{percent:.1f}%', va='center', ha='center', fontsize=12, color='white', weight='bold')\n",
    "\n",
    "ax.set_xlabel('Percentage', fontsize=14)\n",
    "ax.set_title('Sentiment Analysis for Different Categories in British Airways', fontsize=20, weight='bold')\n",
    "ax.legend(title='Sentiment', bbox_to_anchor=(1.05, 1), loc='upper left', prop={'size': 15})\n",
    "ax.set_yticklabels([category.capitalize().replace('_', ' ') for category in categories])\n",
    "ax.tick_params(axis='both', which='major', labelsize=13)\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ba = df[df['mentioned_airlines'].apply(lambda x: 'AirFrance' in x) | df['user_mentions'].apply(lambda x: '106062176' in x)]\n",
    "\n",
    "categories = ['baggage', 'money', 'staff', 'delay_and_cancellation']\n",
    "sentiments = ['positive', 'neutral', 'negative']\n",
    "sentiment_colors = sns.color_palette(\"muted\", 3)\n",
    "\n",
    "# Calculate sentiment percentages for each category\n",
    "sentiment_percentages = {category: [] for category in categories}\n",
    "total_counts = {category: 0 for category in categories}\n",
    "\n",
    "for category in categories:\n",
    "    category_data = df_ba[df_ba[category] == 1]\n",
    "    total_count = category_data.shape[0]\n",
    "    total_counts[category] = total_count\n",
    "    for sentiment in sentiments:\n",
    "        sentiment_count = category_data[category_data['label'] == sentiment].shape[0]\n",
    "        sentiment_percentage = (sentiment_count / total_count) * 100 if total_count > 0 else 0\n",
    "        sentiment_percentages[category].append(sentiment_percentage)\n",
    "\n",
    "# Custom color palette\n",
    "custom_colors = ['#0D2859', '#D83B33', '#A7A9AC']\n",
    "\n",
    "# Prepare the DataFrame for plotting\n",
    "plot_data = pd.DataFrame(sentiment_percentages, index=sentiments).T\n",
    "\n",
    "# Plot the horizontal stacked bar chart\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottoms = [0] * len(categories)\n",
    "\n",
    "for sentiment, color in zip(sentiments, custom_colors):\n",
    "    ax.barh(categories, plot_data[sentiment], left=bottoms, label=sentiment.capitalize(), color=color)\n",
    "    bottoms = [i + j for i, j in zip(bottoms, plot_data[sentiment])]\n",
    "\n",
    "    # Annotate percentages on the bars\n",
    "    for i, (percent, total) in enumerate(zip(plot_data[sentiment], total_counts.values())):\n",
    "        if total > 0:\n",
    "            ax.text(bottoms[i] - percent / 2, i, f'{percent:.1f}%', va='center', ha='center', fontsize=12, color='white', weight='bold')\n",
    "\n",
    "ax.set_xlabel('Percentage', fontsize=14)\n",
    "ax.set_title('Sentiment Analysis for Different Categories in AirFrance', fontsize=20, weight='bold')\n",
    "ax.legend(title='Sentiment', bbox_to_anchor=(1.05, 1), loc='upper left', prop={'size': 15})\n",
    "ax.set_yticklabels([category.capitalize().replace('_', ' ') for category in categories])\n",
    "ax.tick_params(axis='both', which='major', labelsize=13)\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ba = df[df['mentioned_airlines'].apply(lambda x: 'KLM' in x) | df['user_mentions'].apply(lambda x: '56377143' in x)]\n",
    "\n",
    "categories = ['baggage', 'money', 'staff', 'delay_and_cancellation']\n",
    "sentiments = ['positive', 'neutral', 'negative']\n",
    "sentiment_colors = sns.color_palette(\"muted\", 3)\n",
    "\n",
    "# Calculate sentiment percentages for each category\n",
    "sentiment_percentages = {category: [] for category in categories}\n",
    "total_counts = {category: 0 for category in categories}\n",
    "\n",
    "for category in categories:\n",
    "    category_data = df_ba[df_ba[category] == 1]\n",
    "    total_count = category_data.shape[0]\n",
    "    total_counts[category] = total_count\n",
    "    for sentiment in sentiments:\n",
    "        sentiment_count = category_data[category_data['label'] == sentiment].shape[0]\n",
    "        sentiment_percentage = (sentiment_count / total_count) * 100 if total_count > 0 else 0\n",
    "        sentiment_percentages[category].append(sentiment_percentage)\n",
    "\n",
    "# Custom color palette\n",
    "custom_colors = ['#0D2859', '#D83B33', '#A7A9AC']\n",
    "\n",
    "# Prepare the DataFrame for plotting\n",
    "plot_data = pd.DataFrame(sentiment_percentages, index=sentiments).T\n",
    "\n",
    "# Plot the horizontal stacked bar chart\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottoms = [0] * len(categories)\n",
    "\n",
    "for sentiment, color in zip(sentiments, custom_colors):\n",
    "    ax.barh(categories, plot_data[sentiment], left=bottoms, label=sentiment.capitalize(), color=color)\n",
    "    bottoms = [i + j for i, j in zip(bottoms, plot_data[sentiment])]\n",
    "\n",
    "    # Annotate percentages on the bars\n",
    "    for i, (percent, total) in enumerate(zip(plot_data[sentiment], total_counts.values())):\n",
    "        if total > 0:\n",
    "            ax.text(bottoms[i] - percent / 2, i, f'{percent:.1f}%', va='center', ha='center', fontsize=12, color='white', weight='bold')\n",
    "\n",
    "ax.set_xlabel('Percentage', fontsize=14)\n",
    "ax.set_title('Sentiment Analysis for Different Categories in KLM', fontsize=20, weight='bold')\n",
    "ax.legend(title='Sentiment', bbox_to_anchor=(1.05, 1), loc='upper left', prop={'size': 15})\n",
    "ax.set_yticklabels([category.capitalize().replace('_', ' ') for category in categories])\n",
    "ax.tick_params(axis='both', which='major', labelsize=13)\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ba = df[df['mentioned_airlines'].apply(lambda x: 'Lufthansa' in x) | df['user_mentions'].apply(lambda x: '124476322' in x)]\n",
    "\n",
    "categories = ['baggage', 'money', 'staff', 'delay_and_cancellation']\n",
    "sentiments = ['positive', 'neutral', 'negative']\n",
    "sentiment_colors = sns.color_palette(\"muted\", 3)\n",
    "\n",
    "# Calculate sentiment percentages for each category\n",
    "sentiment_percentages = {category: [] for category in categories}\n",
    "total_counts = {category: 0 for category in categories}\n",
    "\n",
    "for category in categories:\n",
    "    category_data = df_ba[df_ba[category] == 1]\n",
    "    total_count = category_data.shape[0]\n",
    "    total_counts[category] = total_count\n",
    "    for sentiment in sentiments:\n",
    "        sentiment_count = category_data[category_data['label'] == sentiment].shape[0]\n",
    "        sentiment_percentage = (sentiment_count / total_count) * 100 if total_count > 0 else 0\n",
    "        sentiment_percentages[category].append(sentiment_percentage)\n",
    "\n",
    "# Custom color palette\n",
    "custom_colors = ['#0D2859', '#D83B33', '#A7A9AC']\n",
    "\n",
    "# Prepare the DataFrame for plotting\n",
    "plot_data = pd.DataFrame(sentiment_percentages, index=sentiments).T\n",
    "\n",
    "# Plot the horizontal stacked bar chart\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottoms = [0] * len(categories)\n",
    "\n",
    "for sentiment, color in zip(sentiments, custom_colors):\n",
    "    ax.barh(categories, plot_data[sentiment], left=bottoms, label=sentiment.capitalize(), color=color)\n",
    "    bottoms = [i + j for i, j in zip(bottoms, plot_data[sentiment])]\n",
    "\n",
    "    # Annotate percentages on the bars\n",
    "    for i, (percent, total) in enumerate(zip(plot_data[sentiment], total_counts.values())):\n",
    "        if total > 0:\n",
    "            ax.text(bottoms[i] - percent / 2, i, f'{percent:.1f}%', va='center', ha='center', fontsize=12, color='white', weight='bold')\n",
    "\n",
    "ax.set_xlabel('Percentage', fontsize=14)\n",
    "ax.set_title('Sentiment Analysis for Different Categories in Lufthansa', fontsize=20, weight='bold')\n",
    "ax.legend(title='Sentiment', bbox_to_anchor=(1.05, 1), loc='upper left', prop={'size': 15})\n",
    "ax.set_yticklabels([category.capitalize().replace('_', ' ') for category in categories])\n",
    "ax.tick_params(axis='both', which='major', labelsize=13)\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ba = df[df['mentioned_airlines'].apply(lambda x: 'British_Airways' in x) | df['user_mentions'].apply(lambda x: '18332190' in x)]\n",
    "\n",
    "# categories = ['baggage', 'money', 'staff', 'delay_and_cancellation']\n",
    "# sentiments = ['positive', 'neutral', 'negative']\n",
    "# sentiment_colors = sns.color_palette(\"muted\", 3)\n",
    "\n",
    "# # Calculate sentiment percentages for each category\n",
    "# sentiment_percentages = {category: [] for category in categories}\n",
    "# total_counts = {category: 0 for category in categories}\n",
    "\n",
    "# for category in categories:\n",
    "#     category_data = df_ba[df_ba[category] == 1]\n",
    "#     total_count = category_data.shape[0]\n",
    "#     total_counts[category] = total_count\n",
    "#     for sentiment in sentiments:\n",
    "#         sentiment_count = category_data[category_data['label'] == sentiment].shape[0]\n",
    "#         sentiment_percentage = (sentiment_count / total_count) * 100 if total_count > 0 else 0\n",
    "#         sentiment_percentages[category].append(sentiment_percentage)\n",
    "\n",
    "# # Prepare the DataFrame for plotting\n",
    "# plot_data = pd.DataFrame(sentiment_percentages, index=sentiments).T\n",
    "\n",
    "# # Plot the horizontal stacked bar chart\n",
    "# fig, ax = plt.subplots(figsize=(20, 10))\n",
    "# bottoms = [0] * len(categories)\n",
    "\n",
    "# for sentiment, color in zip(sentiments, sentiment_colors):\n",
    "#     ax.barh(categories, plot_data[sentiment], left=bottoms, label=sentiment.capitalize(), color=color)\n",
    "#     bottoms = [i + j for i, j in zip(bottoms, plot_data[sentiment])]\n",
    "\n",
    "#     # Annotate percentages on the bars\n",
    "#     for i, (percent, total) in enumerate(zip(plot_data[sentiment], total_counts.values())):\n",
    "#         if total > 0:\n",
    "#             ax.text(bottoms[i] - percent / 2, i, f'{percent:.1f}%', va='center', ha='center', fontsize=12, color='black', weight='bold')\n",
    "\n",
    "# ax.set_xlabel('Percentage', fontsize=14)\n",
    "# ax.set_title('Sentiment Analysis for Different Categories in British Airways', fontsize=20, weight='bold')\n",
    "# ax.legend(title='Sentiment', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "# ax.tick_params(axis='both', which='major', labelsize=13)\n",
    "# plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ba = df[df['mentioned_airlines'].apply(lambda x: 'British_Airways' in x) | df['user_mentions'].apply(lambda x: '18332190' in x)]\n",
    "\n",
    "categories = ['baggage', 'money', 'staff', 'delay_and_cancellation']\n",
    "sentiments = ['positive', 'neutral', 'negative']\n",
    "sentiment_colors = sns.color_palette(\"muted\", 3)\n",
    "\n",
    "# Calculate sentiment counts for each category\n",
    "sentiment_counts = {category: {sentiment: 0 for sentiment in sentiments} for category in categories}\n",
    "total_counts = {category: 0 for category in categories}\n",
    "\n",
    "for category in categories:\n",
    "    category_data = df_ba[df_ba[category] == 1]\n",
    "    total_count = category_data.shape[0]\n",
    "    total_counts[category] = total_count\n",
    "    for sentiment in sentiments:\n",
    "        sentiment_count = category_data[category_data['label'] == sentiment].shape[0]\n",
    "        sentiment_counts[category][sentiment] = sentiment_count\n",
    "\n",
    "# Sort categories by total counts in descending order\n",
    "sorted_categories = sorted(total_counts, key=total_counts.get, reverse=False)\n",
    "\n",
    "# Prepare the DataFrame for plotting\n",
    "plot_data = pd.DataFrame(sentiment_counts).T.loc[sorted_categories]\n",
    "\n",
    "# Plot the horizontal stacked bar chart\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottoms = [0] * len(categories)\n",
    "\n",
    "for sentiment, color in zip(sentiments, sentiment_colors):\n",
    "    sentiment_values = plot_data[sentiment].values\n",
    "    ax.barh(plot_data.index, sentiment_values, left=bottoms, label=sentiment.capitalize(), color=color)\n",
    "    bottoms = [i + j for i, j in zip(bottoms, sentiment_values)]\n",
    "\n",
    "    # Annotate percentages on the bars\n",
    "    for i, (value, total) in enumerate(zip(sentiment_values, total_counts.values())):\n",
    "        if total > 0:\n",
    "            percent = (value / total_counts[plot_data.index[i]]) * 100 if total_counts[plot_data.index[i]] > 0 else 0\n",
    "            if value > 0:  # Only annotate if there's a value to display\n",
    "                ax.text(bottoms[i] - value / 2, i, f'{percent:.1f}%', va='center', ha='center', fontsize=12, color='black', weight='bold')\n",
    "\n",
    "# Annotate total counts at the end of bars\n",
    "for i, category in enumerate(plot_data.index):\n",
    "    ax.text(bottoms[i] + 200, i, f'{total_counts[category]}', va='center', ha='left', fontsize=12, color='black', weight='bold',\n",
    "            bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "# Extend the x-axis limits\n",
    "max_total_sentiments = max(total_counts.values())\n",
    "ax.set_xlim(0, max_total_sentiments + 1000)  # Extend the x-axis by an additional 1000 units\n",
    "\n",
    "ax.set_xlabel('Number of Sentiments', fontsize=14)\n",
    "ax.set_title('Sentiment Analysis for Different Categories in British Airways', fontsize=20, weight='bold')\n",
    "ax.legend(title='Sentiment', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax.tick_params(axis='both', which='major', labelsize=13)\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis for different topic in British Airrways- 2s\n",
    "\n",
    "df_ba = df[df['mentioned_airlines'].apply(lambda x: 'British_Airways' in x) | df['user_mentions'].apply(lambda x: '18332190' in x)]\n",
    "\n",
    "# Plot settings\n",
    "fig, axs = plt.subplots(1, 4, figsize=(40, 10))\n",
    "fig.suptitle('Sentiment Analysis for Different Categories in British Airways', fontsize=35)\n",
    "\n",
    "categories = ['baggage', 'money', 'staff', 'delay_and_cancellation']\n",
    "sentiments = ['positive', 'neutral', 'negative']\n",
    "\n",
    "for ax, category in zip(axs.flatten(), categories):\n",
    "    category_data = df_ba[df_ba[category] == 1]\n",
    "    \n",
    "    sentiment_counts = category_data['label'].value_counts().reindex(sentiments, fill_value=0)\n",
    "    \n",
    "    sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette='Blues', ax=ax)\n",
    "    \n",
    "    for p in ax.patches:\n",
    "        ax.annotate(format(p.get_height(), '.0f'), \n",
    "                    (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                    ha='center', va='center', \n",
    "                    xytext=(0, 9), \n",
    "                    textcoords='offset points',\n",
    "                    fontsize=20, weight='bold', color='black')\n",
    "    \n",
    "    ax.set_title(category.capitalize().replace('_', ' '), fontsize=25, weight='bold')\n",
    "    ax.set_xlabel('Sentiment', fontsize=20)\n",
    "    ax.set_ylabel('Frequency', fontsize=20)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit the suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly distribution of sentiment labels- 2s \n",
    "\n",
    "df['AirFrance'] = df['mentioned_airlines'].apply(lambda x: 'AirFrance' in x) | df['user_mentions'].apply(lambda x: '106062176' in x)\n",
    "df['Lufthansa'] = df['mentioned_airlines'].apply(lambda x: 'Lufthansa' in x) | df['user_mentions'].apply(lambda x: '124476322' in x)\n",
    "df['KLM'] = df['mentioned_airlines'].apply(lambda x: 'KLM' in x) | df['user_mentions'].apply(lambda x: '56377143' in x)\n",
    "df['British_Airways'] = df['mentioned_airlines'].apply(lambda x: 'British_Airways' in x) | df['user_mentions'].apply(lambda x: '18332190' in x)\n",
    "\n",
    "monthly_counts = {sentiment: {airline: [] for airline in airlines_2} for sentiment in sentiments}\n",
    "\n",
    "for sentiment in sentiments:\n",
    "    for airline in airlines_2:\n",
    "        monthly_count = df[df[airline] & (df['label'] == sentiment)].groupby('month').size().reindex(range(1, 12), fill_value=0)\n",
    "        monthly_counts[sentiment][airline] = monthly_count\n",
    "\n",
    "df_positive = pd.DataFrame(monthly_counts['positive'])\n",
    "df_neutral = pd.DataFrame(monthly_counts['neutral'])\n",
    "df_negative = pd.DataFrame(monthly_counts['negative'])\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(14, 18), sharex=True)\n",
    "\n",
    "sns.lineplot(data=df_positive, linewidth=2.5, ax=axs[0])\n",
    "axs[0].set_title('Monthly Distribution of Positive Labels', fontsize=16, weight='bold')\n",
    "axs[0].set_ylabel('Number of Labels', fontsize=14)\n",
    "axs[0].legend(title='Airlines', fontsize=12, title_fontsize='13', loc='upper right')\n",
    "axs[0].grid(True)\n",
    "\n",
    "sns.lineplot(data=df_neutral, linewidth=2.5, ax=axs[1])\n",
    "axs[1].set_title('Monthly Distribution of Neutral Labels', fontsize=16, weight='bold')\n",
    "axs[1].set_ylabel('Number of Labels', fontsize=14)\n",
    "axs[1].legend(title='Airlines', fontsize=12, title_fontsize='13', loc='upper right')\n",
    "axs[1].grid(True)\n",
    "\n",
    "sns.lineplot(data=df_negative, linewidth=2.5, ax=axs[2])\n",
    "axs[2].set_title('Monthly Distribution of Negative Labels', fontsize=16, weight='bold')\n",
    "axs[2].set_ylabel('Number of Labels', fontsize=14)\n",
    "axs[2].legend(title='Airlines', fontsize=12, title_fontsize='13', loc='upper right')\n",
    "axs[2].grid(True)\n",
    "\n",
    "axs[2].set_xlabel('Month of the year', fontsize=14)\n",
    "axs[2].set_xticks(range(1, 13))\n",
    "axs[2].set_xticklabels(['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'], rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the airline user ID\n",
    "airline_user_id = 18332190\n",
    "\n",
    "# Define the query to get all tweets in the conversations involving British Airways\n",
    "query_sankey_baggage = f\"\"\"\n",
    "SELECT tweets.text, tweets.label, tweets.user_id, tweets.timestamp_ms, hasher.conversation_id, hasher.conversation_rank\n",
    "FROM tweets\n",
    "JOIN hasher ON hasher.id = tweets.id\n",
    "JOIN conversations ON hasher.conversation_id = conversations.conversation_id\n",
    "WHERE conversations.airline LIKE '%British_Airways%' \n",
    "    AND tweets.baggage = 1\n",
    "    AND tweets.timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "    AND tweets.timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000\n",
    "ORDER BY tweets.timestamp_ms;\n",
    "\"\"\"\n",
    "\n",
    "# Load the result of the query into a DataFrame\n",
    "df_tweets_british_airways_sankey_baggage = pd.read_sql(query_sankey_baggage, connection)\n",
    "\n",
    "# Convert the datetime column to pandas datetime\n",
    "df_tweets_british_airways_sankey_baggage['datetime'] = pd.to_datetime(df_tweets_british_airways_sankey_baggage['timestamp_ms'], unit='ms')\n",
    "\n",
    "df_tweets_british_airways_sankey_baggage['datetime'] = df_tweets_british_airways_sankey_baggage['datetime'].dt.tz_localize('UTC').dt.tz_convert('Europe/Amsterdam')\n",
    "\n",
    "# Initialize empty lists to store sentiments\n",
    "sentiments_initial_tweet = []\n",
    "sentiments_post_reply = []\n",
    "\n",
    "# Group tweets by conversation_id\n",
    "grouped_sankey_baggage = df_tweets_british_airways_sankey_baggage.groupby('conversation_id')\n",
    "\n",
    "# Process each conversation\n",
    "for conversation_id, group in grouped_sankey_baggage:\n",
    "    # Identify the initial user's tweet (rank 1)\n",
    "    user_initial_tweet = group[(group['user_id'] != airline_user_id) & (group['conversation_rank'] == 1)]\n",
    "    \n",
    "    if not user_initial_tweet.empty:\n",
    "        user_id = user_initial_tweet['user_id'].iloc[0]\n",
    "        initial_tweet_time = user_initial_tweet['datetime'].iloc[0]\n",
    "\n",
    "        # Identify the airline's reply to this initial tweet\n",
    "        airline_reply = group[(group['user_id'] == airline_user_id) & (group['datetime'] > initial_tweet_time)]\n",
    "        if not airline_reply.empty:\n",
    "            airline_reply_time = airline_reply['datetime'].iloc[0]\n",
    "\n",
    "            # Identify the user's reply to the airline's response\n",
    "            user_followup_tweet = group[(group['user_id'] == user_id) & (group['datetime'] > airline_reply_time)]\n",
    "            if not user_followup_tweet.empty:\n",
    "                # Append sentiments for analysis\n",
    "                sentiments_initial_tweet.append(user_initial_tweet['label'].iloc[0])\n",
    "                sentiments_post_reply.append(user_followup_tweet['label'].iloc[0])\n",
    "\n",
    "# Step 1: Prepare data for Sankey diagram\n",
    "transitions = pd.DataFrame({\n",
    "    'initial': sentiments_initial_tweet,\n",
    "    'post_reply': sentiments_post_reply\n",
    "})\n",
    "\n",
    "# Count transitions\n",
    "transition_counts = transitions.value_counts().reset_index(name='count')\n",
    "\n",
    "# Add percentage to transition_counts\n",
    "total_counts = transition_counts['count'].sum()\n",
    "transition_counts['percentage'] = (transition_counts['count'] / total_counts * 100).round(2)\n",
    "\n",
    "# Define nodes (initial sentiments and final grouped sentiments)\n",
    "nodes = ['positive', 'negative', 'neutral', 'positive_end', 'negative_end', 'neutral_end']\n",
    "node_indices = {label: i for i, label in enumerate(nodes)}\n",
    "\n",
    "# Prepare data for Sankey diagram\n",
    "source = [node_indices[row['initial']] for _, row in transition_counts.iterrows()]\n",
    "target = [node_indices[f\"{row['post_reply']}_end\"] for _, row in transition_counts.iterrows()]\n",
    "value = transition_counts['count'].tolist()\n",
    "label = [f\"{row['initial']} to {row['post_reply']}: {row['count']} ({row['percentage']}%)\" for _, row in transition_counts.iterrows()]\n",
    "\n",
    "# Define colors for nodes and links\n",
    "node_colors = {\n",
    "    'positive': 'rgb(44, 160, 44)',  # green\n",
    "    'negative': 'rgb(214, 39, 40)',  # red\n",
    "    'neutral': 'rgb(255, 165, 0)',   # orange\n",
    "    'positive_end': 'rgb(44, 160, 44)',  # green for final positive\n",
    "    'negative_end': 'rgb(214, 39, 40)',  # red for final negative\n",
    "    'neutral_end': 'rgb(255, 165, 0)'    # orange for final neutral\n",
    "}\n",
    "\n",
    "transition_colors = {\n",
    "    'positive': 'rgba(44, 160, 44, 0.4)',  # light green\n",
    "    'negative': 'rgba(214, 39, 40, 0.4)',  # light red\n",
    "    'neutral': 'rgba(255, 165, 0, 0.4)'    # light orange\n",
    "}\n",
    "\n",
    "# Create color list for nodes\n",
    "colors = [node_colors[node] for node in nodes]\n",
    "\n",
    "# Create the Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=30,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=['Positive', 'Negative', 'Neutral', 'Positive End', 'Negative End', 'Neutral End'],\n",
    "        color=colors\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value,\n",
    "        color=[transition_colors[row['initial']] for _, row in transition_counts.iterrows()],  # Use bland colors for transitions\n",
    "        customdata=label,\n",
    "        hovertemplate='%{customdata}<extra></extra>',\n",
    "        label=label  # Add labels directly to the links\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Sentiment Transition (British Airways, baggage)\", \n",
    "    font_size=12,\n",
    "    width=1200,  # increased width to accommodate the legend\n",
    "    height=700,\n",
    "    margin=dict(l=50, r=50, t=50, b=50)  # Adjusted margins\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the airline user ID\n",
    "airline_user_id = 18332190\n",
    "\n",
    "# Define the query to get all tweets in the conversations involving British Airways\n",
    "query_sankey_money = f\"\"\"\n",
    "SELECT tweets.text, tweets.label, tweets.user_id, tweets.timestamp_ms, hasher.conversation_id, hasher.conversation_rank\n",
    "FROM tweets\n",
    "JOIN hasher ON hasher.id = tweets.id\n",
    "JOIN conversations ON hasher.conversation_id = conversations.conversation_id\n",
    "WHERE conversations.airline LIKE '%British_Airways%' \n",
    "    AND tweets.money = 1\n",
    "    AND tweets.timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "    AND tweets.timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000\n",
    "ORDER BY tweets.timestamp_ms;\n",
    "\"\"\"\n",
    "\n",
    "# Load the result of the query into a DataFrame\n",
    "df_tweets_british_airways_sankey_money = pd.read_sql(query_sankey_money, connection)\n",
    "\n",
    "# Convert the datetime column to pandas datetime\n",
    "df_tweets_british_airways_sankey_money['datetime'] = pd.to_datetime(df_tweets_british_airways_sankey_money['timestamp_ms'], unit='ms')\n",
    "\n",
    "df_tweets_british_airways_sankey_money['datetime'] = df_tweets_british_airways_sankey_money['datetime'].dt.tz_localize('UTC').dt.tz_convert('Europe/Amsterdam')\n",
    "\n",
    "# Initialize empty lists to store sentiments\n",
    "sentiments_initial_tweet = []\n",
    "sentiments_post_reply = []\n",
    "\n",
    "# Group tweets by conversation_id\n",
    "grouped_sankey_money = df_tweets_british_airways_sankey_money.groupby('conversation_id')\n",
    "\n",
    "# Process each conversation\n",
    "for conversation_id, group in grouped_sankey_money:\n",
    "    # Identify the initial user's tweet (rank 1)\n",
    "    user_initial_tweet = group[(group['user_id'] != airline_user_id) & (group['conversation_rank'] == 1)]\n",
    "    \n",
    "    if not user_initial_tweet.empty:\n",
    "        user_id = user_initial_tweet['user_id'].iloc[0]\n",
    "        initial_tweet_time = user_initial_tweet['datetime'].iloc[0]\n",
    "\n",
    "        # Identify the airline's reply to this initial tweet\n",
    "        airline_reply = group[(group['user_id'] == airline_user_id) & (group['datetime'] > initial_tweet_time)]\n",
    "        if not airline_reply.empty:\n",
    "            airline_reply_time = airline_reply['datetime'].iloc[0]\n",
    "\n",
    "            # Identify the user's reply to the airline's response\n",
    "            user_followup_tweet = group[(group['user_id'] == user_id) & (group['datetime'] > airline_reply_time)]\n",
    "            if not user_followup_tweet.empty:\n",
    "                # Append sentiments for analysis\n",
    "                sentiments_initial_tweet.append(user_initial_tweet['label'].iloc[0])\n",
    "                sentiments_post_reply.append(user_followup_tweet['label'].iloc[0])\n",
    "\n",
    "# Step 1: Prepare data for Sankey diagram\n",
    "transitions = pd.DataFrame({\n",
    "    'initial': sentiments_initial_tweet,\n",
    "    'post_reply': sentiments_post_reply\n",
    "})\n",
    "\n",
    "# Count transitions\n",
    "transition_counts = transitions.value_counts().reset_index(name='count')\n",
    "\n",
    "# Add percentage to transition_counts\n",
    "total_counts = transition_counts['count'].sum()\n",
    "transition_counts['percentage'] = (transition_counts['count'] / total_counts * 100).round(2)\n",
    "\n",
    "# Define nodes (initial sentiments and final grouped sentiments)\n",
    "nodes = ['positive', 'negative', 'neutral', 'positive_end', 'negative_end', 'neutral_end']\n",
    "node_indices = {label: i for i, label in enumerate(nodes)}\n",
    "\n",
    "# Prepare data for Sankey diagram\n",
    "source = [node_indices[row['initial']] for _, row in transition_counts.iterrows()]\n",
    "target = [node_indices[f\"{row['post_reply']}_end\"] for _, row in transition_counts.iterrows()]\n",
    "value = transition_counts['count'].tolist()\n",
    "label = [f\"{row['initial']} to {row['post_reply']}: {row['count']} ({row['percentage']}%)\" for _, row in transition_counts.iterrows()]\n",
    "\n",
    "# Define colors for nodes and links\n",
    "node_colors = {\n",
    "    'positive': 'rgb(44, 160, 44)',  # green\n",
    "    'negative': 'rgb(214, 39, 40)',  # red\n",
    "    'neutral': 'rgb(255, 165, 0)',   # orange\n",
    "    'positive_end': 'rgb(44, 160, 44)',  # green for final positive\n",
    "    'negative_end': 'rgb(214, 39, 40)',  # red for final negative\n",
    "    'neutral_end': 'rgb(255, 165, 0)'    # orange for final neutral\n",
    "}\n",
    "\n",
    "transition_colors = {\n",
    "    'positive': 'rgba(44, 160, 44, 0.4)',  # light green\n",
    "    'negative': 'rgba(214, 39, 40, 0.4)',  # light red\n",
    "    'neutral': 'rgba(255, 165, 0, 0.4)'    # light orange\n",
    "}\n",
    "\n",
    "# Create color list for nodes\n",
    "colors = [node_colors[node] for node in nodes]\n",
    "\n",
    "# Create the Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=30,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=['Positive', 'Negative', 'Neutral', 'Positive End', 'Negative End', 'Neutral End'],\n",
    "        color=colors\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value,\n",
    "        color=[transition_colors[row['initial']] for _, row in transition_counts.iterrows()],  # Use bland colors for transitions\n",
    "        customdata=label,\n",
    "        hovertemplate='%{customdata}<extra></extra>',\n",
    "        label=label  # Add labels directly to the links\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Sentiment Transition (British Airways, money)\", \n",
    "    font_size=12,\n",
    "    width=1200,  # increased width to accommodate the legend\n",
    "    height=700,\n",
    "    margin=dict(l=50, r=50, t=50, b=50)  # Adjusted margins\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the airline user ID\n",
    "airline_user_id = 18332190\n",
    "\n",
    "# Define the query to get all tweets in the conversations involving British Airways\n",
    "query_sankey_staff = f\"\"\"\n",
    "SELECT tweets.text, tweets.label, tweets.user_id, tweets.timestamp_ms, hasher.conversation_id, hasher.conversation_rank\n",
    "FROM tweets\n",
    "JOIN hasher ON hasher.id = tweets.id\n",
    "JOIN conversations ON hasher.conversation_id = conversations.conversation_id\n",
    "WHERE conversations.airline LIKE '%British_Airways%' \n",
    "    AND tweets.staff = 1\n",
    "    AND tweets.timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "    AND tweets.timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000\n",
    "ORDER BY tweets.timestamp_ms;\n",
    "\"\"\"\n",
    "\n",
    "# Load the result of the query into a DataFrame\n",
    "df_tweets_british_airways_sankey_staff = pd.read_sql(query_sankey_staff, connection)\n",
    "\n",
    "# Convert the datetime column to pandas datetime\n",
    "df_tweets_british_airways_sankey_staff['datetime'] = pd.to_datetime(df_tweets_british_airways_sankey_staff['timestamp_ms'], unit='ms')\n",
    "\n",
    "df_tweets_british_airways_sankey_staff['datetime'] = df_tweets_british_airways_sankey_staff['datetime'].dt.tz_localize('UTC').dt.tz_convert('Europe/Amsterdam')\n",
    "\n",
    "# Initialize empty lists to store sentiments\n",
    "sentiments_initial_tweet = []\n",
    "sentiments_post_reply = []\n",
    "\n",
    "# Group tweets by conversation_id\n",
    "grouped_sankey_staff = df_tweets_british_airways_sankey_staff.groupby('conversation_id')\n",
    "\n",
    "# Process each conversation\n",
    "for conversation_id, group in grouped_sankey_staff:\n",
    "    # Identify the initial user's tweet (rank 1)\n",
    "    user_initial_tweet = group[(group['user_id'] != airline_user_id) & (group['conversation_rank'] == 1)]\n",
    "    \n",
    "    if not user_initial_tweet.empty:\n",
    "        user_id = user_initial_tweet['user_id'].iloc[0]\n",
    "        initial_tweet_time = user_initial_tweet['datetime'].iloc[0]\n",
    "\n",
    "        # Identify the airline's reply to this initial tweet\n",
    "        airline_reply = group[(group['user_id'] == airline_user_id) & (group['datetime'] > initial_tweet_time)]\n",
    "        if not airline_reply.empty:\n",
    "            airline_reply_time = airline_reply['datetime'].iloc[0]\n",
    "\n",
    "            # Identify the user's reply to the airline's response\n",
    "            user_followup_tweet = group[(group['user_id'] == user_id) & (group['datetime'] > airline_reply_time)]\n",
    "            if not user_followup_tweet.empty:\n",
    "                # Append sentiments for analysis\n",
    "                sentiments_initial_tweet.append(user_initial_tweet['label'].iloc[0])\n",
    "                sentiments_post_reply.append(user_followup_tweet['label'].iloc[0])\n",
    "\n",
    "# Step 1: Prepare data for Sankey diagram\n",
    "transitions = pd.DataFrame({\n",
    "    'initial': sentiments_initial_tweet,\n",
    "    'post_reply': sentiments_post_reply\n",
    "})\n",
    "\n",
    "# Count transitions\n",
    "transition_counts = transitions.value_counts().reset_index(name='count')\n",
    "\n",
    "# Add percentage to transition_counts\n",
    "total_counts = transition_counts['count'].sum()\n",
    "transition_counts['percentage'] = (transition_counts['count'] / total_counts * 100).round(2)\n",
    "\n",
    "# Define nodes (initial sentiments and final grouped sentiments)\n",
    "nodes = ['positive', 'negative', 'neutral', 'positive_end', 'negative_end', 'neutral_end']\n",
    "node_indices = {label: i for i, label in enumerate(nodes)}\n",
    "\n",
    "# Prepare data for Sankey diagram\n",
    "source = [node_indices[row['initial']] for _, row in transition_counts.iterrows()]\n",
    "target = [node_indices[f\"{row['post_reply']}_end\"] for _, row in transition_counts.iterrows()]\n",
    "value = transition_counts['count'].tolist()\n",
    "label = [f\"{row['initial']} to {row['post_reply']}: {row['count']} ({row['percentage']}%)\" for _, row in transition_counts.iterrows()]\n",
    "\n",
    "# Define colors for nodes and links\n",
    "node_colors = {\n",
    "    'positive': 'rgb(44, 160, 44)',  # green\n",
    "    'negative': 'rgb(214, 39, 40)',  # red\n",
    "    'neutral': 'rgb(255, 165, 0)',   # orange\n",
    "    'positive_end': 'rgb(44, 160, 44)',  # green for final positive\n",
    "    'negative_end': 'rgb(214, 39, 40)',  # red for final negative\n",
    "    'neutral_end': 'rgb(255, 165, 0)'    # orange for final neutral\n",
    "}\n",
    "\n",
    "transition_colors = {\n",
    "    'positive': 'rgba(44, 160, 44, 0.4)',  # light green\n",
    "    'negative': 'rgba(214, 39, 40, 0.4)',  # light red\n",
    "    'neutral': 'rgba(255, 165, 0, 0.4)'    # light orange\n",
    "}\n",
    "\n",
    "# Create color list for nodes\n",
    "colors = [node_colors[node] for node in nodes]\n",
    "\n",
    "# Create the Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=30,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=['Positive', 'Negative', 'Neutral', 'Positive End', 'Negative End', 'Neutral End'],\n",
    "        color=colors\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value,\n",
    "        color=[transition_colors[row['initial']] for _, row in transition_counts.iterrows()],  # Use bland colors for transitions\n",
    "        customdata=label,\n",
    "        hovertemplate='%{customdata}<extra></extra>',\n",
    "        label=label  # Add labels directly to the links\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Sentiment Transition (British Airways, staff)\", \n",
    "    font_size=12,\n",
    "    width=1200,  # increased width to accommodate the legend\n",
    "    height=700,\n",
    "    margin=dict(l=50, r=50, t=50, b=50)  # Adjusted margins\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the airline user ID\n",
    "airline_user_id = 18332190\n",
    "\n",
    "# Define the query to get all tweets in the conversations involving British Airways\n",
    "query_sankey_delay = f\"\"\"\n",
    "SELECT tweets.text, tweets.label, tweets.user_id, tweets.timestamp_ms, hasher.conversation_id, hasher.conversation_rank\n",
    "FROM tweets\n",
    "JOIN hasher ON hasher.id = tweets.id\n",
    "JOIN conversations ON hasher.conversation_id = conversations.conversation_id\n",
    "WHERE conversations.airline LIKE '%British_Airways%' \n",
    "    AND tweets.delay_and_cancellation = 1\n",
    "    AND tweets.timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "    AND tweets.timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000\n",
    "ORDER BY tweets.timestamp_ms;\n",
    "\"\"\"\n",
    "\n",
    "# Load the result of the query into a DataFrame\n",
    "df_tweets_british_airways_sankey_delay = pd.read_sql(query_sankey_delay, connection)\n",
    "\n",
    "# Convert the datetime column to pandas datetime\n",
    "df_tweets_british_airways_sankey_delay['datetime'] = pd.to_datetime(df_tweets_british_airways_sankey_delay['timestamp_ms'], unit='ms')\n",
    "\n",
    "df_tweets_british_airways_sankey_delay['datetime'] = df_tweets_british_airways_sankey_delay['datetime'].dt.tz_localize('UTC').dt.tz_convert('Europe/Amsterdam')\n",
    "\n",
    "# Initialize empty lists to store sentiments\n",
    "sentiments_initial_tweet = []\n",
    "sentiments_post_reply = []\n",
    "\n",
    "# Group tweets by conversation_id\n",
    "grouped_sankey_delay= df_tweets_british_airways_sankey_delay.groupby('conversation_id')\n",
    "\n",
    "# Process each conversation\n",
    "for conversation_id, group in grouped_sankey_delay:\n",
    "    # Identify the initial user's tweet (rank 1)\n",
    "    user_initial_tweet = group[(group['user_id'] != airline_user_id) & (group['conversation_rank'] == 1)]\n",
    "    \n",
    "    if not user_initial_tweet.empty:\n",
    "        user_id = user_initial_tweet['user_id'].iloc[0]\n",
    "        initial_tweet_time = user_initial_tweet['datetime'].iloc[0]\n",
    "\n",
    "        # Identify the airline's reply to this initial tweet\n",
    "        airline_reply = group[(group['user_id'] == airline_user_id) & (group['datetime'] > initial_tweet_time)]\n",
    "        if not airline_reply.empty:\n",
    "            airline_reply_time = airline_reply['datetime'].iloc[0]\n",
    "\n",
    "            # Identify the user's reply to the airline's response\n",
    "            user_followup_tweet = group[(group['user_id'] == user_id) & (group['datetime'] > airline_reply_time)]\n",
    "            if not user_followup_tweet.empty:\n",
    "                # Append sentiments for analysis\n",
    "                sentiments_initial_tweet.append(user_initial_tweet['label'].iloc[0])\n",
    "                sentiments_post_reply.append(user_followup_tweet['label'].iloc[0])\n",
    "\n",
    "# Step 1: Prepare data for Sankey diagram\n",
    "transitions = pd.DataFrame({\n",
    "    'initial': sentiments_initial_tweet,\n",
    "    'post_reply': sentiments_post_reply\n",
    "})\n",
    "\n",
    "# Count transitions\n",
    "transition_counts = transitions.value_counts().reset_index(name='count')\n",
    "\n",
    "# Add percentage to transition_counts\n",
    "total_counts = transition_counts['count'].sum()\n",
    "transition_counts['percentage'] = (transition_counts['count'] / total_counts * 100).round(2)\n",
    "\n",
    "# Define nodes (initial sentiments and final grouped sentiments)\n",
    "nodes = ['positive', 'negative', 'neutral', 'positive_end', 'negative_end', 'neutral_end']\n",
    "node_indices = {label: i for i, label in enumerate(nodes)}\n",
    "\n",
    "# Prepare data for Sankey diagram\n",
    "source = [node_indices[row['initial']] for _, row in transition_counts.iterrows()]\n",
    "target = [node_indices[f\"{row['post_reply']}_end\"] for _, row in transition_counts.iterrows()]\n",
    "value = transition_counts['count'].tolist()\n",
    "label = [f\"{row['initial']} to {row['post_reply']}: {row['count']} ({row['percentage']}%)\" for _, row in transition_counts.iterrows()]\n",
    "\n",
    "# Define colors for nodes and links\n",
    "node_colors = {\n",
    "    'positive': 'rgb(44, 160, 44)',  # green\n",
    "    'negative': 'rgb(214, 39, 40)',  # red\n",
    "    'neutral': 'rgb(255, 165, 0)',   # orange\n",
    "    'positive_end': 'rgb(44, 160, 44)',  # green for final positive\n",
    "    'negative_end': 'rgb(214, 39, 40)',  # red for final negative\n",
    "    'neutral_end': 'rgb(255, 165, 0)'    # orange for final neutral\n",
    "}\n",
    "\n",
    "transition_colors = {\n",
    "    'positive': 'rgba(44, 160, 44, 0.4)',  # light green\n",
    "    'negative': 'rgba(214, 39, 40, 0.4)',  # light red\n",
    "    'neutral': 'rgba(255, 165, 0, 0.4)'    # light orange\n",
    "}\n",
    "\n",
    "# Create color list for nodes\n",
    "colors = [node_colors[node] for node in nodes]\n",
    "\n",
    "# Create the Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=30,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=['Positive', 'Negative', 'Neutral', 'Positive End', 'Negative End', 'Neutral End'],\n",
    "        color=colors\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value,\n",
    "        color=[transition_colors[row['initial']] for _, row in transition_counts.iterrows()],  # Use bland colors for transitions\n",
    "        customdata=label,\n",
    "        hovertemplate='%{customdata}<extra></extra>',\n",
    "        label=label  # Add labels directly to the links\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Sentiment Transition (British Airways, delay and cancellation)\", \n",
    "    font_size=12,\n",
    "    width=1200,  # increased width to accommodate the legend\n",
    "    height=700,\n",
    "    margin=dict(l=50, r=50, t=50, b=50)  # Adjusted margins\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the airline user ID\n",
    "klm_user_id = 56377143\n",
    "\n",
    "# Define the query to get all tweets in the conversations involving British Airways\n",
    "query_sankey_klm_baggage = f\"\"\"\n",
    "SELECT tweets.text, tweets.label, tweets.user_id, tweets.timestamp_ms, hasher.conversation_id, hasher.conversation_rank\n",
    "FROM tweets\n",
    "JOIN hasher ON hasher.id = tweets.id\n",
    "JOIN conversations ON hasher.conversation_id = conversations.conversation_id\n",
    "WHERE conversations.airline LIKE '%KLM%' \n",
    "    AND tweets.baggage = 1\n",
    "    AND tweets.timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "    AND tweets.timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000\n",
    "ORDER BY tweets.timestamp_ms;\n",
    "\"\"\"\n",
    "\n",
    "# Load the result of the query into a DataFrame\n",
    "df_tweets_klm_sankey_baggage = pd.read_sql(query_sankey_klm_baggage, connection)\n",
    "\n",
    "# Convert the datetime column to pandas datetime\n",
    "df_tweets_klm_sankey_baggage['datetime'] = pd.to_datetime(df_tweets_klm_sankey_baggage['timestamp_ms'], unit='ms')\n",
    "\n",
    "df_tweets_klm_sankey_baggage['datetime'] = df_tweets_klm_sankey_baggage['datetime'].dt.tz_localize('UTC').dt.tz_convert('Europe/Amsterdam')\n",
    "\n",
    "# Initialize empty lists to store sentiments\n",
    "sentiments_initial_tweet = []\n",
    "sentiments_post_reply = []\n",
    "\n",
    "# Group tweets by conversation_id\n",
    "grouped_sankey_klm_baggage = df_tweets_klm_sankey_baggage.groupby('conversation_id')\n",
    "\n",
    "# Process each conversation\n",
    "for conversation_id, group in grouped_sankey_klm_baggage:\n",
    "    # Identify the initial user's tweet (rank 1)\n",
    "    user_initial_tweet = group[(group['user_id'] != klm_user_id) & (group['conversation_rank'] == 1)]\n",
    "    \n",
    "    if not user_initial_tweet.empty:\n",
    "        user_id = user_initial_tweet['user_id'].iloc[0]\n",
    "        initial_tweet_time = user_initial_tweet['datetime'].iloc[0]\n",
    "\n",
    "        # Identify the airline's reply to this initial tweet\n",
    "        airline_reply = group[(group['user_id'] == klm_user_id) & (group['datetime'] > initial_tweet_time)]\n",
    "        if not airline_reply.empty:\n",
    "            airline_reply_time = airline_reply['datetime'].iloc[0]\n",
    "\n",
    "            # Identify the user's reply to the airline's response\n",
    "            user_followup_tweet = group[(group['user_id'] == user_id) & (group['datetime'] > airline_reply_time)]\n",
    "            if not user_followup_tweet.empty:\n",
    "                # Append sentiments for analysis\n",
    "                sentiments_initial_tweet.append(user_initial_tweet['label'].iloc[0])\n",
    "                sentiments_post_reply.append(user_followup_tweet['label'].iloc[0])\n",
    "\n",
    "# Step 1: Prepare data for Sankey diagram\n",
    "transitions = pd.DataFrame({\n",
    "    'initial': sentiments_initial_tweet,\n",
    "    'post_reply': sentiments_post_reply\n",
    "})\n",
    "\n",
    "# Count transitions\n",
    "transition_counts = transitions.value_counts().reset_index(name='count')\n",
    "\n",
    "# Add percentage to transition_counts\n",
    "total_counts = transition_counts['count'].sum()\n",
    "transition_counts['percentage'] = (transition_counts['count'] / total_counts * 100).round(2)\n",
    "\n",
    "# Define nodes (initial sentiments and final grouped sentiments)\n",
    "nodes = ['positive', 'negative', 'neutral', 'positive_end', 'negative_end', 'neutral_end']\n",
    "node_indices = {label: i for i, label in enumerate(nodes)}\n",
    "\n",
    "# Prepare data for Sankey diagram\n",
    "source = [node_indices[row['initial']] for _, row in transition_counts.iterrows()]\n",
    "target = [node_indices[f\"{row['post_reply']}_end\"] for _, row in transition_counts.iterrows()]\n",
    "value = transition_counts['count'].tolist()\n",
    "label = [f\"{row['initial']} to {row['post_reply']}: {row['count']} ({row['percentage']}%)\" for _, row in transition_counts.iterrows()]\n",
    "\n",
    "# Define colors for nodes and links\n",
    "node_colors = {\n",
    "    'positive': 'rgb(44, 160, 44)',  # green\n",
    "    'negative': 'rgb(214, 39, 40)',  # red\n",
    "    'neutral': 'rgb(255, 165, 0)',   # orange\n",
    "    'positive_end': 'rgb(44, 160, 44)',  # green for final positive\n",
    "    'negative_end': 'rgb(214, 39, 40)',  # red for final negative\n",
    "    'neutral_end': 'rgb(255, 165, 0)'    # orange for final neutral\n",
    "}\n",
    "\n",
    "transition_colors = {\n",
    "    'positive': 'rgba(44, 160, 44, 0.4)',  # light green\n",
    "    'negative': 'rgba(214, 39, 40, 0.4)',  # light red\n",
    "    'neutral': 'rgba(255, 165, 0, 0.4)'    # light orange\n",
    "}\n",
    "\n",
    "# Create color list for nodes\n",
    "colors = [node_colors[node] for node in nodes]\n",
    "\n",
    "# Create the Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=30,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=['Positive', 'Negative', 'Neutral', 'Positive End', 'Negative End', 'Neutral End'],\n",
    "        color=colors\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value,\n",
    "        color=[transition_colors[row['initial']] for _, row in transition_counts.iterrows()],  # Use bland colors for transitions\n",
    "        customdata=label,\n",
    "        hovertemplate='%{customdata}<extra></extra>',\n",
    "        label=label  # Add labels directly to the links\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Sentiment Transition (KLM, baggage)\", \n",
    "    font_size=12,\n",
    "    width=1200,  # increased width to accommodate the legend\n",
    "    height=700,\n",
    "    margin=dict(l=50, r=50, t=50, b=50)  # Adjusted margins\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the airline user ID\n",
    "klm_user_id = 56377143\n",
    "\n",
    "# Define the query to get all tweets in the conversations involving British Airways\n",
    "query_sankey_klm_money = f\"\"\"\n",
    "SELECT tweets.text, tweets.label, tweets.user_id, tweets.timestamp_ms, hasher.conversation_id, hasher.conversation_rank\n",
    "FROM tweets\n",
    "JOIN hasher ON hasher.id = tweets.id\n",
    "JOIN conversations ON hasher.conversation_id = conversations.conversation_id\n",
    "WHERE conversations.airline LIKE '%KLM%' \n",
    "    AND tweets.money = 1\n",
    "    AND tweets.timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "    AND tweets.timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000\n",
    "ORDER BY tweets.timestamp_ms;\n",
    "\"\"\"\n",
    "\n",
    "# Load the result of the query into a DataFrame\n",
    "df_tweets_klm_sankey_money = pd.read_sql(query_sankey_klm_money, connection)\n",
    "\n",
    "# Convert the datetime column to pandas datetime\n",
    "df_tweets_klm_sankey_money['datetime'] = pd.to_datetime(df_tweets_klm_sankey_money['timestamp_ms'], unit='ms')\n",
    "\n",
    "df_tweets_klm_sankey_money['datetime'] = df_tweets_klm_sankey_money['datetime'].dt.tz_localize('UTC').dt.tz_convert('Europe/Amsterdam')\n",
    "\n",
    "# Initialize empty lists to store sentiments\n",
    "sentiments_initial_tweet = []\n",
    "sentiments_post_reply = []\n",
    "\n",
    "# Group tweets by conversation_id\n",
    "grouped_sankey_klm_money = df_tweets_klm_sankey_money.groupby('conversation_id')\n",
    "\n",
    "# Process each conversation\n",
    "for conversation_id, group in grouped_sankey_klm_money:\n",
    "    # Identify the initial user's tweet (rank 1)\n",
    "    user_initial_tweet = group[(group['user_id'] != klm_user_id) & (group['conversation_rank'] == 1)]\n",
    "    \n",
    "    if not user_initial_tweet.empty:\n",
    "        user_id = user_initial_tweet['user_id'].iloc[0]\n",
    "        initial_tweet_time = user_initial_tweet['datetime'].iloc[0]\n",
    "\n",
    "        # Identify the airline's reply to this initial tweet\n",
    "        airline_reply = group[(group['user_id'] == klm_user_id) & (group['datetime'] > initial_tweet_time)]\n",
    "        if not airline_reply.empty:\n",
    "            airline_reply_time = airline_reply['datetime'].iloc[0]\n",
    "\n",
    "            # Identify the user's reply to the airline's response\n",
    "            user_followup_tweet = group[(group['user_id'] == user_id) & (group['datetime'] > airline_reply_time)]\n",
    "            if not user_followup_tweet.empty:\n",
    "                # Append sentiments for analysis\n",
    "                sentiments_initial_tweet.append(user_initial_tweet['label'].iloc[0])\n",
    "                sentiments_post_reply.append(user_followup_tweet['label'].iloc[0])\n",
    "\n",
    "# Step 1: Prepare data for Sankey diagram\n",
    "transitions = pd.DataFrame({\n",
    "    'initial': sentiments_initial_tweet,\n",
    "    'post_reply': sentiments_post_reply\n",
    "})\n",
    "\n",
    "# Count transitions\n",
    "transition_counts = transitions.value_counts().reset_index(name='count')\n",
    "\n",
    "# Add percentage to transition_counts\n",
    "total_counts = transition_counts['count'].sum()\n",
    "transition_counts['percentage'] = (transition_counts['count'] / total_counts * 100).round(2)\n",
    "\n",
    "# Define nodes (initial sentiments and final grouped sentiments)\n",
    "nodes = ['positive', 'negative', 'neutral', 'positive_end', 'negative_end', 'neutral_end']\n",
    "node_indices = {label: i for i, label in enumerate(nodes)}\n",
    "\n",
    "# Prepare data for Sankey diagram\n",
    "source = [node_indices[row['initial']] for _, row in transition_counts.iterrows()]\n",
    "target = [node_indices[f\"{row['post_reply']}_end\"] for _, row in transition_counts.iterrows()]\n",
    "value = transition_counts['count'].tolist()\n",
    "label = [f\"{row['initial']} to {row['post_reply']}: {row['count']} ({row['percentage']}%)\" for _, row in transition_counts.iterrows()]\n",
    "\n",
    "# Define colors for nodes and links\n",
    "node_colors = {\n",
    "    'positive': 'rgb(44, 160, 44)',  # green\n",
    "    'negative': 'rgb(214, 39, 40)',  # red\n",
    "    'neutral': 'rgb(255, 165, 0)',   # orange\n",
    "    'positive_end': 'rgb(44, 160, 44)',  # green for final positive\n",
    "    'negative_end': 'rgb(214, 39, 40)',  # red for final negative\n",
    "    'neutral_end': 'rgb(255, 165, 0)'    # orange for final neutral\n",
    "}\n",
    "\n",
    "transition_colors = {\n",
    "    'positive': 'rgba(44, 160, 44, 0.4)',  # light green\n",
    "    'negative': 'rgba(214, 39, 40, 0.4)',  # light red\n",
    "    'neutral': 'rgba(255, 165, 0, 0.4)'    # light orange\n",
    "}\n",
    "\n",
    "# Create color list for nodes\n",
    "colors = [node_colors[node] for node in nodes]\n",
    "\n",
    "# Create the Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=30,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=['Positive', 'Negative', 'Neutral', 'Positive End', 'Negative End', 'Neutral End'],\n",
    "        color=colors\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value,\n",
    "        color=[transition_colors[row['initial']] for _, row in transition_counts.iterrows()],  # Use bland colors for transitions\n",
    "        customdata=label,\n",
    "        hovertemplate='%{customdata}<extra></extra>',\n",
    "        label=label  # Add labels directly to the links\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Sentiment Transition (KLM, money)\", \n",
    "    font_size=12,\n",
    "    width=1200,  # increased width to accommodate the legend\n",
    "    height=700,\n",
    "    margin=dict(l=50, r=50, t=50, b=50)  # Adjusted margins\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the airline user ID\n",
    "klm_user_id = 56377143\n",
    "\n",
    "# Define the query to get all tweets in the conversations involving British Airways\n",
    "query_sankey_klm_staff = f\"\"\"\n",
    "SELECT tweets.text, tweets.label, tweets.user_id, tweets.timestamp_ms, hasher.conversation_id, hasher.conversation_rank\n",
    "FROM tweets\n",
    "JOIN hasher ON hasher.id = tweets.id\n",
    "JOIN conversations ON hasher.conversation_id = conversations.conversation_id\n",
    "WHERE conversations.airline LIKE '%KLM%' \n",
    "    AND tweets.staff = 1\n",
    "    AND tweets.timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "    AND tweets.timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000\n",
    "ORDER BY tweets.timestamp_ms;\n",
    "\"\"\"\n",
    "\n",
    "# Load the result of the query into a DataFrame\n",
    "df_tweets_klm_sankey_staff = pd.read_sql(query_sankey_klm_staff, connection)\n",
    "\n",
    "# Convert the datetime column to pandas datetime\n",
    "df_tweets_klm_sankey_staff['datetime'] = pd.to_datetime(df_tweets_klm_sankey_staff['timestamp_ms'], unit='ms')\n",
    "\n",
    "df_tweets_klm_sankey_staff['datetime'] = df_tweets_klm_sankey_staff['datetime'].dt.tz_localize('UTC').dt.tz_convert('Europe/Amsterdam')\n",
    "\n",
    "# Initialize empty lists to store sentiments\n",
    "sentiments_initial_tweet = []\n",
    "sentiments_post_reply = []\n",
    "\n",
    "# Group tweets by conversation_id\n",
    "grouped_sankey_klm_staff = df_tweets_klm_sankey_staff.groupby('conversation_id')\n",
    "\n",
    "# Process each conversation\n",
    "for conversation_id, group in grouped_sankey_klm_staff:\n",
    "    # Identify the initial user's tweet (rank 1)\n",
    "    user_initial_tweet = group[(group['user_id'] != klm_user_id) & (group['conversation_rank'] == 1)]\n",
    "    \n",
    "    if not user_initial_tweet.empty:\n",
    "        user_id = user_initial_tweet['user_id'].iloc[0]\n",
    "        initial_tweet_time = user_initial_tweet['datetime'].iloc[0]\n",
    "\n",
    "        # Identify the airline's reply to this initial tweet\n",
    "        airline_reply = group[(group['user_id'] == klm_user_id) & (group['datetime'] > initial_tweet_time)]\n",
    "        if not airline_reply.empty:\n",
    "            airline_reply_time = airline_reply['datetime'].iloc[0]\n",
    "\n",
    "            # Identify the user's reply to the airline's response\n",
    "            user_followup_tweet = group[(group['user_id'] == user_id) & (group['datetime'] > airline_reply_time)]\n",
    "            if not user_followup_tweet.empty:\n",
    "                # Append sentiments for analysis\n",
    "                sentiments_initial_tweet.append(user_initial_tweet['label'].iloc[0])\n",
    "                sentiments_post_reply.append(user_followup_tweet['label'].iloc[0])\n",
    "\n",
    "# Step 1: Prepare data for Sankey diagram\n",
    "transitions = pd.DataFrame({\n",
    "    'initial': sentiments_initial_tweet,\n",
    "    'post_reply': sentiments_post_reply\n",
    "})\n",
    "\n",
    "# Count transitions\n",
    "transition_counts = transitions.value_counts().reset_index(name='count')\n",
    "\n",
    "# Add percentage to transition_counts\n",
    "total_counts = transition_counts['count'].sum()\n",
    "transition_counts['percentage'] = (transition_counts['count'] / total_counts * 100).round(2)\n",
    "\n",
    "# Define nodes (initial sentiments and final grouped sentiments)\n",
    "nodes = ['positive', 'negative', 'neutral', 'positive_end', 'negative_end', 'neutral_end']\n",
    "node_indices = {label: i for i, label in enumerate(nodes)}\n",
    "\n",
    "# Prepare data for Sankey diagram\n",
    "source = [node_indices[row['initial']] for _, row in transition_counts.iterrows()]\n",
    "target = [node_indices[f\"{row['post_reply']}_end\"] for _, row in transition_counts.iterrows()]\n",
    "value = transition_counts['count'].tolist()\n",
    "label = [f\"{row['initial']} to {row['post_reply']}: {row['count']} ({row['percentage']}%)\" for _, row in transition_counts.iterrows()]\n",
    "\n",
    "# Define colors for nodes and links\n",
    "node_colors = {\n",
    "    'positive': 'rgb(44, 160, 44)',  # green\n",
    "    'negative': 'rgb(214, 39, 40)',  # red\n",
    "    'neutral': 'rgb(255, 165, 0)',   # orange\n",
    "    'positive_end': 'rgb(44, 160, 44)',  # green for final positive\n",
    "    'negative_end': 'rgb(214, 39, 40)',  # red for final negative\n",
    "    'neutral_end': 'rgb(255, 165, 0)'    # orange for final neutral\n",
    "}\n",
    "\n",
    "transition_colors = {\n",
    "    'positive': 'rgba(44, 160, 44, 0.4)',  # light green\n",
    "    'negative': 'rgba(214, 39, 40, 0.4)',  # light red\n",
    "    'neutral': 'rgba(255, 165, 0, 0.4)'    # light orange\n",
    "}\n",
    "\n",
    "# Create color list for nodes\n",
    "colors = [node_colors[node] for node in nodes]\n",
    "\n",
    "# Create the Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=30,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=['Positive', 'Negative', 'Neutral', 'Positive End', 'Negative End', 'Neutral End'],\n",
    "        color=colors\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value,\n",
    "        color=[transition_colors[row['initial']] for _, row in transition_counts.iterrows()],  # Use bland colors for transitions\n",
    "        customdata=label,\n",
    "        hovertemplate='%{customdata}<extra></extra>',\n",
    "        label=label  # Add labels directly to the links\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Sentiment Transition (KLM, staff)\", \n",
    "    font_size=12,\n",
    "    width=1200,  # increased width to accommodate the legend\n",
    "    height=700,\n",
    "    margin=dict(l=50, r=50, t=50, b=50)  # Adjusted margins\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the airline user ID\n",
    "klm_user_id = 56377143\n",
    "\n",
    "# Define the query to get all tweets in the conversations involving British Airways\n",
    "query_sankey_klm_delay = f\"\"\"\n",
    "SELECT tweets.text, tweets.label, tweets.user_id, tweets.timestamp_ms, hasher.conversation_id, hasher.conversation_rank\n",
    "FROM tweets\n",
    "JOIN hasher ON hasher.id = tweets.id\n",
    "JOIN conversations ON hasher.conversation_id = conversations.conversation_id\n",
    "WHERE conversations.airline LIKE '%KLM%' \n",
    "    AND tweets.delay_and_cancellation = 1\n",
    "    AND tweets.timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "    AND tweets.timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000\n",
    "ORDER BY tweets.timestamp_ms;\n",
    "\"\"\"\n",
    "\n",
    "# Load the result of the query into a DataFrame\n",
    "df_tweets_klm_sankey_delay = pd.read_sql(query_sankey_klm_delay, connection)\n",
    "\n",
    "# Convert the datetime column to pandas datetime\n",
    "df_tweets_klm_sankey_delay['datetime'] = pd.to_datetime(df_tweets_klm_sankey_delay['timestamp_ms'], unit='ms')\n",
    "\n",
    "df_tweets_klm_sankey_delay['datetime'] = df_tweets_klm_sankey_delay['datetime'].dt.tz_localize('UTC').dt.tz_convert('Europe/Amsterdam')\n",
    "\n",
    "# Initialize empty lists to store sentiments\n",
    "sentiments_initial_tweet = []\n",
    "sentiments_post_reply = []\n",
    "\n",
    "# Group tweets by conversation_id\n",
    "grouped_sankey_klm_delay = df_tweets_klm_sankey_delay.groupby('conversation_id')\n",
    "\n",
    "# Process each conversation\n",
    "for conversation_id, group in grouped_sankey_klm_delay:\n",
    "    # Identify the initial user's tweet (rank 1)\n",
    "    user_initial_tweet = group[(group['user_id'] != klm_user_id) & (group['conversation_rank'] == 1)]\n",
    "    \n",
    "    if not user_initial_tweet.empty:\n",
    "        user_id = user_initial_tweet['user_id'].iloc[0]\n",
    "        initial_tweet_time = user_initial_tweet['datetime'].iloc[0]\n",
    "\n",
    "        # Identify the airline's reply to this initial tweet\n",
    "        airline_reply = group[(group['user_id'] == klm_user_id) & (group['datetime'] > initial_tweet_time)]\n",
    "        if not airline_reply.empty:\n",
    "            airline_reply_time = airline_reply['datetime'].iloc[0]\n",
    "\n",
    "            # Identify the user's reply to the airline's response\n",
    "            user_followup_tweet = group[(group['user_id'] == user_id) & (group['datetime'] > airline_reply_time)]\n",
    "            if not user_followup_tweet.empty:\n",
    "                # Append sentiments for analysis\n",
    "                sentiments_initial_tweet.append(user_initial_tweet['label'].iloc[0])\n",
    "                sentiments_post_reply.append(user_followup_tweet['label'].iloc[0])\n",
    "\n",
    "# Step 1: Prepare data for Sankey diagram\n",
    "transitions = pd.DataFrame({\n",
    "    'initial': sentiments_initial_tweet,\n",
    "    'post_reply': sentiments_post_reply\n",
    "})\n",
    "\n",
    "# Count transitions\n",
    "transition_counts = transitions.value_counts().reset_index(name='count')\n",
    "\n",
    "# Add percentage to transition_counts\n",
    "total_counts = transition_counts['count'].sum()\n",
    "transition_counts['percentage'] = (transition_counts['count'] / total_counts * 100).round(2)\n",
    "\n",
    "# Define nodes (initial sentiments and final grouped sentiments)\n",
    "nodes = ['positive', 'negative', 'neutral', 'positive_end', 'negative_end', 'neutral_end']\n",
    "node_indices = {label: i for i, label in enumerate(nodes)}\n",
    "\n",
    "# Prepare data for Sankey diagram\n",
    "source = [node_indices[row['initial']] for _, row in transition_counts.iterrows()]\n",
    "target = [node_indices[f\"{row['post_reply']}_end\"] for _, row in transition_counts.iterrows()]\n",
    "value = transition_counts['count'].tolist()\n",
    "label = [f\"{row['initial']} to {row['post_reply']}: {row['count']} ({row['percentage']}%)\" for _, row in transition_counts.iterrows()]\n",
    "\n",
    "# Define colors for nodes and links\n",
    "node_colors = {\n",
    "    'positive': 'rgb(44, 160, 44)',  # green\n",
    "    'negative': 'rgb(214, 39, 40)',  # red\n",
    "    'neutral': 'rgb(255, 165, 0)',   # orange\n",
    "    'positive_end': 'rgb(44, 160, 44)',  # green for final positive\n",
    "    'negative_end': 'rgb(214, 39, 40)',  # red for final negative\n",
    "    'neutral_end': 'rgb(255, 165, 0)'    # orange for final neutral\n",
    "}\n",
    "\n",
    "transition_colors = {\n",
    "    'positive': 'rgba(44, 160, 44, 0.4)',  # light green\n",
    "    'negative': 'rgba(214, 39, 40, 0.4)',  # light red\n",
    "    'neutral': 'rgba(255, 165, 0, 0.4)'    # light orange\n",
    "}\n",
    "\n",
    "# Create color list for nodes\n",
    "colors = [node_colors[node] for node in nodes]\n",
    "\n",
    "# Create the Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=30,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=['Positive', 'Negative', 'Neutral', 'Positive End', 'Negative End', 'Neutral End'],\n",
    "        color=colors\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value,\n",
    "        color=[transition_colors[row['initial']] for _, row in transition_counts.iterrows()],  # Use bland colors for transitions\n",
    "        customdata=label,\n",
    "        hovertemplate='%{customdata}<extra></extra>',\n",
    "        label=label  # Add labels directly to the links\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Sentiment Transition (KLM, delay and cancellation)\", \n",
    "    font_size=12,\n",
    "    width=1200,  # increased width to accommodate the legend\n",
    "    height=700,\n",
    "    margin=dict(l=50, r=50, t=50, b=50)  # Adjusted margins\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the airline user ID\n",
    "lu_user_id = 124476322\n",
    "\n",
    "# Define the query to get all tweets in the conversations involving British Airways\n",
    "query_sankey_lu_baggage = f\"\"\"\n",
    "SELECT tweets.text, tweets.label, tweets.user_id, tweets.timestamp_ms, hasher.conversation_id, hasher.conversation_rank\n",
    "FROM tweets\n",
    "JOIN hasher ON hasher.id = tweets.id\n",
    "JOIN conversations ON hasher.conversation_id = conversations.conversation_id\n",
    "WHERE conversations.airline LIKE '%Lufthansa%' \n",
    "    AND tweets.baggage = 1\n",
    "    AND tweets.timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "    AND tweets.timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000\n",
    "ORDER BY tweets.timestamp_ms;\n",
    "\"\"\"\n",
    "\n",
    "# Load the result of the query into a DataFrame\n",
    "df_tweets_lu_sankey_baggage = pd.read_sql(query_sankey_lu_baggage, connection)\n",
    "\n",
    "# Convert the datetime column to pandas datetime\n",
    "df_tweets_lu_sankey_baggage['datetime'] = pd.to_datetime(df_tweets_lu_sankey_baggage['timestamp_ms'], unit='ms')\n",
    "\n",
    "df_tweets_lu_sankey_baggage['datetime'] = df_tweets_lu_sankey_baggage['datetime'].dt.tz_localize('UTC').dt.tz_convert('Europe/Amsterdam')\n",
    "\n",
    "# Initialize empty lists to store sentiments\n",
    "sentiments_initial_tweet = []\n",
    "sentiments_post_reply = []\n",
    "\n",
    "# Group tweets by conversation_id\n",
    "grouped_sankey_lu_baggage = df_tweets_lu_sankey_baggage.groupby('conversation_id')\n",
    "\n",
    "# Process each conversation\n",
    "for conversation_id, group in grouped_sankey_lu_baggage:\n",
    "    # Identify the initial user's tweet (rank 1)\n",
    "    user_initial_tweet = group[(group['user_id'] != lu_user_id) & (group['conversation_rank'] == 1)]\n",
    "    \n",
    "    if not user_initial_tweet.empty:\n",
    "        user_id = user_initial_tweet['user_id'].iloc[0]\n",
    "        initial_tweet_time = user_initial_tweet['datetime'].iloc[0]\n",
    "\n",
    "        # Identify the airline's reply to this initial tweet\n",
    "        airline_reply = group[(group['user_id'] == lu_user_id) & (group['datetime'] > initial_tweet_time)]\n",
    "        if not airline_reply.empty:\n",
    "            airline_reply_time = airline_reply['datetime'].iloc[0]\n",
    "\n",
    "            # Identify the user's reply to the airline's response\n",
    "            user_followup_tweet = group[(group['user_id'] == user_id) & (group['datetime'] > airline_reply_time)]\n",
    "            if not user_followup_tweet.empty:\n",
    "                # Append sentiments for analysis\n",
    "                sentiments_initial_tweet.append(user_initial_tweet['label'].iloc[0])\n",
    "                sentiments_post_reply.append(user_followup_tweet['label'].iloc[0])\n",
    "\n",
    "# Step 1: Prepare data for Sankey diagram\n",
    "transitions = pd.DataFrame({\n",
    "    'initial': sentiments_initial_tweet,\n",
    "    'post_reply': sentiments_post_reply\n",
    "})\n",
    "\n",
    "# Count transitions\n",
    "transition_counts = transitions.value_counts().reset_index(name='count')\n",
    "\n",
    "# Add percentage to transition_counts\n",
    "total_counts = transition_counts['count'].sum()\n",
    "transition_counts['percentage'] = (transition_counts['count'] / total_counts * 100).round(2)\n",
    "\n",
    "# Define nodes (initial sentiments and final grouped sentiments)\n",
    "nodes = ['positive', 'negative', 'neutral', 'positive_end', 'negative_end', 'neutral_end']\n",
    "node_indices = {label: i for i, label in enumerate(nodes)}\n",
    "\n",
    "# Prepare data for Sankey diagram\n",
    "source = [node_indices[row['initial']] for _, row in transition_counts.iterrows()]\n",
    "target = [node_indices[f\"{row['post_reply']}_end\"] for _, row in transition_counts.iterrows()]\n",
    "value = transition_counts['count'].tolist()\n",
    "label = [f\"{row['initial']} to {row['post_reply']}: {row['count']} ({row['percentage']}%)\" for _, row in transition_counts.iterrows()]\n",
    "\n",
    "# Define colors for nodes and links\n",
    "node_colors = {\n",
    "    'positive': 'rgb(44, 160, 44)',  # green\n",
    "    'negative': 'rgb(214, 39, 40)',  # red\n",
    "    'neutral': 'rgb(255, 165, 0)',   # orange\n",
    "    'positive_end': 'rgb(44, 160, 44)',  # green for final positive\n",
    "    'negative_end': 'rgb(214, 39, 40)',  # red for final negative\n",
    "    'neutral_end': 'rgb(255, 165, 0)'    # orange for final neutral\n",
    "}\n",
    "\n",
    "transition_colors = {\n",
    "    'positive': 'rgba(44, 160, 44, 0.4)',  # light green\n",
    "    'negative': 'rgba(214, 39, 40, 0.4)',  # light red\n",
    "    'neutral': 'rgba(255, 165, 0, 0.4)'    # light orange\n",
    "}\n",
    "\n",
    "# Create color list for nodes\n",
    "colors = [node_colors[node] for node in nodes]\n",
    "\n",
    "# Create the Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=30,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=['Positive', 'Negative', 'Neutral', 'Positive End', 'Negative End', 'Neutral End'],\n",
    "        color=colors\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value,\n",
    "        color=[transition_colors[row['initial']] for _, row in transition_counts.iterrows()],  # Use bland colors for transitions\n",
    "        customdata=label,\n",
    "        hovertemplate='%{customdata}<extra></extra>',\n",
    "        label=label  # Add labels directly to the links\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Sentiment Transition (Lufthansa, baggage)\", \n",
    "    font_size=12,\n",
    "    width=1200,  # increased width to accommodate the legend\n",
    "    height=700,\n",
    "    margin=dict(l=50, r=50, t=50, b=50)  # Adjusted margins\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the airline user ID\n",
    "lu_user_id = 124476322\n",
    "\n",
    "# Define the query to get all tweets in the conversations involving British Airways\n",
    "query_sankey_lu_money = f\"\"\"\n",
    "SELECT tweets.text, tweets.label, tweets.user_id, tweets.timestamp_ms, hasher.conversation_id, hasher.conversation_rank\n",
    "FROM tweets\n",
    "JOIN hasher ON hasher.id = tweets.id\n",
    "JOIN conversations ON hasher.conversation_id = conversations.conversation_id\n",
    "WHERE conversations.airline LIKE '%Lufthansa%' \n",
    "    AND tweets.money = 1\n",
    "    AND tweets.timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "    AND tweets.timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000\n",
    "ORDER BY tweets.timestamp_ms;\n",
    "\"\"\"\n",
    "\n",
    "# Load the result of the query into a DataFrame\n",
    "df_tweets_lu_sankey_money = pd.read_sql(query_sankey_lu_money, connection)\n",
    "\n",
    "# Convert the datetime column to pandas datetime\n",
    "df_tweets_lu_sankey_money['datetime'] = pd.to_datetime(df_tweets_lu_sankey_money['timestamp_ms'], unit='ms')\n",
    "\n",
    "df_tweets_lu_sankey_money['datetime'] = df_tweets_lu_sankey_money['datetime'].dt.tz_localize('UTC').dt.tz_convert('Europe/Amsterdam')\n",
    "\n",
    "# Initialize empty lists to store sentiments\n",
    "sentiments_initial_tweet = []\n",
    "sentiments_post_reply = []\n",
    "\n",
    "# Group tweets by conversation_id\n",
    "grouped_sankey_lu_money= df_tweets_lu_sankey_money.groupby('conversation_id')\n",
    "\n",
    "# Process each conversation\n",
    "for conversation_id, group in grouped_sankey_lu_money:\n",
    "    # Identify the initial user's tweet (rank 1)\n",
    "    user_initial_tweet = group[(group['user_id'] != lu_user_id) & (group['conversation_rank'] == 1)]\n",
    "    \n",
    "    if not user_initial_tweet.empty:\n",
    "        user_id = user_initial_tweet['user_id'].iloc[0]\n",
    "        initial_tweet_time = user_initial_tweet['datetime'].iloc[0]\n",
    "\n",
    "        # Identify the airline's reply to this initial tweet\n",
    "        airline_reply = group[(group['user_id'] == lu_user_id) & (group['datetime'] > initial_tweet_time)]\n",
    "        if not airline_reply.empty:\n",
    "            airline_reply_time = airline_reply['datetime'].iloc[0]\n",
    "\n",
    "            # Identify the user's reply to the airline's response\n",
    "            user_followup_tweet = group[(group['user_id'] == user_id) & (group['datetime'] > airline_reply_time)]\n",
    "            if not user_followup_tweet.empty:\n",
    "                # Append sentiments for analysis\n",
    "                sentiments_initial_tweet.append(user_initial_tweet['label'].iloc[0])\n",
    "                sentiments_post_reply.append(user_followup_tweet['label'].iloc[0])\n",
    "\n",
    "# Step 1: Prepare data for Sankey diagram\n",
    "transitions = pd.DataFrame({\n",
    "    'initial': sentiments_initial_tweet,\n",
    "    'post_reply': sentiments_post_reply\n",
    "})\n",
    "\n",
    "# Count transitions\n",
    "transition_counts = transitions.value_counts().reset_index(name='count')\n",
    "\n",
    "# Add percentage to transition_counts\n",
    "total_counts = transition_counts['count'].sum()\n",
    "transition_counts['percentage'] = (transition_counts['count'] / total_counts * 100).round(2)\n",
    "\n",
    "# Define nodes (initial sentiments and final grouped sentiments)\n",
    "nodes = ['positive', 'negative', 'neutral', 'positive_end', 'negative_end', 'neutral_end']\n",
    "node_indices = {label: i for i, label in enumerate(nodes)}\n",
    "\n",
    "# Prepare data for Sankey diagram\n",
    "source = [node_indices[row['initial']] for _, row in transition_counts.iterrows()]\n",
    "target = [node_indices[f\"{row['post_reply']}_end\"] for _, row in transition_counts.iterrows()]\n",
    "value = transition_counts['count'].tolist()\n",
    "label = [f\"{row['initial']} to {row['post_reply']}: {row['count']} ({row['percentage']}%)\" for _, row in transition_counts.iterrows()]\n",
    "\n",
    "# Define colors for nodes and links\n",
    "node_colors = {\n",
    "    'positive': 'rgb(44, 160, 44)',  # green\n",
    "    'negative': 'rgb(214, 39, 40)',  # red\n",
    "    'neutral': 'rgb(255, 165, 0)',   # orange\n",
    "    'positive_end': 'rgb(44, 160, 44)',  # green for final positive\n",
    "    'negative_end': 'rgb(214, 39, 40)',  # red for final negative\n",
    "    'neutral_end': 'rgb(255, 165, 0)'    # orange for final neutral\n",
    "}\n",
    "\n",
    "transition_colors = {\n",
    "    'positive': 'rgba(44, 160, 44, 0.4)',  # light green\n",
    "    'negative': 'rgba(214, 39, 40, 0.4)',  # light red\n",
    "    'neutral': 'rgba(255, 165, 0, 0.4)'    # light orange\n",
    "}\n",
    "\n",
    "# Create color list for nodes\n",
    "colors = [node_colors[node] for node in nodes]\n",
    "\n",
    "# Create the Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=30,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=['Positive', 'Negative', 'Neutral', 'Positive End', 'Negative End', 'Neutral End'],\n",
    "        color=colors\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value,\n",
    "        color=[transition_colors[row['initial']] for _, row in transition_counts.iterrows()],  # Use bland colors for transitions\n",
    "        customdata=label,\n",
    "        hovertemplate='%{customdata}<extra></extra>',\n",
    "        label=label  # Add labels directly to the links\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Sentiment Transition (Lufthansa, money)\", \n",
    "    font_size=12,\n",
    "    width=1200,  # increased width to accommodate the legend\n",
    "    height=700,\n",
    "    margin=dict(l=50, r=50, t=50, b=50)  # Adjusted margins\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the airline user ID\n",
    "lu_user_id = 124476322\n",
    "\n",
    "# Define the query to get all tweets in the conversations involving British Airways\n",
    "query_sankey_lu_staff = f\"\"\"\n",
    "SELECT tweets.text, tweets.label, tweets.user_id, tweets.timestamp_ms, hasher.conversation_id, hasher.conversation_rank\n",
    "FROM tweets\n",
    "JOIN hasher ON hasher.id = tweets.id\n",
    "JOIN conversations ON hasher.conversation_id = conversations.conversation_id\n",
    "WHERE conversations.airline LIKE '%Lufthansa%' \n",
    "    AND tweets.staff = 1\n",
    "    AND tweets.timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "    AND tweets.timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000\n",
    "ORDER BY tweets.timestamp_ms;\n",
    "\"\"\"\n",
    "\n",
    "# Load the result of the query into a DataFrame\n",
    "df_tweets_lu_sankey_staff = pd.read_sql(query_sankey_lu_staff, connection)\n",
    "\n",
    "# Convert the datetime column to pandas datetime\n",
    "df_tweets_lu_sankey_staff['datetime'] = pd.to_datetime(df_tweets_lu_sankey_staff['timestamp_ms'], unit='ms')\n",
    "\n",
    "df_tweets_lu_sankey_staff['datetime'] = df_tweets_lu_sankey_staff['datetime'].dt.tz_localize('UTC').dt.tz_convert('Europe/Amsterdam')\n",
    "\n",
    "# Initialize empty lists to store sentiments\n",
    "sentiments_initial_tweet = []\n",
    "sentiments_post_reply = []\n",
    "\n",
    "# Group tweets by conversation_id\n",
    "grouped_sankey_lu_staff = df_tweets_lu_sankey_staff.groupby('conversation_id')\n",
    "\n",
    "# Process each conversation\n",
    "for conversation_id, group in grouped_sankey_lu_staff:\n",
    "    # Identify the initial user's tweet (rank 1)\n",
    "    user_initial_tweet = group[(group['user_id'] != lu_user_id) & (group['conversation_rank'] == 1)]\n",
    "    \n",
    "    if not user_initial_tweet.empty:\n",
    "        user_id = user_initial_tweet['user_id'].iloc[0]\n",
    "        initial_tweet_time = user_initial_tweet['datetime'].iloc[0]\n",
    "\n",
    "        # Identify the airline's reply to this initial tweet\n",
    "        airline_reply = group[(group['user_id'] == lu_user_id) & (group['datetime'] > initial_tweet_time)]\n",
    "        if not airline_reply.empty:\n",
    "            airline_reply_time = airline_reply['datetime'].iloc[0]\n",
    "\n",
    "            # Identify the user's reply to the airline's response\n",
    "            user_followup_tweet = group[(group['user_id'] == user_id) & (group['datetime'] > airline_reply_time)]\n",
    "            if not user_followup_tweet.empty:\n",
    "                # Append sentiments for analysis\n",
    "                sentiments_initial_tweet.append(user_initial_tweet['label'].iloc[0])\n",
    "                sentiments_post_reply.append(user_followup_tweet['label'].iloc[0])\n",
    "\n",
    "# Step 1: Prepare data for Sankey diagram\n",
    "transitions = pd.DataFrame({\n",
    "    'initial': sentiments_initial_tweet,\n",
    "    'post_reply': sentiments_post_reply\n",
    "})\n",
    "\n",
    "# Count transitions\n",
    "transition_counts = transitions.value_counts().reset_index(name='count')\n",
    "\n",
    "# Add percentage to transition_counts\n",
    "total_counts = transition_counts['count'].sum()\n",
    "transition_counts['percentage'] = (transition_counts['count'] / total_counts * 100).round(2)\n",
    "\n",
    "# Define nodes (initial sentiments and final grouped sentiments)\n",
    "nodes = ['positive', 'negative', 'neutral', 'positive_end', 'negative_end', 'neutral_end']\n",
    "node_indices = {label: i for i, label in enumerate(nodes)}\n",
    "\n",
    "# Prepare data for Sankey diagram\n",
    "source = [node_indices[row['initial']] for _, row in transition_counts.iterrows()]\n",
    "target = [node_indices[f\"{row['post_reply']}_end\"] for _, row in transition_counts.iterrows()]\n",
    "value = transition_counts['count'].tolist()\n",
    "label = [f\"{row['initial']} to {row['post_reply']}: {row['count']} ({row['percentage']}%)\" for _, row in transition_counts.iterrows()]\n",
    "\n",
    "# Define colors for nodes and links\n",
    "node_colors = {\n",
    "    'positive': 'rgb(44, 160, 44)',  # green\n",
    "    'negative': 'rgb(214, 39, 40)',  # red\n",
    "    'neutral': 'rgb(255, 165, 0)',   # orange\n",
    "    'positive_end': 'rgb(44, 160, 44)',  # green for final positive\n",
    "    'negative_end': 'rgb(214, 39, 40)',  # red for final negative\n",
    "    'neutral_end': 'rgb(255, 165, 0)'    # orange for final neutral\n",
    "}\n",
    "\n",
    "transition_colors = {\n",
    "    'positive': 'rgba(44, 160, 44, 0.4)',  # light green\n",
    "    'negative': 'rgba(214, 39, 40, 0.4)',  # light red\n",
    "    'neutral': 'rgba(255, 165, 0, 0.4)'    # light orange\n",
    "}\n",
    "\n",
    "# Create color list for nodes\n",
    "colors = [node_colors[node] for node in nodes]\n",
    "\n",
    "# Create the Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=30,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=['Positive', 'Negative', 'Neutral', 'Positive End', 'Negative End', 'Neutral End'],\n",
    "        color=colors\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value,\n",
    "        color=[transition_colors[row['initial']] for _, row in transition_counts.iterrows()],  # Use bland colors for transitions\n",
    "        customdata=label,\n",
    "        hovertemplate='%{customdata}<extra></extra>',\n",
    "        label=label  # Add labels directly to the links\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Sentiment Transition (Lufthansa, staff)\", \n",
    "    font_size=12,\n",
    "    width=1200,  # increased width to accommodate the legend\n",
    "    height=700,\n",
    "    margin=dict(l=50, r=50, t=50, b=50)  # Adjusted margins\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the airline user ID\n",
    "lu_user_id = 124476322\n",
    "\n",
    "# Define the query to get all tweets in the conversations involving British Airways\n",
    "query_sankey_lu_delay = f\"\"\"\n",
    "SELECT tweets.text, tweets.label, tweets.user_id, tweets.timestamp_ms, hasher.conversation_id, hasher.conversation_rank\n",
    "FROM tweets\n",
    "JOIN hasher ON hasher.id = tweets.id\n",
    "JOIN conversations ON hasher.conversation_id = conversations.conversation_id\n",
    "WHERE conversations.airline LIKE '%Lufthansa%' \n",
    "    AND tweets.delay_and_cancellation = 1\n",
    "    AND tweets.timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "    AND tweets.timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000\n",
    "ORDER BY tweets.timestamp_ms;\n",
    "\"\"\"\n",
    "\n",
    "# Load the result of the query into a DataFrame\n",
    "df_tweets_lu_sankey_delay = pd.read_sql(query_sankey_lu_delay, connection)\n",
    "\n",
    "# Convert the datetime column to pandas datetime\n",
    "df_tweets_lu_sankey_delay['datetime'] = pd.to_datetime(df_tweets_lu_sankey_delay['timestamp_ms'], unit='ms')\n",
    "\n",
    "df_tweets_lu_sankey_delay['datetime'] = df_tweets_lu_sankey_delay['datetime'].dt.tz_localize('UTC').dt.tz_convert('Europe/Amsterdam')\n",
    "\n",
    "# Initialize empty lists to store sentiments\n",
    "sentiments_initial_tweet = []\n",
    "sentiments_post_reply = []\n",
    "\n",
    "# Group tweets by conversation_id\n",
    "grouped_sankey_lu_delay= df_tweets_lu_sankey_delay.groupby('conversation_id')\n",
    "\n",
    "# Process each conversation\n",
    "for conversation_id, group in grouped_sankey_lu_delay:\n",
    "    # Identify the initial user's tweet (rank 1)\n",
    "    user_initial_tweet = group[(group['user_id'] != lu_user_id) & (group['conversation_rank'] == 1)]\n",
    "    \n",
    "    if not user_initial_tweet.empty:\n",
    "        user_id = user_initial_tweet['user_id'].iloc[0]\n",
    "        initial_tweet_time = user_initial_tweet['datetime'].iloc[0]\n",
    "\n",
    "        # Identify the airline's reply to this initial tweet\n",
    "        airline_reply = group[(group['user_id'] == lu_user_id) & (group['datetime'] > initial_tweet_time)]\n",
    "        if not airline_reply.empty:\n",
    "            airline_reply_time = airline_reply['datetime'].iloc[0]\n",
    "\n",
    "            # Identify the user's reply to the airline's response\n",
    "            user_followup_tweet = group[(group['user_id'] == user_id) & (group['datetime'] > airline_reply_time)]\n",
    "            if not user_followup_tweet.empty:\n",
    "                # Append sentiments for analysis\n",
    "                sentiments_initial_tweet.append(user_initial_tweet['label'].iloc[0])\n",
    "                sentiments_post_reply.append(user_followup_tweet['label'].iloc[0])\n",
    "\n",
    "# Step 1: Prepare data for Sankey diagram\n",
    "transitions = pd.DataFrame({\n",
    "    'initial': sentiments_initial_tweet,\n",
    "    'post_reply': sentiments_post_reply\n",
    "})\n",
    "\n",
    "# Count transitions\n",
    "transition_counts = transitions.value_counts().reset_index(name='count')\n",
    "\n",
    "# Add percentage to transition_counts\n",
    "total_counts = transition_counts['count'].sum()\n",
    "transition_counts['percentage'] = (transition_counts['count'] / total_counts * 100).round(2)\n",
    "\n",
    "# Define nodes (initial sentiments and final grouped sentiments)\n",
    "nodes = ['positive', 'negative', 'neutral', 'positive_end', 'negative_end', 'neutral_end']\n",
    "node_indices = {label: i for i, label in enumerate(nodes)}\n",
    "\n",
    "# Prepare data for Sankey diagram\n",
    "source = [node_indices[row['initial']] for _, row in transition_counts.iterrows()]\n",
    "target = [node_indices[f\"{row['post_reply']}_end\"] for _, row in transition_counts.iterrows()]\n",
    "value = transition_counts['count'].tolist()\n",
    "label = [f\"{row['initial']} to {row['post_reply']}: {row['count']} ({row['percentage']}%)\" for _, row in transition_counts.iterrows()]\n",
    "\n",
    "# Define colors for nodes and links\n",
    "node_colors = {\n",
    "    'positive': 'rgb(44, 160, 44)',  # green\n",
    "    'negative': 'rgb(214, 39, 40)',  # red\n",
    "    'neutral': 'rgb(255, 165, 0)',   # orange\n",
    "    'positive_end': 'rgb(44, 160, 44)',  # green for final positive\n",
    "    'negative_end': 'rgb(214, 39, 40)',  # red for final negative\n",
    "    'neutral_end': 'rgb(255, 165, 0)'    # orange for final neutral\n",
    "}\n",
    "\n",
    "transition_colors = {\n",
    "    'positive': 'rgba(44, 160, 44, 0.4)',  # light green\n",
    "    'negative': 'rgba(214, 39, 40, 0.4)',  # light red\n",
    "    'neutral': 'rgba(255, 165, 0, 0.4)'    # light orange\n",
    "}\n",
    "\n",
    "# Create color list for nodes\n",
    "colors = [node_colors[node] for node in nodes]\n",
    "\n",
    "# Create the Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=30,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=['Positive', 'Negative', 'Neutral', 'Positive End', 'Negative End', 'Neutral End'],\n",
    "        color=colors\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value,\n",
    "        color=[transition_colors[row['initial']] for _, row in transition_counts.iterrows()],  # Use bland colors for transitions\n",
    "        customdata=label,\n",
    "        hovertemplate='%{customdata}<extra></extra>',\n",
    "        label=label  # Add labels directly to the links\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Sentiment Transition (Lufthansa, delay and cancellation)\", \n",
    "    font_size=12,\n",
    "    width=1200,  # increased width to accommodate the legend\n",
    "    height=700,\n",
    "    margin=dict(l=50, r=50, t=50, b=50)  # Adjusted margins\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the airline user ID\n",
    "af_user_id = 106062176\n",
    "\n",
    "# Define the query to get all tweets in the conversations involving British Airways\n",
    "query_sankey_af_baggage = f\"\"\"\n",
    "SELECT tweets.text, tweets.label, tweets.user_id, tweets.timestamp_ms, hasher.conversation_id, hasher.conversation_rank\n",
    "FROM tweets\n",
    "JOIN hasher ON hasher.id = tweets.id\n",
    "JOIN conversations ON hasher.conversation_id = conversations.conversation_id\n",
    "WHERE conversations.airline LIKE '%AirFrance%' \n",
    "    AND tweets.baggage = 1\n",
    "    AND tweets.timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "    AND tweets.timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000\n",
    "ORDER BY tweets.timestamp_ms;\n",
    "\"\"\"\n",
    "\n",
    "# Load the result of the query into a DataFrame\n",
    "df_tweets_af_sankey_baggage = pd.read_sql(query_sankey_af_baggage, connection)\n",
    "\n",
    "# Convert the datetime column to pandas datetime\n",
    "df_tweets_af_sankey_baggage['datetime'] = pd.to_datetime(df_tweets_af_sankey_baggage['timestamp_ms'], unit='ms')\n",
    "\n",
    "df_tweets_af_sankey_baggage['datetime'] = df_tweets_af_sankey_baggage['datetime'].dt.tz_localize('UTC').dt.tz_convert('Europe/Amsterdam')\n",
    "\n",
    "# Initialize empty lists to store sentiments\n",
    "sentiments_initial_tweet = []\n",
    "sentiments_post_reply = []\n",
    "\n",
    "# Group tweets by conversation_id\n",
    "grouped_sankey_af_baggage = df_tweets_af_sankey_baggage.groupby('conversation_id')\n",
    "\n",
    "# Process each conversation\n",
    "for conversation_id, group in grouped_sankey_af_baggage:\n",
    "    # Identify the initial user's tweet (rank 1)\n",
    "    user_initial_tweet = group[(group['user_id'] != af_user_id) & (group['conversation_rank'] == 1)]\n",
    "    \n",
    "    if not user_initial_tweet.empty:\n",
    "        user_id = user_initial_tweet['user_id'].iloc[0]\n",
    "        initial_tweet_time = user_initial_tweet['datetime'].iloc[0]\n",
    "\n",
    "        # Identify the airline's reply to this initial tweet\n",
    "        airline_reply = group[(group['user_id'] == af_user_id) & (group['datetime'] > initial_tweet_time)]\n",
    "        if not airline_reply.empty:\n",
    "            airline_reply_time = airline_reply['datetime'].iloc[0]\n",
    "\n",
    "            # Identify the user's reply to the airline's response\n",
    "            user_followup_tweet = group[(group['user_id'] == user_id) & (group['datetime'] > airline_reply_time)]\n",
    "            if not user_followup_tweet.empty:\n",
    "                # Append sentiments for analysis\n",
    "                sentiments_initial_tweet.append(user_initial_tweet['label'].iloc[0])\n",
    "                sentiments_post_reply.append(user_followup_tweet['label'].iloc[0])\n",
    "\n",
    "# Step 1: Prepare data for Sankey diagram\n",
    "transitions = pd.DataFrame({\n",
    "    'initial': sentiments_initial_tweet,\n",
    "    'post_reply': sentiments_post_reply\n",
    "})\n",
    "\n",
    "# Count transitions\n",
    "transition_counts = transitions.value_counts().reset_index(name='count')\n",
    "\n",
    "# Add percentage to transition_counts\n",
    "total_counts = transition_counts['count'].sum()\n",
    "transition_counts['percentage'] = (transition_counts['count'] / total_counts * 100).round(2)\n",
    "\n",
    "# Define nodes (initial sentiments and final grouped sentiments)\n",
    "nodes = ['positive', 'negative', 'neutral', 'positive_end', 'negative_end', 'neutral_end']\n",
    "node_indices = {label: i for i, label in enumerate(nodes)}\n",
    "\n",
    "# Prepare data for Sankey diagram\n",
    "source = [node_indices[row['initial']] for _, row in transition_counts.iterrows()]\n",
    "target = [node_indices[f\"{row['post_reply']}_end\"] for _, row in transition_counts.iterrows()]\n",
    "value = transition_counts['count'].tolist()\n",
    "label = [f\"{row['initial']} to {row['post_reply']}: {row['count']} ({row['percentage']}%)\" for _, row in transition_counts.iterrows()]\n",
    "\n",
    "# Define colors for nodes and links\n",
    "node_colors = {\n",
    "    'positive': 'rgb(44, 160, 44)',  # green\n",
    "    'negative': 'rgb(214, 39, 40)',  # red\n",
    "    'neutral': 'rgb(255, 165, 0)',   # orange\n",
    "    'positive_end': 'rgb(44, 160, 44)',  # green for final positive\n",
    "    'negative_end': 'rgb(214, 39, 40)',  # red for final negative\n",
    "    'neutral_end': 'rgb(255, 165, 0)'    # orange for final neutral\n",
    "}\n",
    "\n",
    "transition_colors = {\n",
    "    'positive': 'rgba(44, 160, 44, 0.4)',  # light green\n",
    "    'negative': 'rgba(214, 39, 40, 0.4)',  # light red\n",
    "    'neutral': 'rgba(255, 165, 0, 0.4)'    # light orange\n",
    "}\n",
    "\n",
    "# Create color list for nodes\n",
    "colors = [node_colors[node] for node in nodes]\n",
    "\n",
    "# Create the Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=30,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=['Positive', 'Negative', 'Neutral', 'Positive End', 'Negative End', 'Neutral End'],\n",
    "        color=colors\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value,\n",
    "        color=[transition_colors[row['initial']] for _, row in transition_counts.iterrows()],  # Use bland colors for transitions\n",
    "        customdata=label,\n",
    "        hovertemplate='%{customdata}<extra></extra>',\n",
    "        label=label  # Add labels directly to the links\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Sentiment Transition (Air France, baggage)\", \n",
    "    font_size=12,\n",
    "    width=1200,  # increased width to accommodate the legend\n",
    "    height=700,\n",
    "    margin=dict(l=50, r=50, t=50, b=50)  # Adjusted margins\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the airline user ID\n",
    "af_user_id = 106062176\n",
    "\n",
    "# Define the query to get all tweets in the conversations involving British Airways\n",
    "query_sankey_af_money = f\"\"\"\n",
    "SELECT tweets.text, tweets.label, tweets.user_id, tweets.timestamp_ms, hasher.conversation_id, hasher.conversation_rank\n",
    "FROM tweets\n",
    "JOIN hasher ON hasher.id = tweets.id\n",
    "JOIN conversations ON hasher.conversation_id = conversations.conversation_id\n",
    "WHERE conversations.airline LIKE '%AirFrance%' \n",
    "    AND tweets.money = 1\n",
    "    AND tweets.timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "    AND tweets.timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000\n",
    "ORDER BY tweets.timestamp_ms;\n",
    "\"\"\"\n",
    "\n",
    "# Load the result of the query into a DataFrame\n",
    "df_tweets_af_sankey_money = pd.read_sql(query_sankey_af_money, connection)\n",
    "\n",
    "# Convert the datetime column to pandas datetime\n",
    "df_tweets_af_sankey_money['datetime'] = pd.to_datetime(df_tweets_af_sankey_money['timestamp_ms'], unit='ms')\n",
    "\n",
    "df_tweets_af_sankey_money['datetime'] = df_tweets_af_sankey_money['datetime'].dt.tz_localize('UTC').dt.tz_convert('Europe/Amsterdam')\n",
    "\n",
    "# Initialize empty lists to store sentiments\n",
    "sentiments_initial_tweet = []\n",
    "sentiments_post_reply = []\n",
    "\n",
    "# Group tweets by conversation_id\n",
    "grouped_sankey_af_money = df_tweets_af_sankey_money.groupby('conversation_id')\n",
    "\n",
    "# Process each conversation\n",
    "for conversation_id, group in grouped_sankey_af_money:\n",
    "    # Identify the initial user's tweet (rank 1)\n",
    "    user_initial_tweet = group[(group['user_id'] != af_user_id) & (group['conversation_rank'] == 1)]\n",
    "    \n",
    "    if not user_initial_tweet.empty:\n",
    "        user_id = user_initial_tweet['user_id'].iloc[0]\n",
    "        initial_tweet_time = user_initial_tweet['datetime'].iloc[0]\n",
    "\n",
    "        # Identify the airline's reply to this initial tweet\n",
    "        airline_reply = group[(group['user_id'] == af_user_id) & (group['datetime'] > initial_tweet_time)]\n",
    "        if not airline_reply.empty:\n",
    "            airline_reply_time = airline_reply['datetime'].iloc[0]\n",
    "\n",
    "            # Identify the user's reply to the airline's response\n",
    "            user_followup_tweet = group[(group['user_id'] == user_id) & (group['datetime'] > airline_reply_time)]\n",
    "            if not user_followup_tweet.empty:\n",
    "                # Append sentiments for analysis\n",
    "                sentiments_initial_tweet.append(user_initial_tweet['label'].iloc[0])\n",
    "                sentiments_post_reply.append(user_followup_tweet['label'].iloc[0])\n",
    "\n",
    "# Step 1: Prepare data for Sankey diagram\n",
    "transitions = pd.DataFrame({\n",
    "    'initial': sentiments_initial_tweet,\n",
    "    'post_reply': sentiments_post_reply\n",
    "})\n",
    "\n",
    "# Count transitions\n",
    "transition_counts = transitions.value_counts().reset_index(name='count')\n",
    "\n",
    "# Add percentage to transition_counts\n",
    "total_counts = transition_counts['count'].sum()\n",
    "transition_counts['percentage'] = (transition_counts['count'] / total_counts * 100).round(2)\n",
    "\n",
    "# Define nodes (initial sentiments and final grouped sentiments)\n",
    "nodes = ['positive', 'negative', 'neutral', 'positive_end', 'negative_end', 'neutral_end']\n",
    "node_indices = {label: i for i, label in enumerate(nodes)}\n",
    "\n",
    "# Prepare data for Sankey diagram\n",
    "source = [node_indices[row['initial']] for _, row in transition_counts.iterrows()]\n",
    "target = [node_indices[f\"{row['post_reply']}_end\"] for _, row in transition_counts.iterrows()]\n",
    "value = transition_counts['count'].tolist()\n",
    "label = [f\"{row['initial']} to {row['post_reply']}: {row['count']} ({row['percentage']}%)\" for _, row in transition_counts.iterrows()]\n",
    "\n",
    "# Define colors for nodes and links\n",
    "node_colors = {\n",
    "    'positive': 'rgb(44, 160, 44)',  # green\n",
    "    'negative': 'rgb(214, 39, 40)',  # red\n",
    "    'neutral': 'rgb(255, 165, 0)',   # orange\n",
    "    'positive_end': 'rgb(44, 160, 44)',  # green for final positive\n",
    "    'negative_end': 'rgb(214, 39, 40)',  # red for final negative\n",
    "    'neutral_end': 'rgb(255, 165, 0)'    # orange for final neutral\n",
    "}\n",
    "\n",
    "transition_colors = {\n",
    "    'positive': 'rgba(44, 160, 44, 0.4)',  # light green\n",
    "    'negative': 'rgba(214, 39, 40, 0.4)',  # light red\n",
    "    'neutral': 'rgba(255, 165, 0, 0.4)'    # light orange\n",
    "}\n",
    "\n",
    "# Create color list for nodes\n",
    "colors = [node_colors[node] for node in nodes]\n",
    "\n",
    "# Create the Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=30,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=['Positive', 'Negative', 'Neutral', 'Positive End', 'Negative End', 'Neutral End'],\n",
    "        color=colors\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value,\n",
    "        color=[transition_colors[row['initial']] for _, row in transition_counts.iterrows()],  # Use bland colors for transitions\n",
    "        customdata=label,\n",
    "        hovertemplate='%{customdata}<extra></extra>',\n",
    "        label=label  # Add labels directly to the links\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Sentiment Transition (Air France, money)\", \n",
    "    font_size=12,\n",
    "    width=1200,  # increased width to accommodate the legend\n",
    "    height=700,\n",
    "    margin=dict(l=50, r=50, t=50, b=50)  # Adjusted margins\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the airline user ID\n",
    "af_user_id = 106062176\n",
    "\n",
    "# Define the query to get all tweets in the conversations involving British Airways\n",
    "query_sankey_af_staff = f\"\"\"\n",
    "SELECT tweets.text, tweets.label, tweets.user_id, tweets.timestamp_ms, hasher.conversation_id, hasher.conversation_rank\n",
    "FROM tweets\n",
    "JOIN hasher ON hasher.id = tweets.id\n",
    "JOIN conversations ON hasher.conversation_id = conversations.conversation_id\n",
    "WHERE conversations.airline LIKE '%AirFrance%' \n",
    "    AND tweets.staff = 1\n",
    "    AND tweets.timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "    AND tweets.timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000\n",
    "ORDER BY tweets.timestamp_ms;\n",
    "\"\"\"\n",
    "\n",
    "# Load the result of the query into a DataFrame\n",
    "df_tweets_af_sankey_staff = pd.read_sql(query_sankey_af_staff, connection)\n",
    "\n",
    "# Convert the datetime column to pandas datetime\n",
    "df_tweets_af_sankey_staff['datetime'] = pd.to_datetime(df_tweets_af_sankey_staff['timestamp_ms'], unit='ms')\n",
    "\n",
    "df_tweets_af_sankey_staff['datetime'] = df_tweets_af_sankey_staff['datetime'].dt.tz_localize('UTC').dt.tz_convert('Europe/Amsterdam')\n",
    "\n",
    "# Initialize empty lists to store sentiments\n",
    "sentiments_initial_tweet = []\n",
    "sentiments_post_reply = []\n",
    "\n",
    "# Group tweets by conversation_id\n",
    "grouped_sankey_af_staff = df_tweets_af_sankey_staff.groupby('conversation_id')\n",
    "\n",
    "# Process each conversation\n",
    "for conversation_id, group in grouped_sankey_af_staff:\n",
    "    # Identify the initial user's tweet (rank 1)\n",
    "    user_initial_tweet = group[(group['user_id'] != af_user_id) & (group['conversation_rank'] == 1)]\n",
    "    \n",
    "    if not user_initial_tweet.empty:\n",
    "        user_id = user_initial_tweet['user_id'].iloc[0]\n",
    "        initial_tweet_time = user_initial_tweet['datetime'].iloc[0]\n",
    "\n",
    "        # Identify the airline's reply to this initial tweet\n",
    "        airline_reply = group[(group['user_id'] == af_user_id) & (group['datetime'] > initial_tweet_time)]\n",
    "        if not airline_reply.empty:\n",
    "            airline_reply_time = airline_reply['datetime'].iloc[0]\n",
    "\n",
    "            # Identify the user's reply to the airline's response\n",
    "            user_followup_tweet = group[(group['user_id'] == user_id) & (group['datetime'] > airline_reply_time)]\n",
    "            if not user_followup_tweet.empty:\n",
    "                # Append sentiments for analysis\n",
    "                sentiments_initial_tweet.append(user_initial_tweet['label'].iloc[0])\n",
    "                sentiments_post_reply.append(user_followup_tweet['label'].iloc[0])\n",
    "\n",
    "# Step 1: Prepare data for Sankey diagram\n",
    "transitions = pd.DataFrame({\n",
    "    'initial': sentiments_initial_tweet,\n",
    "    'post_reply': sentiments_post_reply\n",
    "})\n",
    "\n",
    "# Count transitions\n",
    "transition_counts = transitions.value_counts().reset_index(name='count')\n",
    "\n",
    "# Add percentage to transition_counts\n",
    "total_counts = transition_counts['count'].sum()\n",
    "transition_counts['percentage'] = (transition_counts['count'] / total_counts * 100).round(2)\n",
    "\n",
    "# Define nodes (initial sentiments and final grouped sentiments)\n",
    "nodes = ['positive', 'negative', 'neutral', 'positive_end', 'negative_end', 'neutral_end']\n",
    "node_indices = {label: i for i, label in enumerate(nodes)}\n",
    "\n",
    "# Prepare data for Sankey diagram\n",
    "source = [node_indices[row['initial']] for _, row in transition_counts.iterrows()]\n",
    "target = [node_indices[f\"{row['post_reply']}_end\"] for _, row in transition_counts.iterrows()]\n",
    "value = transition_counts['count'].tolist()\n",
    "label = [f\"{row['initial']} to {row['post_reply']}: {row['count']} ({row['percentage']}%)\" for _, row in transition_counts.iterrows()]\n",
    "\n",
    "# Define colors for nodes and links\n",
    "node_colors = {\n",
    "    'positive': 'rgb(44, 160, 44)',  # green\n",
    "    'negative': 'rgb(214, 39, 40)',  # red\n",
    "    'neutral': 'rgb(255, 165, 0)',   # orange\n",
    "    'positive_end': 'rgb(44, 160, 44)',  # green for final positive\n",
    "    'negative_end': 'rgb(214, 39, 40)',  # red for final negative\n",
    "    'neutral_end': 'rgb(255, 165, 0)'    # orange for final neutral\n",
    "}\n",
    "\n",
    "transition_colors = {\n",
    "    'positive': 'rgba(44, 160, 44, 0.4)',  # light green\n",
    "    'negative': 'rgba(214, 39, 40, 0.4)',  # light red\n",
    "    'neutral': 'rgba(255, 165, 0, 0.4)'    # light orange\n",
    "}\n",
    "\n",
    "# Create color list for nodes\n",
    "colors = [node_colors[node] for node in nodes]\n",
    "\n",
    "# Create the Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=30,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=['Positive', 'Negative', 'Neutral', 'Positive End', 'Negative End', 'Neutral End'],\n",
    "        color=colors\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value,\n",
    "        color=[transition_colors[row['initial']] for _, row in transition_counts.iterrows()],  # Use bland colors for transitions\n",
    "        customdata=label,\n",
    "        hovertemplate='%{customdata}<extra></extra>',\n",
    "        label=label  # Add labels directly to the links\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Sentiment Transition (Air France, staff)\", \n",
    "    font_size=12,\n",
    "    width=1200,  # increased width to accommodate the legend\n",
    "    height=700,\n",
    "    margin=dict(l=50, r=50, t=50, b=50)  # Adjusted margins\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the airline user ID\n",
    "af_user_id = 106062176\n",
    "\n",
    "# Define the query to get all tweets in the conversations involving British Airways\n",
    "query_sankey_af_delay = f\"\"\"\n",
    "SELECT tweets.text, tweets.label, tweets.user_id, tweets.timestamp_ms, hasher.conversation_id, hasher.conversation_rank\n",
    "FROM tweets\n",
    "JOIN hasher ON hasher.id = tweets.id\n",
    "JOIN conversations ON hasher.conversation_id = conversations.conversation_id\n",
    "WHERE conversations.airline LIKE '%AirFrance%' \n",
    "    AND tweets.delay_and_cancellation = 1\n",
    "    AND tweets.timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "    AND tweets.timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000\n",
    "ORDER BY tweets.timestamp_ms;\n",
    "\"\"\"\n",
    "\n",
    "# Load the result of the query into a DataFrame\n",
    "df_tweets_af_sankey_delay = pd.read_sql(query_sankey_af_delay, connection)\n",
    "\n",
    "# Convert the datetime column to pandas datetime\n",
    "df_tweets_af_sankey_delay['datetime'] = pd.to_datetime(df_tweets_af_sankey_delay['timestamp_ms'], unit='ms')\n",
    "\n",
    "df_tweets_af_sankey_delay['datetime'] = df_tweets_af_sankey_delay['datetime'].dt.tz_localize('UTC').dt.tz_convert('Europe/Amsterdam')\n",
    "\n",
    "# Initialize empty lists to store sentiments\n",
    "sentiments_initial_tweet = []\n",
    "sentiments_post_reply = []\n",
    "\n",
    "# Group tweets by conversation_id\n",
    "grouped_sankey_af_delay = df_tweets_af_sankey_delay.groupby('conversation_id')\n",
    "\n",
    "# Process each conversation\n",
    "for conversation_id, group in grouped_sankey_af_delay:\n",
    "    # Identify the initial user's tweet (rank 1)\n",
    "    user_initial_tweet = group[(group['user_id'] != af_user_id) & (group['conversation_rank'] == 1)]\n",
    "    \n",
    "    if not user_initial_tweet.empty:\n",
    "        user_id = user_initial_tweet['user_id'].iloc[0]\n",
    "        initial_tweet_time = user_initial_tweet['datetime'].iloc[0]\n",
    "\n",
    "        # Identify the airline's reply to this initial tweet\n",
    "        airline_reply = group[(group['user_id'] == af_user_id) & (group['datetime'] > initial_tweet_time)]\n",
    "        if not airline_reply.empty:\n",
    "            airline_reply_time = airline_reply['datetime'].iloc[0]\n",
    "\n",
    "            # Identify the user's reply to the airline's response\n",
    "            user_followup_tweet = group[(group['user_id'] == user_id) & (group['datetime'] > airline_reply_time)]\n",
    "            if not user_followup_tweet.empty:\n",
    "                # Append sentiments for analysis\n",
    "                sentiments_initial_tweet.append(user_initial_tweet['label'].iloc[0])\n",
    "                sentiments_post_reply.append(user_followup_tweet['label'].iloc[0])\n",
    "\n",
    "# Step 1: Prepare data for Sankey diagram\n",
    "transitions = pd.DataFrame({\n",
    "    'initial': sentiments_initial_tweet,\n",
    "    'post_reply': sentiments_post_reply\n",
    "})\n",
    "\n",
    "# Count transitions\n",
    "transition_counts = transitions.value_counts().reset_index(name='count')\n",
    "\n",
    "# Add percentage to transition_counts\n",
    "total_counts = transition_counts['count'].sum()\n",
    "transition_counts['percentage'] = (transition_counts['count'] / total_counts * 100).round(2)\n",
    "\n",
    "# Define nodes (initial sentiments and final grouped sentiments)\n",
    "nodes = ['positive', 'negative', 'neutral', 'positive_end', 'negative_end', 'neutral_end']\n",
    "node_indices = {label: i for i, label in enumerate(nodes)}\n",
    "\n",
    "# Prepare data for Sankey diagram\n",
    "source = [node_indices[row['initial']] for _, row in transition_counts.iterrows()]\n",
    "target = [node_indices[f\"{row['post_reply']}_end\"] for _, row in transition_counts.iterrows()]\n",
    "value = transition_counts['count'].tolist()\n",
    "label = [f\"{row['initial']} to {row['post_reply']}: {row['count']} ({row['percentage']}%)\" for _, row in transition_counts.iterrows()]\n",
    "\n",
    "# Define colors for nodes and links\n",
    "node_colors = {\n",
    "    'positive': 'rgb(44, 160, 44)',  # green\n",
    "    'negative': 'rgb(214, 39, 40)',  # red\n",
    "    'neutral': 'rgb(255, 165, 0)',   # orange\n",
    "    'positive_end': 'rgb(44, 160, 44)',  # green for final positive\n",
    "    'negative_end': 'rgb(214, 39, 40)',  # red for final negative\n",
    "    'neutral_end': 'rgb(255, 165, 0)'    # orange for final neutral\n",
    "}\n",
    "\n",
    "transition_colors = {\n",
    "    'positive': 'rgba(44, 160, 44, 0.4)',  # light green\n",
    "    'negative': 'rgba(214, 39, 40, 0.4)',  # light red\n",
    "    'neutral': 'rgba(255, 165, 0, 0.4)'    # light orange\n",
    "}\n",
    "\n",
    "# Create color list for nodes\n",
    "colors = [node_colors[node] for node in nodes]\n",
    "\n",
    "# Create the Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=30,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=['Positive', 'Negative', 'Neutral', 'Positive End', 'Negative End', 'Neutral End'],\n",
    "        color=colors,\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value,\n",
    "        color=[transition_colors[row['initial']] for _, row in transition_counts.iterrows()],  # Use bland colors for transitions\n",
    "        customdata=label,\n",
    "        hovertemplate='%{customdata}<extra></extra>',\n",
    "        label=label  # Add labels directly to the links\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Sentiment Transition (Air France, delay and cancellation)\", \n",
    "    font_size=20,\n",
    "    width=1200,  # increased width to accommodate the legend\n",
    "    height=700,\n",
    "    margin=dict(l=50, r=50, t=50, b=50)  # Adjusted margins\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the airline user ID\n",
    "airline_user_id = 18332190\n",
    "\n",
    "# Define the query to get all tweets in the conversations involving British Airways\n",
    "query_sankey_ba_general = f\"\"\"\n",
    "SELECT tweets.text, tweets.label, tweets.user_id, tweets.timestamp_ms, hasher.conversation_id, hasher.conversation_rank\n",
    "FROM tweets\n",
    "JOIN hasher ON hasher.id = tweets.id\n",
    "JOIN conversations ON hasher.conversation_id = conversations.conversation_id\n",
    "WHERE conversations.airline LIKE '%British_Airways%' \n",
    "    AND tweets.timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "    AND tweets.timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000\n",
    "ORDER BY tweets.timestamp_ms;\n",
    "\"\"\"\n",
    "\n",
    "# Load the result of the query into a DataFrame\n",
    "df_tweets_british_airways_sankey = pd.read_sql(query_sankey_ba_general, connection)\n",
    "\n",
    "# Convert the datetime column to pandas datetime\n",
    "df_tweets_british_airways_sankey['datetime'] = pd.to_datetime(df_tweets_british_airways_sankey['timestamp_ms'], unit='ms')\n",
    "\n",
    "df_tweets_british_airways_sankey['datetime'] = df_tweets_british_airways_sankey['datetime'].dt.tz_localize('UTC').dt.tz_convert('Europe/Amsterdam')\n",
    "\n",
    "# Initialize empty lists to store sentiments\n",
    "sentiments_initial_tweet = []\n",
    "sentiments_post_reply = []\n",
    "\n",
    "# Group tweets by conversation_id\n",
    "grouped_sankey_ba = df_tweets_british_airways_sankey.groupby('conversation_id')\n",
    "\n",
    "# Process each conversation\n",
    "for conversation_id, group in grouped_sankey_ba:\n",
    "    # Identify the initial user's tweet (rank 1)\n",
    "    user_initial_tweet = group[(group['user_id'] != airline_user_id) & (group['conversation_rank'] == 1)]\n",
    "    \n",
    "    if not user_initial_tweet.empty:\n",
    "        user_id = user_initial_tweet['user_id'].iloc[0]\n",
    "        initial_tweet_time = user_initial_tweet['datetime'].iloc[0]\n",
    "\n",
    "        # Identify the airline's reply to this initial tweet\n",
    "        airline_reply = group[(group['user_id'] == airline_user_id) & (group['datetime'] > initial_tweet_time)]\n",
    "        if not airline_reply.empty:\n",
    "            airline_reply_time = airline_reply['datetime'].iloc[0]\n",
    "\n",
    "            # Identify the user's reply to the airline's response\n",
    "            user_followup_tweet = group[(group['user_id'] == user_id) & (group['datetime'] > airline_reply_time)]\n",
    "            if not user_followup_tweet.empty:\n",
    "                # Append sentiments for analysis\n",
    "                sentiments_initial_tweet.append(user_initial_tweet['label'].iloc[0])\n",
    "                sentiments_post_reply.append(user_followup_tweet['label'].iloc[0])\n",
    "\n",
    "# Step 1: Prepare data for Sankey diagram\n",
    "transitions = pd.DataFrame({\n",
    "    'initial': sentiments_initial_tweet,\n",
    "    'post_reply': sentiments_post_reply\n",
    "})\n",
    "\n",
    "# Count transitions\n",
    "transition_counts = transitions.value_counts().reset_index(name='count')\n",
    "\n",
    "# Add percentage to transition_counts\n",
    "total_counts = transition_counts['count'].sum()\n",
    "transition_counts['percentage'] = (transition_counts['count'] / total_counts * 100).round(2)\n",
    "\n",
    "# Define nodes (initial sentiments and final grouped sentiments)\n",
    "nodes = ['positive', 'negative', 'neutral', 'positive_end', 'negative_end', 'neutral_end']\n",
    "node_indices = {label: i for i, label in enumerate(nodes)}\n",
    "\n",
    "# Prepare data for Sankey diagram\n",
    "source = [node_indices[row['initial']] for _, row in transition_counts.iterrows()]\n",
    "target = [node_indices[f\"{row['post_reply']}_end\"] for _, row in transition_counts.iterrows()]\n",
    "value = transition_counts['count'].tolist()\n",
    "label = [f\"{row['initial']} to {row['post_reply']}: {row['count']} ({row['percentage']}%)\" for _, row in transition_counts.iterrows()]\n",
    "\n",
    "# Define colors for nodes and links\n",
    "node_colors = {\n",
    "    'positive': 'rgb(44, 160, 44)',  # green\n",
    "    'negative': 'rgb(214, 39, 40)',  # red\n",
    "    'neutral': 'rgb(255, 165, 0)',   # orange\n",
    "    'positive_end': 'rgb(44, 160, 44)',  # green for final positive\n",
    "    'negative_end': 'rgb(214, 39, 40)',  # red for final negative\n",
    "    'neutral_end': 'rgb(255, 165, 0)'    # orange for final neutral\n",
    "}\n",
    "\n",
    "transition_colors = {\n",
    "    'positive': 'rgba(44, 160, 44, 0.4)',  # light green\n",
    "    'negative': 'rgba(214, 39, 40, 0.4)',  # light red\n",
    "    'neutral': 'rgba(255, 165, 0, 0.4)'    # light orange\n",
    "}\n",
    "\n",
    "# Create color list for nodes\n",
    "colors = [node_colors[node] for node in nodes]\n",
    "\n",
    "# Create the Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=30,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=['Positive', 'Negative', 'Neutral', 'Positive End', 'Negative End', 'Neutral End'],\n",
    "        color=colors\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value,\n",
    "        color=[transition_colors[row['initial']] for _, row in transition_counts.iterrows()],  # Use bland colors for transitions\n",
    "        customdata=label,\n",
    "        hovertemplate='%{customdata}<extra></extra>',\n",
    "        label=label  # Add labels directly to the links\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Sentiment Transition (British Airways)\", \n",
    "    font_size=18,\n",
    "    width=1200,  # increased width to accommodate the legend\n",
    "    height=700,\n",
    "    margin=dict(l=50, r=50, t=50, b=50)  # Adjusted margins\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the airline user ID\n",
    "klm_user_id = 56377143\n",
    "\n",
    "# Define the query to get all tweets in the conversations involving British Airways\n",
    "query_sankey_klm = f\"\"\"\n",
    "SELECT tweets.text, tweets.label, tweets.user_id, tweets.timestamp_ms, hasher.conversation_id, hasher.conversation_rank\n",
    "FROM tweets\n",
    "JOIN hasher ON hasher.id = tweets.id\n",
    "JOIN conversations ON hasher.conversation_id = conversations.conversation_id\n",
    "WHERE conversations.airline LIKE '%KLM%' \n",
    "    AND tweets.timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "    AND tweets.timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000\n",
    "ORDER BY tweets.timestamp_ms;\n",
    "\"\"\"\n",
    "\n",
    "# Load the result of the query into a DataFrame\n",
    "df_tweets_klm_sankey = pd.read_sql(query_sankey_klm, connection)\n",
    "\n",
    "# Convert the datetime column to pandas datetime\n",
    "df_tweets_klm_sankey['datetime'] = pd.to_datetime(df_tweets_klm_sankey['timestamp_ms'], unit='ms')\n",
    "\n",
    "df_tweets_klm_sankey['datetime'] = df_tweets_klm_sankey['datetime'].dt.tz_localize('UTC').dt.tz_convert('Europe/Amsterdam')\n",
    "\n",
    "# Initialize empty lists to store sentiments\n",
    "sentiments_initial_tweet = []\n",
    "sentiments_post_reply = []\n",
    "\n",
    "# Group tweets by conversation_id\n",
    "grouped_sankey_klm = df_tweets_klm_sankey.groupby('conversation_id')\n",
    "\n",
    "# Process each conversation\n",
    "for conversation_id, group in grouped_sankey_klm:\n",
    "    # Identify the initial user's tweet (rank 1)\n",
    "    user_initial_tweet = group[(group['user_id'] != klm_user_id) & (group['conversation_rank'] == 1)]\n",
    "    \n",
    "    if not user_initial_tweet.empty:\n",
    "        user_id = user_initial_tweet['user_id'].iloc[0]\n",
    "        initial_tweet_time = user_initial_tweet['datetime'].iloc[0]\n",
    "\n",
    "        # Identify the airline's reply to this initial tweet\n",
    "        airline_reply = group[(group['user_id'] == klm_user_id) & (group['datetime'] > initial_tweet_time)]\n",
    "        if not airline_reply.empty:\n",
    "            airline_reply_time = airline_reply['datetime'].iloc[0]\n",
    "\n",
    "            # Identify the user's reply to the airline's response\n",
    "            user_followup_tweet = group[(group['user_id'] == user_id) & (group['datetime'] > airline_reply_time)]\n",
    "            if not user_followup_tweet.empty:\n",
    "                # Append sentiments for analysis\n",
    "                sentiments_initial_tweet.append(user_initial_tweet['label'].iloc[0])\n",
    "                sentiments_post_reply.append(user_followup_tweet['label'].iloc[0])\n",
    "\n",
    "# Step 1: Prepare data for Sankey diagram\n",
    "transitions = pd.DataFrame({\n",
    "    'initial': sentiments_initial_tweet,\n",
    "    'post_reply': sentiments_post_reply\n",
    "})\n",
    "\n",
    "# Count transitions\n",
    "transition_counts = transitions.value_counts().reset_index(name='count')\n",
    "\n",
    "# Add percentage to transition_counts\n",
    "total_counts = transition_counts['count'].sum()\n",
    "transition_counts['percentage'] = (transition_counts['count'] / total_counts * 100).round(2)\n",
    "\n",
    "# Define nodes (initial sentiments and final grouped sentiments)\n",
    "nodes = ['positive', 'negative', 'neutral', 'positive_end', 'negative_end', 'neutral_end']\n",
    "node_indices = {label: i for i, label in enumerate(nodes)}\n",
    "\n",
    "# Prepare data for Sankey diagram\n",
    "source = [node_indices[row['initial']] for _, row in transition_counts.iterrows()]\n",
    "target = [node_indices[f\"{row['post_reply']}_end\"] for _, row in transition_counts.iterrows()]\n",
    "value = transition_counts['count'].tolist()\n",
    "label = [f\"{row['initial']} to {row['post_reply']}: {row['count']} ({row['percentage']}%)\" for _, row in transition_counts.iterrows()]\n",
    "\n",
    "# Define colors for nodes and links\n",
    "node_colors = {\n",
    "    'positive': 'rgb(44, 160, 44)',  # green\n",
    "    'negative': 'rgb(214, 39, 40)',  # red\n",
    "    'neutral': 'rgb(255, 165, 0)',   # orange\n",
    "    'positive_end': 'rgb(44, 160, 44)',  # green for final positive\n",
    "    'negative_end': 'rgb(214, 39, 40)',  # red for final negative\n",
    "    'neutral_end': 'rgb(255, 165, 0)'    # orange for final neutral\n",
    "}\n",
    "\n",
    "transition_colors = {\n",
    "    'positive': 'rgba(44, 160, 44, 0.4)',  # light green\n",
    "    'negative': 'rgba(214, 39, 40, 0.4)',  # light red\n",
    "    'neutral': 'rgba(255, 165, 0, 0.4)'    # light orange\n",
    "}\n",
    "\n",
    "# Create color list for nodes\n",
    "colors = [node_colors[node] for node in nodes]\n",
    "\n",
    "# Create the Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=30,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=['Positive', 'Negative', 'Neutral', 'Positive End', 'Negative End', 'Neutral End'],\n",
    "        color=colors\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value,\n",
    "        color=[transition_colors[row['initial']] for _, row in transition_counts.iterrows()],  # Use bland colors for transitions\n",
    "        customdata=label,\n",
    "        hovertemplate='%{customdata}<extra></extra>',\n",
    "        label=label  # Add labels directly to the links\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Sentiment Transition (KLM)\", \n",
    "    font_size=12,\n",
    "    width=1200,  # increased width to accommodate the legend\n",
    "    height=700,\n",
    "    margin=dict(l=50, r=50, t=50, b=50)  # Adjusted margins\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the airline user ID\n",
    "lu_user_id = 124476322\n",
    "\n",
    "# Define the query to get all tweets in the conversations involving British Airways\n",
    "query_sankey_lu = f\"\"\"\n",
    "SELECT tweets.text, tweets.label, tweets.user_id, tweets.timestamp_ms, hasher.conversation_id, hasher.conversation_rank\n",
    "FROM tweets\n",
    "JOIN hasher ON hasher.id = tweets.id\n",
    "JOIN conversations ON hasher.conversation_id = conversations.conversation_id\n",
    "WHERE conversations.airline LIKE '%Lufthansa%' \n",
    "    AND tweets.timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "    AND tweets.timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000\n",
    "ORDER BY tweets.timestamp_ms;\n",
    "\"\"\"\n",
    "\n",
    "# Load the result of the query into a DataFrame\n",
    "df_tweets_lu_sankey = pd.read_sql(query_sankey_lu, connection)\n",
    "\n",
    "# Convert the datetime column to pandas datetime\n",
    "df_tweets_lu_sankey['datetime'] = pd.to_datetime(df_tweets_lu_sankey['timestamp_ms'], unit='ms')\n",
    "\n",
    "df_tweets_lu_sankey['datetime'] = df_tweets_lu_sankey['datetime'].dt.tz_localize('UTC').dt.tz_convert('Europe/Amsterdam')\n",
    "\n",
    "# Initialize empty lists to store sentiments\n",
    "sentiments_initial_tweet = []\n",
    "sentiments_post_reply = []\n",
    "\n",
    "# Group tweets by conversation_id\n",
    "grouped_sankey_lu= df_tweets_lu_sankey.groupby('conversation_id')\n",
    "\n",
    "# Process each conversation\n",
    "for conversation_id, group in grouped_sankey_lu:\n",
    "    # Identify the initial user's tweet (rank 1)\n",
    "    user_initial_tweet = group[(group['user_id'] != lu_user_id) & (group['conversation_rank'] == 1)]\n",
    "    \n",
    "    if not user_initial_tweet.empty:\n",
    "        user_id = user_initial_tweet['user_id'].iloc[0]\n",
    "        initial_tweet_time = user_initial_tweet['datetime'].iloc[0]\n",
    "\n",
    "        # Identify the airline's reply to this initial tweet\n",
    "        airline_reply = group[(group['user_id'] == lu_user_id) & (group['datetime'] > initial_tweet_time)]\n",
    "        if not airline_reply.empty:\n",
    "            airline_reply_time = airline_reply['datetime'].iloc[0]\n",
    "\n",
    "            # Identify the user's reply to the airline's response\n",
    "            user_followup_tweet = group[(group['user_id'] == user_id) & (group['datetime'] > airline_reply_time)]\n",
    "            if not user_followup_tweet.empty:\n",
    "                # Append sentiments for analysis\n",
    "                sentiments_initial_tweet.append(user_initial_tweet['label'].iloc[0])\n",
    "                sentiments_post_reply.append(user_followup_tweet['label'].iloc[0])\n",
    "\n",
    "# Step 1: Prepare data for Sankey diagram\n",
    "transitions = pd.DataFrame({\n",
    "    'initial': sentiments_initial_tweet,\n",
    "    'post_reply': sentiments_post_reply\n",
    "})\n",
    "\n",
    "# Count transitions\n",
    "transition_counts = transitions.value_counts().reset_index(name='count')\n",
    "\n",
    "# Add percentage to transition_counts\n",
    "total_counts = transition_counts['count'].sum()\n",
    "transition_counts['percentage'] = (transition_counts['count'] / total_counts * 100).round(2)\n",
    "\n",
    "# Define nodes (initial sentiments and final grouped sentiments)\n",
    "nodes = ['positive', 'negative', 'neutral', 'positive_end', 'negative_end', 'neutral_end']\n",
    "node_indices = {label: i for i, label in enumerate(nodes)}\n",
    "\n",
    "# Prepare data for Sankey diagram\n",
    "source = [node_indices[row['initial']] for _, row in transition_counts.iterrows()]\n",
    "target = [node_indices[f\"{row['post_reply']}_end\"] for _, row in transition_counts.iterrows()]\n",
    "value = transition_counts['count'].tolist()\n",
    "label = [f\"{row['initial']} to {row['post_reply']}: {row['count']} ({row['percentage']}%)\" for _, row in transition_counts.iterrows()]\n",
    "\n",
    "# Define colors for nodes and links\n",
    "node_colors = {\n",
    "    'positive': 'rgb(44, 160, 44)',  # green\n",
    "    'negative': 'rgb(214, 39, 40)',  # red\n",
    "    'neutral': 'rgb(255, 165, 0)',   # orange\n",
    "    'positive_end': 'rgb(44, 160, 44)',  # green for final positive\n",
    "    'negative_end': 'rgb(214, 39, 40)',  # red for final negative\n",
    "    'neutral_end': 'rgb(255, 165, 0)'    # orange for final neutral\n",
    "}\n",
    "\n",
    "transition_colors = {\n",
    "    'positive': 'rgba(44, 160, 44, 0.4)',  # light green\n",
    "    'negative': 'rgba(214, 39, 40, 0.4)',  # light red\n",
    "    'neutral': 'rgba(255, 165, 0, 0.4)'    # light orange\n",
    "}\n",
    "\n",
    "# Create color list for nodes\n",
    "colors = [node_colors[node] for node in nodes]\n",
    "\n",
    "# Create the Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=30,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=['Positive', 'Negative', 'Neutral', 'Positive End', 'Negative End', 'Neutral End'],\n",
    "        color=colors\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value,\n",
    "        color=[transition_colors[row['initial']] for _, row in transition_counts.iterrows()],  # Use bland colors for transitions\n",
    "        customdata=label,\n",
    "        hovertemplate='%{customdata}<extra></extra>',\n",
    "        label=label  # Add labels directly to the links\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Sentiment Transition (Lufthansa)\", \n",
    "    font_size=12,\n",
    "    width=1200,  # increased width to accommodate the legend\n",
    "    height=700,\n",
    "    margin=dict(l=50, r=50, t=50, b=50)  # Adjusted margins\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the airline user ID\n",
    "af_user_id = 106062176\n",
    "\n",
    "# Define the query to get all tweets in the conversations involving British Airways\n",
    "query_sankey_af = f\"\"\"\n",
    "SELECT tweets.text, tweets.label, tweets.user_id, tweets.timestamp_ms, hasher.conversation_id, hasher.conversation_rank\n",
    "FROM tweets\n",
    "JOIN hasher ON hasher.id = tweets.id\n",
    "JOIN conversations ON hasher.conversation_id = conversations.conversation_id\n",
    "WHERE conversations.airline LIKE '%AirFrance%' \n",
    "    AND tweets.timestamp_ms >= UNIX_TIMESTAMP('{start_date}') * 1000\n",
    "    AND tweets.timestamp_ms <= UNIX_TIMESTAMP('{end_date}') * 1000\n",
    "ORDER BY tweets.timestamp_ms;\n",
    "\"\"\"\n",
    "\n",
    "# Load the result of the query into a DataFrame\n",
    "df_tweets_af_sankey = pd.read_sql(query_sankey_af, connection)\n",
    "\n",
    "# Convert the datetime column to pandas datetime\n",
    "df_tweets_af_sankey['datetime'] = pd.to_datetime(df_tweets_af_sankey['timestamp_ms'], unit='ms')\n",
    "\n",
    "df_tweets_af_sankey['datetime'] = df_tweets_af_sankey['datetime'].dt.tz_localize('UTC').dt.tz_convert('Europe/Amsterdam')\n",
    "\n",
    "# Initialize empty lists to store sentiments\n",
    "sentiments_initial_tweet = []\n",
    "sentiments_post_reply = []\n",
    "\n",
    "# Group tweets by conversation_id\n",
    "grouped_sankey_af = df_tweets_af_sankey.groupby('conversation_id')\n",
    "\n",
    "# Process each conversation\n",
    "for conversation_id, group in grouped_sankey_af:\n",
    "    # Identify the initial user's tweet (rank 1)\n",
    "    user_initial_tweet = group[(group['user_id'] != af_user_id) & (group['conversation_rank'] == 1)]\n",
    "    \n",
    "    if not user_initial_tweet.empty:\n",
    "        user_id = user_initial_tweet['user_id'].iloc[0]\n",
    "        initial_tweet_time = user_initial_tweet['datetime'].iloc[0]\n",
    "\n",
    "        # Identify the airline's reply to this initial tweet\n",
    "        airline_reply = group[(group['user_id'] == af_user_id) & (group['datetime'] > initial_tweet_time)]\n",
    "        if not airline_reply.empty:\n",
    "            airline_reply_time = airline_reply['datetime'].iloc[0]\n",
    "\n",
    "            # Identify the user's reply to the airline's response\n",
    "            user_followup_tweet = group[(group['user_id'] == user_id) & (group['datetime'] > airline_reply_time)]\n",
    "            if not user_followup_tweet.empty:\n",
    "                # Append sentiments for analysis\n",
    "                sentiments_initial_tweet.append(user_initial_tweet['label'].iloc[0])\n",
    "                sentiments_post_reply.append(user_followup_tweet['label'].iloc[0])\n",
    "\n",
    "# Step 1: Prepare data for Sankey diagram\n",
    "transitions = pd.DataFrame({\n",
    "    'initial': sentiments_initial_tweet,\n",
    "    'post_reply': sentiments_post_reply\n",
    "})\n",
    "\n",
    "# Count transitions\n",
    "transition_counts = transitions.value_counts().reset_index(name='count')\n",
    "\n",
    "# Add percentage to transition_counts\n",
    "total_counts = transition_counts['count'].sum()\n",
    "transition_counts['percentage'] = (transition_counts['count'] / total_counts * 100).round(2)\n",
    "\n",
    "# Define nodes (initial sentiments and final grouped sentiments)\n",
    "nodes = ['positive', 'negative', 'neutral', 'positive_end', 'negative_end', 'neutral_end']\n",
    "node_indices = {label: i for i, label in enumerate(nodes)}\n",
    "\n",
    "# Prepare data for Sankey diagram\n",
    "source = [node_indices[row['initial']] for _, row in transition_counts.iterrows()]\n",
    "target = [node_indices[f\"{row['post_reply']}_end\"] for _, row in transition_counts.iterrows()]\n",
    "value = transition_counts['count'].tolist()\n",
    "label = [f\"{row['initial']} to {row['post_reply']}: {row['count']} ({row['percentage']}%)\" for _, row in transition_counts.iterrows()]\n",
    "\n",
    "# Define colors for nodes and links\n",
    "node_colors = {\n",
    "    'positive': 'rgb(44, 160, 44)',  # green\n",
    "    'negative': 'rgb(214, 39, 40)',  # red\n",
    "    'neutral': 'rgb(255, 165, 0)',   # orange\n",
    "    'positive_end': 'rgb(44, 160, 44)',  # green for final positive\n",
    "    'negative_end': 'rgb(214, 39, 40)',  # red for final negative\n",
    "    'neutral_end': 'rgb(255, 165, 0)'    # orange for final neutral\n",
    "}\n",
    "\n",
    "transition_colors = {\n",
    "    'positive': 'rgba(44, 160, 44, 0.4)',  # light green\n",
    "    'negative': 'rgba(214, 39, 40, 0.4)',  # light red\n",
    "    'neutral': 'rgba(255, 165, 0, 0.4)'    # light orange\n",
    "}\n",
    "\n",
    "# Create color list for nodes\n",
    "colors = [node_colors[node] for node in nodes]\n",
    "\n",
    "# Create the Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=30,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=['Positive', 'Negative', 'Neutral', 'Positive End', 'Negative End', 'Neutral End'],\n",
    "        color=colors\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value,\n",
    "        color=[transition_colors[row['initial']] for _, row in transition_counts.iterrows()],  # Use bland colors for transitions\n",
    "        customdata=label,\n",
    "        hovertemplate='%{customdata}<extra></extra>',\n",
    "        label=label  # Add labels directly to the links\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Sentiment Transition (Air France)\", \n",
    "    font_size=12,\n",
    "    width=1200,  # increased width to accommodate the legend\n",
    "    height=700,\n",
    "    margin=dict(l=50, r=50, t=50, b=50)  # Adjusted margins\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
