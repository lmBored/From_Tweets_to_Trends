{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiofiles\n",
    "from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import re\n",
    "import timeit\n",
    "import logging\n",
    "import json\n",
    "import itertools\n",
    "import csv\n",
    "import datetime\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the root directory to sys.path\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(root_dir)\n",
    "os.chdir(root_dir)\n",
    "\n",
    "# Ensure the logging directory exists\n",
    "tmp_dir = os.path.join(root_dir, 'tmp')\n",
    "os.makedirs(tmp_dir, exist_ok=True)\n",
    "\n",
    "log_file = os.path.join(tmp_dir, 'tweets_loading.log')\n",
    "logging.basicConfig(filename=log_file, level=logging.DEBUG, \n",
    "                    format='%(asctime)s %(levelname)s %(name)s %(message)s')\n",
    "logger=logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(root_dir)\n",
    "data = [Path(\"data/\"+file) for file in os.listdir('data')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "# import time\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "configr = AutoConfig.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "# device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_with_missed_data = ['data/airlines-1565894560588.json',\n",
    "                        'data/airlines-1569957146471.json',\n",
    "                        'data/airlines-1573229502947.json',\n",
    "                        'data/airlines-1575313134067.json',\n",
    "                        'data/airlines-1570104381202.json',\n",
    "                        'data/airlines-1560138591670.json',\n",
    "                        'data/airlines-1560138591670.json']\n",
    "\n",
    "airlines_dict = {\"KLM\": 56377143 ,\n",
    "                \"AirFrance\": 106062176 ,\n",
    "                \"British_Airways\": 18332190 ,\n",
    "                \"AmericanAir\": 22536055 ,\n",
    "                \"Lufthansa\": 124476322 ,\n",
    "                \"AirBerlin\": 26223583 ,\n",
    "                \"AirBerlin assist\": 2182373406 ,\n",
    "                \"easyJet\": 38676903 ,\n",
    "                \"RyanAir\": 1542862735 ,\n",
    "                \"SingaporeAir\": 253340062 ,\n",
    "                \"Qantas\": 218730857 ,\n",
    "                \"EtihadAirways\": 45621423 ,\n",
    "                \"VirginAtlantic\": 20626359\n",
    "                }\n",
    "\n",
    "tweets_keys = ['id',\n",
    "                'text',\n",
    "                'in_reply_to_status_id',\n",
    "                'coordinates',\n",
    "                'timestamp_ms',\n",
    "                'quoted_status_id']\n",
    "\n",
    "users_keys = ['id',\n",
    "            'verified',\n",
    "            'followers_count',\n",
    "            'statuses_count']\n",
    "\n",
    "airlines_list_dict = {\"KLM\":  ['klm'],\n",
    "                    \"AirFrance\":  ['airfrance', 'air france'],\n",
    "                    \"British_Airways\":  ['british_airways', 'british airways'],\n",
    "                    \"AmericanAir\":  ['americanair', 'american airlines', 'american air'],\n",
    "                    \"Lufthansa\":  ['lufthansa'],\n",
    "                    \"AirBerlin\":  ['airberlin', 'air berlin'],\n",
    "                    \"AirBerlin assist\":  ['airberlin assist', 'air berlin assist', 'airberlinassist'],\n",
    "                    \"easyJet\":  ['easyjet', 'easy jet'],\n",
    "                    \"RyanAir\":  ['ryanair', 'ryan air'],\n",
    "                    \"SingaporeAir\":  ['singaporeair', 'singapore airlines', 'singapore air'],\n",
    "                    \"Qantas\":  ['qantas'],\n",
    "                    \"EtihadAirways\":  ['etihad airways', 'etihadairways', 'etihad'],\n",
    "                    \"VirginAtlantic\":  ['virgin atlantic', 'virginatlantic']}\n",
    "\n",
    "languages_list = ['en', 'de', 'es', 'fr', 'nl', 'it']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text(text):\n",
    "    text = re.sub(r'([A-Za-z])\\1{2,}', r'\\1\\1', text) # replace repeated texts, normalization\n",
    "    text = re.sub(r'[^A-Za-z0-9]+', '', text) # remove special characters\n",
    "    text = re.sub(r'@\\S+', '@user', text) # replace user mentions\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roberta(text, tokenizer, model, configr):\n",
    "    ptext = transform_text(text)\n",
    "    \n",
    "    input = tokenizer(ptext, return_tensors='pt')\n",
    "    input = input.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(**input)\n",
    "        \n",
    "    scores = output[0][0].cpu().detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    \n",
    "    sentiment_score = scores[2].item() - scores[0].item()\n",
    "    max_score = np.argmax(scores)\n",
    "    label = configr.id2label[max_score]\n",
    "    \n",
    "    return label, sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_transformer(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = re.sub(r' 0 ', 'zero', text) # replace 0 with zero\n",
    "    # text = re.sub(r'([A-Za-z])\\1{2,}', r'\\1', text) # replace repeated texts, normalization\n",
    "    # text = re.sub(r'[^A-Za-z ]', '', text) # remove special characters\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    text = re.sub(r'[,.!?]', '', text)\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor_tweets(tweet, tokenizer, model, configr):\n",
    "    try:\n",
    "        if 'delete' not in tweet:\n",
    "            # start_time = time.time()\n",
    "            # Get the text from the tweet\n",
    "            text = tweet['text']\n",
    "            if 'retweeted_status' in tweet:\n",
    "                if 'extended_tweet' in tweet['retweeted_status']:\n",
    "                    text = tweet['retweeted_status']['extended_tweet']['full_text']  # Get the full text from the extended tweet\n",
    "                \n",
    "            text = text_transformer(text)  # Apply text transformation\n",
    "\n",
    "            # Initialize a dictionary to store tweet information\n",
    "            tweets_info = {k:0 if k in ['id', 'in_reply_to_status_id', 'timestamp_ms', 'quoted_status_id'] else 'NULL' for k in tweets_keys}\n",
    "            tweets_info.update({k:v for k,v in tweet.items() if k in tweets_keys})  # Update the dictionary with tweet information\n",
    "            \n",
    "            tweets_info['user_id'] = tweet['user']['id']\n",
    "\n",
    "            if 'coordinates' in tweet and tweet['coordinates'] != None:\n",
    "                # Format the coordinates as a string\n",
    "                tweets_info['coordinates'] = str(tweet['coordinates']['coordinates'])\n",
    "                tweets_info['coordinates'] = re.sub(r'[\\[\\]]', '', tweets_info['coordinates'])\n",
    "                tweets_info['coordinates'] = re.sub(r',', ' and ', tweets_info['coordinates'])\n",
    "            \n",
    "            lang = tweet['lang']  # Get the language of the tweet\n",
    "            # Set language as 'und' if it is not specified\n",
    "            if ('lang' in tweet and tweet['lang'] == None) or 'lang' not in tweet:\n",
    "                lang = 'und'  # Set language as 'und' if it is not specified\n",
    "\n",
    "            if lang not in languages_list:\n",
    "                return None  # Return None if the language is not in the supported languages\n",
    "\n",
    "            # Initialize a list to store mentioned airlines\n",
    "            airlines_mentioned = []\n",
    "            for airline in airlines_list_dict:\n",
    "                for i in airlines_list_dict[airline]:\n",
    "                    if i in text.lower():\n",
    "                        airlines_mentioned.append(airline)  # Add mentioned airlines to the list\n",
    "            \n",
    "            # Initialize a list to store mentioned user IDs\n",
    "            mentioned_id = []\n",
    "            if tweet.get('entities') and tweet['entities'].get('user_mentions'):  # Check if 'entities' and 'user_mentions' exist and are not None\n",
    "                mentioned_id = [i['id'] for i in tweet['entities']['user_mentions']]  # Get the IDs of mentioned users\n",
    "            \n",
    "            label, score = roberta(text, tokenizer, model, configr)\n",
    "                \n",
    "            # Initialize a dictionary to store extended tweet information\n",
    "            # extended_tweets = {'text':text, 'language':lang, 'mentioned_airlines':airlines_mentioned, 'user_mentions':mentioned_id}\n",
    "            extended_tweets = {'text': text, 'language': lang, 'mentioned_airlines': airlines_mentioned, 'user_mentions': mentioned_id, 'label': label, 'score': score}\n",
    "            tweets_info.update(extended_tweets)  # Update the tweet information dictionary with extended tweet information\n",
    "        \n",
    "            if 'retweeted_status' in tweet:\n",
    "                # Initialize a dictionary to store retweeted status information\n",
    "                retweeted_status = {'id': 0}\n",
    "                retweeted_status.update({k:v for k,v in tweet['retweeted_status'].items() if k in ['id']})  # Update the dictionary with retweeted status information\n",
    "                retweeted_status['retweeted_status_id'] = retweeted_status.pop('id')\n",
    "                \n",
    "                if 'user' in tweet['retweeted_status']:\n",
    "                    retweeted_status['retweeted_status_user_id'] = tweet['retweeted_status']['user']['id']\n",
    "                else:\n",
    "                    retweeted_status['retweeted_status_user_id'] = 0\n",
    "                \n",
    "                # Set none values to 'NULL'\n",
    "                for i in retweeted_status:\n",
    "                    if retweeted_status[i] == None:\n",
    "                        retweeted_status[i] == 'NULL'\n",
    "                tweets_info.update(retweeted_status)  # Update the tweet information dictionary with retweeted status information\n",
    "\n",
    "            else: # 'retweeted_status' not in tweet\n",
    "                # Initialize a dictionary to store retweeted status information\n",
    "                retweeted_status = {'retweeted_status_id': 0, 'retweeted_status_user_id': 0}\n",
    "                tweets_info.update(retweeted_status)  # Update the tweet information dictionary with retweeted status information\n",
    "                        \n",
    "            # Set nullable integer values to 0\n",
    "            nullables_int = ['in_reply_to_status_id', 'quoted_status_id']\n",
    "            for i in nullables_int:\n",
    "                if tweets_info[i] == None:\n",
    "                    tweets_info[i] = 0  # Set nullable integer values to 0\n",
    "                    \n",
    "            # Set nullable boolean values to 0\n",
    "            nullables = ['coordinates']\n",
    "            for i in nullables:\n",
    "                if tweets_info[i] == None:\n",
    "                    tweets_info[i] = 'NULL'  # Set nullable values to 'NULL'\n",
    "\n",
    "            # print(f\"Preprocessor time taken: {time.time() - start_time}\")\n",
    "            \n",
    "            return tweets_info  # Return the processed tweet information\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error: {e}\")  # Log the error\n",
    "        logger.error(e)\n",
    "        return None  # Return None in case of an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reader(path):\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                yield json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON for line: {line}\", file=sys.stderr)\n",
    "                logger.error(e)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_adder_tweets(data, output_file = 'tweets_dataset_gpu.csv'):\n",
    "    # Check if the output file already exists and has contents\n",
    "    if os.path.exists(output_file) and os.path.getsize(output_file) > 0:\n",
    "        print(f\"üìõ {output_file} already exists and has contents. Overwrite? [y/n]\")\n",
    "        while True:\n",
    "            # Ask the user if they want to overwrite the file\n",
    "            choice = input()\n",
    "            if choice == 'y':\n",
    "                break\n",
    "            elif choice == 'n':\n",
    "                return\n",
    "            else:\n",
    "                print(\"Invalid choice.\")\n",
    "\n",
    "    # Open the output file in write mode\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = None\n",
    "        elapsed = 0\n",
    "        length_data = len(data)\n",
    "\n",
    "        # Iterate over the data files\n",
    "        for i, path in enumerate(data):\n",
    "            errors = 0\n",
    "            dataset, dataset0 = itertools.tee(reader(path)) # Read the data file\n",
    "            n = sum(1 for _ in dataset0) # Count the number of tweets in the data file\n",
    "            print(f\"üìç Processing: {path}\")\n",
    "            \n",
    "            start = timeit.default_timer() # Start the timer\n",
    "            elapsed_per_tweet = 0\n",
    "            for j, tweet in enumerate(dataset): # Iterate over the tweets in the data file\n",
    "                start_per_tweet = timeit.default_timer()\n",
    "                # Preprocess the tweet\n",
    "                p_tweet = preprocessor_tweets(tweet, tokenizer, model, configr)\n",
    "                if p_tweet is not None:\n",
    "                    # Write the header row if it doesn't exist\n",
    "                    if writer is None:\n",
    "                        writer = csv.DictWriter(file, fieldnames=p_tweet.keys())\n",
    "                        writer.writeheader()\n",
    "                    \n",
    "                    # Initialize a list to store the raw values\n",
    "                    raw_values = []         \n",
    "                    for k, v in p_tweet.items():\n",
    "                        v = str(v)\n",
    "                        v = re.sub(r',', '', v)\n",
    "                        v = re.sub(r'http\\S+', 'url_removed', v)\n",
    "                        v = re.sub(r'\\n', '', v)\n",
    "                        v = v.strip()\n",
    "                        if k not in ['text', 'coordinates', 'mentioned_airlines', 'user_mentions', 'language']:\n",
    "                            v = v.replace(\"'\", \"\")\n",
    "                        else:\n",
    "                            v = \"'\" + v.replace(\"'\", \"\") + \"'\"\n",
    "                        raw_values.append(v)\n",
    "                        \n",
    "                        # Set the values of the tweet to append to the CSV file\n",
    "                        values = \",\".join(raw_values)\n",
    "                        # p_tweet[k] = v\n",
    "\n",
    "                    try:\n",
    "                        # writer.writerow(p_tweet)\n",
    "                        file.write(f\"{values}\\n\") # Write the tweet to the CSV file\n",
    "\n",
    "                    # Handle json.JSONDecodeError exceptions\n",
    "                    except json.JSONDecodeError as jso:\n",
    "                        if path in file_with_missed_data:\n",
    "                            logging.error(f\"File missing. {jso}\")\n",
    "                            logger.error(jso)\n",
    "                            pass\n",
    "\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error: {e}, Tweet: {tweet}\")\n",
    "                        logger.error(e)\n",
    "                        errors += 1\n",
    "                        \n",
    "                    finally:\n",
    "                        duration_per_tweet = timeit.default_timer() - start_per_tweet\n",
    "                        counter_per_tweet = j + 1\n",
    "                        elapsed_per_tweet += duration_per_tweet\n",
    "                        time_remaining_per_tweet = (n - counter_per_tweet) * (elapsed_per_tweet / counter_per_tweet)\n",
    "                        print(f\"üõù Process: {(counter_per_tweet/n)*100:.2f}% - #Ô∏è‚É£ {counter_per_tweet}/{n} tweets processed - ‚è≥ Time remaining : {str(datetime.timedelta(seconds=time_remaining_per_tweet))}\", end='\\r')\n",
    "                        sys.stdout.flush()\n",
    "            print()\n",
    "                \n",
    "            # Calculate the duration of the process\n",
    "            duration = timeit.default_timer() - start\n",
    "\n",
    "            # Print the status of the process\n",
    "            if errors == 0:\n",
    "                print(f\"‚úÖ {path} appended.\")\n",
    "            else:\n",
    "                print(f\"‚ùå {path} not appended processed - {errors} exceptions ignored.\", file=sys.stderr)\n",
    "\n",
    "            # Print the progress of the process\n",
    "            counter = i + 1\n",
    "            elapsed += duration\n",
    "            time_remaining = (length_data - counter) * (elapsed / counter)\n",
    "            print(f\"‚èØÔ∏è Process: {(counter/length_data)*100:.2f}% - #Ô∏è‚É£ {counter}/{length_data} files processed - ‚è≥ Time remaining : {str(datetime.timedelta(seconds=time_remaining))}\")\n",
    "            print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roberta_batch(texts, tokenizer, model, configr):\n",
    "    ptexts = [transform_text(text) for text in texts]\n",
    "    \n",
    "    inputs = tokenizer(ptexts, return_tensors='pt', padding=True, truncation=True)\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    scores = outputs.logits.cpu().numpy()\n",
    "    scores = softmax(scores, axis=1)\n",
    "    \n",
    "    labels = [configr.id2label[np.argmax(score)] for score in scores]\n",
    "    sentiment_scores = [score[2] - score[0] for score in scores]\n",
    "    \n",
    "    return labels, sentiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def reader(path):\n",
    "    async with aiofiles.open(path, mode='r') as f:\n",
    "        async for line in f:\n",
    "            try:\n",
    "                yield json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON for line: {line}\", file=sys.stderr)\n",
    "                logging.error(e)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_preprocess_tweet(executor, tweet):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    return await loop.run_in_executor(executor, preprocessor_tweets, tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/master/anaconda3/envs/jbg030/lib/python3.12/collections/__init__.py:447: RuntimeWarning: coroutine 'csv_adder_tweets' was never awaited\n",
      "  @classmethod\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "async def csv_adder_tweets(data, output_file='tweets_dataset_gpu.csv', batch_size=32):\n",
    "    if os.path.exists(output_file) and os.path.getsize(output_file) > 0:\n",
    "        print(f\"üìõ {output_file} already exists and has contents. Overwrite? [y/n]\")\n",
    "        while True:\n",
    "            choice = input()\n",
    "            if choice == 'y':\n",
    "                break\n",
    "            elif choice == 'n':\n",
    "                return\n",
    "            else:\n",
    "                print(\"Invalid choice.\")\n",
    "\n",
    "    async with aiofiles.open(output_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = None\n",
    "        elapsed = 0\n",
    "        length_data = len(data)\n",
    "\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            for i, path in enumerate(data):\n",
    "                errors = 0\n",
    "                dataset, dataset0 = itertools.tee(await reader(path))\n",
    "                n = sum(1 for _ in dataset0)\n",
    "                print(f\"üìç Processing: {path}\")\n",
    "                \n",
    "                start = timeit.default_timer()\n",
    "                elapsed_per_tweet = 0\n",
    "                tweet_batch = []\n",
    "\n",
    "                for j, tweet in enumerate(dataset):\n",
    "                    start_per_tweet = timeit.default_timer()\n",
    "                    tweet_batch.append(tweet)\n",
    "\n",
    "                    if len(tweet_batch) == batch_size or j == n - 1:\n",
    "                        preprocess_tasks = [async_preprocess_tweet(executor, tweet) for tweet in tweet_batch]\n",
    "                        preprocessed_tweets = await asyncio.gather(*preprocess_tasks)\n",
    "\n",
    "                        texts = [pt['text'] for pt in preprocessed_tweets if pt]\n",
    "                        if texts:\n",
    "                            labels, scores = roberta_batch(texts, tokenizer, model, configr)\n",
    "                            \n",
    "                            for pt, label, score in zip(preprocessed_tweets, labels, scores):\n",
    "                                if pt:\n",
    "                                    p_tweet = pt['tweets_info']\n",
    "                                    p_tweet.update({'label': label, 'score': score})\n",
    "                                    p_tweet.update(pt)\n",
    "\n",
    "                                    if writer is None:\n",
    "                                        writer = csv.DictWriter(file, fieldnames=p_tweet.keys())\n",
    "                                        await writer.writeheader()\n",
    "                                    \n",
    "                                    raw_values = []\n",
    "                                    for k, v in p_tweet.items():\n",
    "                                        v = str(v)\n",
    "                                        v = re.sub(r',', '', v)\n",
    "                                        v = re.sub(r'http\\S+', 'url_removed', v)\n",
    "                                        v = re.sub(r'\\n', '', v)\n",
    "                                        v = v.strip()\n",
    "                                        if k not in ['text', 'coordinates', 'mentioned_airlines', 'user_mentions', 'language']:\n",
    "                                            v = v.replace(\"'\", \"\")\n",
    "                                        else:\n",
    "                                            v = \"'\" + v.replace(\"'\", \"\") + \"'\"\n",
    "                                        raw_values.append(v)\n",
    "                                    \n",
    "                                    values = \",\".join(raw_values)\n",
    "                                    try:\n",
    "                                        await file.write(f\"{values}\\n\")\n",
    "                                    except json.JSONDecodeError as jso:\n",
    "                                        logging.error(f\"File missing. {jso}\")\n",
    "                                    except Exception as e:\n",
    "                                        logging.error(f\"Error: {e}, Tweet: {tweet}\")\n",
    "                                        errors += 1\n",
    "                                        \n",
    "                                    finally:\n",
    "                                        duration_per_tweet = timeit.default_timer() - start_per_tweet\n",
    "                                        counter_per_tweet = j + 1\n",
    "                                        elapsed_per_tweet += duration_per_tweet\n",
    "                                        time_remaining_per_tweet = (n - counter_per_tweet) * (elapsed_per_tweet / counter_per_tweet)\n",
    "                                        print(f\"üõù Process: {(counter_per_tweet/n)*100:.2f}% - #Ô∏è‚É£ {counter_per_tweet}/{n} tweets processed - ‚è≥ Time remaining : {str(datetime.timedelta(seconds=time_remaining_per_tweet))}\", end='\\r')\n",
    "                                        sys.stdout.flush()\n",
    "\n",
    "                        tweet_batch = []\n",
    "\n",
    "                print()\n",
    "                \n",
    "                duration = timeit.default_timer() - start\n",
    "                if errors == 0:\n",
    "                    print(f\"‚úÖ {path} appended.\")\n",
    "                else:\n",
    "                    print(f\"‚ùå {path} not appended processed - {errors} exceptions ignored.\", file=sys.stderr)\n",
    "\n",
    "                counter = i + 1\n",
    "                elapsed += duration\n",
    "                time_remaining = (length_data - counter) * (elapsed / counter)\n",
    "                print(f\"‚èØÔ∏è Process: {(counter/length_data)*100:.2f}% - #Ô∏è‚É£ {counter}/{length_data} files processed - ‚è≥ Time remaining : {str(datetime.timedelta(seconds=time_remaining))}\")\n",
    "                print(\"-----------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jbg030",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
