{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "817ef5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from config import config\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import datetime\n",
    "import logging\n",
    "import timeit\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sentiment import sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "976884fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/master/anaconda3/envs/jbg030/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "# import time\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "configr = AutoConfig.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a64887f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n",
      "-0.7785103525966406\n"
     ]
    }
   ],
   "source": [
    "label, score = sentiment_score.roberta(\"I work 40 hours a week for me to be this poor\", tokenizer, model, configr)\n",
    "print(label)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41c5415b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.248, 'neu': 0.752, 'pos': 0.0, 'compound': -0.561}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "analyzer.polarity_scores(\"I work 40 hours a week for me to be this poor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "730c4785",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = mysql.connector.connect(host=config.get('HOST'), user=config.get('USERNAME'), password=config.get('PASSWORD'),database=config.get('DATABASE'), allow_local_infile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3277c3e2-56b5-4f54-b12a-c3e0fb6b5c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connection.cursor()\n",
    "cursor.execute(\"SELECT * FROM tweets ORDER BY timestamp_ms DESC\")\n",
    "tweets = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a481c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment_column(connection):\n",
    "    add_column_query = \"\"\"\n",
    "    ALTER TABLE `tweets`\n",
    "    ADD COLUMN `sentiment` FLOAT NULL;\n",
    "    \"\"\"\n",
    "    connection.cursor().execute(add_column_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae654a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_sentiment_column(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2831806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    id, text = tweet\n",
    "    try:\n",
    "        score = sentiment_score.roberta(text)\n",
    "        return (id, score)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}, Tweet: {id}\", file=sys.stderr)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eae6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import timeit\n",
    "import datetime\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def update_sentiment_scores(connection):\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"SELECT `id`, `text` FROM `tweets`\")\n",
    "    tweets = cursor.fetchall()\n",
    "    total_tweets = len(tweets)\n",
    "\n",
    "    counter = 0\n",
    "    elapsed = 0\n",
    "    errors = 0\n",
    "\n",
    "    with Pool() as p:\n",
    "        results = []\n",
    "        for tweet in tweets:\n",
    "            start = timeit.default_timer()\n",
    "            res = process_tweet(tweet)\n",
    "            results.append(res)\n",
    "            duration = timeit.default_timer() - start\n",
    "            elapsed += duration\n",
    "            if counter == 0:\n",
    "                time_remaining = 0  # or some default value\n",
    "            else:\n",
    "                time_remaining = (total_tweets - counter) * (elapsed / counter)\n",
    "            print(f\"Processed {counter} out of {total_tweets} tweets. Estimated time left: {str(datetime.timedelta(seconds=time_remaining))}\")\n",
    "            counter += 1\n",
    "\n",
    "    # Filter out None results\n",
    "    results = [r for r in results if r is not None]\n",
    "\n",
    "    # Batch update\n",
    "    update_query = \"UPDATE `tweets` SET `sentiment` = CASE `id` \"\n",
    "    for id, score in results:\n",
    "        update_query += f\"WHEN {id} THEN {score} \"\n",
    "    update_query += \"END WHERE `id` IN (\" + \",\".join(str(id) for id, _ in results) + \")\"\n",
    "    cursor.execute(update_query)\n",
    "    connection.commit()\n",
    "    print(\"Database update completed\")\n",
    "\n",
    "update_sentiment_scores(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1961ead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "def update_sentiment_scores_spark():\n",
    "    # Create a SparkSession with the MySQL JDBC driver\n",
    "    spark = SparkSession.builder \\\n",
    "        .config(\"spark.jars\", \"/path/to/mysql-connector-java-version-bin.jar\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Read environment variables\n",
    "    host = os.getenv('HOST')\n",
    "    username = os.getenv('USERNAME')\n",
    "    password = os.getenv('PASSWORD')\n",
    "    database = os.getenv('DATABASE')\n",
    "\n",
    "    # JDBC connection string\n",
    "    url = f\"jdbc:mysql://{host}:3306/{database}?user={username}&password={password}\"\n",
    "\n",
    "    # Load the tweets into a DataFrame\n",
    "    df = spark.read.jdbc(url=url, table=\"tweets\")\n",
    "\n",
    "    # Define a UDF to calculate the sentiment score\n",
    "    sentiment_score_udf = udf(lambda text: sentiment_score.roberta(text), FloatType())\n",
    "\n",
    "    # Calculate the sentiment scores\n",
    "    df = df.withColumn(\"sentiment\", sentiment_score_udf(df[\"text\"]))\n",
    "\n",
    "    # Write the updated DataFrame back to the database\n",
    "    df.write.jdbc(url=url, table=\"tweets\", mode=\"overwrite\")\n",
    "\n",
    "update_sentiment_scores_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485f33ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sentiment_scores(connection):\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"SELECT `id`, `text` FROM `tweets`\")\n",
    "    tweets = cursor.fetchall()\n",
    "\n",
    "    counter = 0\n",
    "    elapsed = 0\n",
    "    n = len(tweets)\n",
    "    errors = 0\n",
    "\n",
    "    for tweet in tweets:\n",
    "        start = timeit.default_timer()\n",
    "        id, text = tweet\n",
    "        try:\n",
    "            score = sentiment_score.roberta(text)\n",
    "            print(f\"üìä Sentiment score: {score}\")\n",
    "            cursor.execute(f\"UPDATE `tweets` SET `sentiment` = {score} WHERE `id` = {id}\")\n",
    "            if counter % round(n/10) == 0:\n",
    "                connection.commit()\n",
    "            print(f\"‚úÖ Tweet {id} updated.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}, Tweet: {id}\", file=sys.stderr)\n",
    "            errors += 1\n",
    "\n",
    "        counter += 1\n",
    "        duration = timeit.default_timer() - start\n",
    "        elapsed += duration\n",
    "        time_remaining = (n - counter) * (elapsed / counter)\n",
    "        print(f\"‚èØÔ∏è Process: {(counter/n)*100:.2f}% - #Ô∏è‚É£ {counter}/{n} tweets updated - ‚è≥ Time remaining : {str(datetime.timedelta(seconds=time_remaining))}\")\n",
    "        print(\"-----------------------------------\")\n",
    "\n",
    "update_sentiment_scores(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb491723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddfb95ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/master/anaconda3/envs/jbg030/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "sentiment_task = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\", tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "810b8c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I have an iphone\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4806a9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'neutral', 'score': 0.6373922228813171}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_task(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
